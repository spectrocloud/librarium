---
partial_category: self-hosted
partial_name: upgrade-palette-prereqs
---

- A healthy {props.version} management cluster where you can access the Local UI of the leader node.

  - Verify that your local machine can access the Local UI, as airgapped environments may have strict network policies preventing direct access.

- If using an external registry, the Palette CLI must be installed on your local machine to upload the content to the external registry. Refer to the <VersionedLink text="Palette CLI" url="/automation/palette-cli"/> guide for installation instructions.

  - Ensure your local machine has network access to the external registry server and you have the necessary permissions to push images to the registry.

- Access to the [Artifact Studio](https://artifact-studio.spectrocloud.com/) to download the content bundle for {props.version}.

  :::tip

  If you do not have access to Artifact Studio, contact your Spectro Cloud representative or [open a support ticket](https://support.spectrocloud.com/).

  :::

- Check that your upgrade path is supported by referring to the <PaletteVertexUrlMapper edition={props.edition} text="Supported Upgrade Paths" url="/upgrade#supported-upgrade-paths"/>.

- If upgrading from version **4.7.15**, you must run an additional script before upgrading {props.version}. This script changes the access mode of the `zot` deployment's PersistentVolumeClaim (PVC) from `RWX` (ReadWriteMany) to `RWO` (ReadWriteOnce), which is required for the upgrade process.

  <details>

  <summary> Click to expand the instructions for the 4.7.15 script </summary>

  1. Log in to Local UI of the leader node of your Palette management cluster. For example, `https://<palette-leader-node-ip>:5080`.

  2. From the left main menu, click **Cluster**.

  3. On the **Overview** tab, within the **Environment** section, click the link for the **Admin Kubeconfig File** to download the kubeconfig file.

  4. On your local machine, ensure you have `kubectl` installed and set the `KUBECONFIG` environment variable to point to the file.

     ```bash
     export KUBECONFIG=/path/to/downloaded/kubeconfig/file
     ```

  5. Issue the following command to check the PVCs in the `zot-system` namespace.

     ```bash
     kubectl get pvc --namespace zot-system
     ```

     Note the PVC name from the output, for example, `zot-pvc`. The access mode should be `RWX`. If it is already `RWO`, you can skip the remaining steps.

     ```shell hideClipboard title="Example output"
     NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          VOLUMEATTRIBUTESCLASS   AGE
     zot-pvc   Bound    pvc-6d603d91-d5f6-459a-b600-0a699cbb4936   250Gi      RWX            linstor-lvm-storage   <unset>                 3h58m
     ```

  6. Issue the following command to check the deployments in the `zot-system` namespace.

     ```bash
     kubectl get deploy --namespace zot-system
     ```

     Note the deployment name from the output, for example, `zot`.

     ```shell hideClipboard title="Example output"
     NAME   READY   UP-TO-DATE   AVAILABLE   AGE
     zot    1/1     1            1           3h59m
     ```

  7. Use the following command to create a script named `change-pvc-access-mode.sh`.

     The script safely changes the access mode of a Kubernetes `PersistentVolumeClaim` (PVC) without losing data, by temporarily deleting and re-creating the PVC while keeping the underlying `PersistentVolume` (PV) intact. It pauses your app during the change, then resumes it. For version 4.7.15, this is required for the `zot` deployment before upgrading.

     ```bash
     cat > change-pvc-access-mode.sh <<'SCRIPT'
     #!/bin/bash

     # Script to change PVC access mode while preserving data in LINSTOR/Piraeus
     # Usage: ./change-pvc-access-mode.sh <namespace> <pvc-name> <deployment-name> <new-access-mode>
     # Example: ./change-pvc-access-mode.sh zot-system zot-pvc zot ReadWriteOnce

     set -e

     # Colors for output
     RED='\033[0;31m'
     GREEN='\033[0;32m'
     YELLOW='\033[1;33m'
     NC='\033[0m' # No Color

     # Check arguments
     if [ "$#" -ne 4 ]; then
         echo -e "${RED}Error: Invalid number of arguments${NC}"
         echo "Usage: $0 <namespace> <pvc-name> <deployment-name> <new-access-mode>"
         echo "Access modes: ReadWriteOnce, ReadWriteMany, ReadOnlyMany"
         exit 1
     fi

     NAMESPACE="$1"
     PVC_NAME="$2"
     DEPLOYMENT_NAME="$3"
     NEW_ACCESS_MODE="$4"

     # Validate access mode
     if [[ ! "$NEW_ACCESS_MODE" =~ ^(ReadWriteOnce|ReadWriteMany|ReadOnlyMany)$ ]]; then
         echo -e "${RED}Error: Invalid access mode. Must be one of: ReadWriteOnce, ReadWriteMany, ReadOnlyMany${NC}"
         exit 1
     fi

     echo -e "${YELLOW}=== PVC Access Mode Migration Script ===${NC}"
     echo "Namespace: $NAMESPACE"
     echo "PVC: $PVC_NAME"
     echo "Deployment: $DEPLOYMENT_NAME"
     echo "New Access Mode: $NEW_ACCESS_MODE"
     echo ""

     # Step 1: Get PV name
     echo -e "${GREEN}[1/9] Getting PV name...${NC}"
     PV_NAME=$(kubectl get pvc "$PVC_NAME" -n "$NAMESPACE" -o jsonpath='{.spec.volumeName}')
     if [ -z "$PV_NAME" ]; then
         echo -e "${RED}Error: Could not find PV for PVC $PVC_NAME${NC}"
         exit 1
     fi
     echo "PV Name: $PV_NAME"

     # Step 2: Backup current configuration
     echo -e "${GREEN}[2/9] Backing up current PVC and PV configuration...${NC}"
     kubectl get pvc "$PVC_NAME" -n "$NAMESPACE" -o yaml > "${PVC_NAME}-backup-$(date +%Y%m%d-%H%M%S).yaml"
     kubectl get pv "$PV_NAME" -o yaml > "${PV_NAME}-backup-$(date +%Y%m%d-%H%M%S).yaml"
     echo "Backups created in current directory"

     # Step 3: Set PV reclaim policy to Retain
     echo -e "${GREEN}[3/9] Setting PV reclaim policy to Retain...${NC}"
     CURRENT_POLICY=$(kubectl get pv "$PV_NAME" -o jsonpath='{.spec.persistentVolumeReclaimPolicy}')
     echo "Current reclaim policy: $CURRENT_POLICY"
     if [ "$CURRENT_POLICY" != "Retain" ]; then
         kubectl patch pv "$PV_NAME" -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
         echo "Reclaim policy changed to Retain"
     else
         echo "Already set to Retain"
     fi

     # Step 4: Get current replica count
     echo -e "${GREEN}[4/9] Getting current deployment replica count...${NC}"
     REPLICA_COUNT=$(kubectl get deployment "$DEPLOYMENT_NAME" -n "$NAMESPACE" -o jsonpath='{.spec.replicas}')
     echo "Current replicas: $REPLICA_COUNT"

     # Step 5: Scale down deployment
     echo -e "${GREEN}[5/9] Scaling down deployment to 0...${NC}"
     kubectl scale deployment "$DEPLOYMENT_NAME" -n "$NAMESPACE" --replicas=0
     echo "Waiting for pods to terminate..."
     kubectl wait --for=delete pod -l app="$DEPLOYMENT_NAME" -n "$NAMESPACE" --timeout=120s 2>/dev/null || true
     sleep 5

     # Step 6: Delete PVC
     echo -e "${GREEN}[6/9] Deleting PVC (data preserved in PV)...${NC}"
     kubectl delete pvc "$PVC_NAME" -n "$NAMESPACE"
     echo "Waiting for PV to be Released..."
     sleep 5

     PV_STATUS=$(kubectl get pv "$PV_NAME" -o jsonpath='{.status.phase}')
     echo "PV Status: $PV_STATUS"

     # Step 7: Remove claimRef and update access mode
     echo -e "${GREEN}[7/9] Removing claimRef from PV...${NC}"
     kubectl patch pv "$PV_NAME" --type json -p='[{"op": "remove", "path": "/spec/claimRef"}]'

     echo -e "${GREEN}[7/9] Updating PV access mode to $NEW_ACCESS_MODE...${NC}"
     kubectl patch pv "$PV_NAME" -p "{\"spec\":{\"accessModes\":[\"$NEW_ACCESS_MODE\"]}}"

     PV_STATUS=$(kubectl get pv "$PV_NAME" -o jsonpath='{.status.phase}')
     echo "PV Status: $PV_STATUS"

     # Step 8: Create new PVC
     echo -e "${GREEN}[8/9] Creating new PVC with updated access mode...${NC}"

     # Get original PVC details
     STORAGE_SIZE=$(kubectl get pv "$PV_NAME" -o jsonpath='{.spec.capacity.storage}')
     STORAGE_CLASS=$(kubectl get pv "$PV_NAME" -o jsonpath='{.spec.storageClassName}')
     VOLUME_MODE=$(kubectl get pv "$PV_NAME" -o jsonpath='{.spec.volumeMode}')

     cat <<EOF | kubectl apply -f -
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: $PVC_NAME
       namespace: $NAMESPACE
       labels:
         app.kubernetes.io/managed-by: Helm
     spec:
       accessModes:
       - $NEW_ACCESS_MODE
       resources:
         requests:
           storage: $STORAGE_SIZE
       storageClassName: $STORAGE_CLASS
       volumeMode: $VOLUME_MODE
       volumeName: $PV_NAME
     EOF

     echo "Waiting for PVC to bind..."
     sleep 5
     kubectl get pvc "$PVC_NAME" -n "$NAMESPACE"

     # Step 9: Scale deployment back up
     echo -e "${GREEN}[9/9] Scaling deployment back to $REPLICA_COUNT replicas...${NC}"
     kubectl scale deployment "$DEPLOYMENT_NAME" -n "$NAMESPACE" --replicas="$REPLICA_COUNT"

     echo ""
     echo -e "${GREEN}=== Migration Complete ===${NC}"
     echo ""
     echo "Verifying final state..."
     kubectl get pvc "$PVC_NAME" -n "$NAMESPACE"
     echo ""
     kubectl get pods -n "$NAMESPACE"
     echo ""
     echo -e "${YELLOW}Note: Wait for pods to be Running and verify your application is working correctly${NC}"
     echo -e "${YELLOW}Backup files created: ${PVC_NAME}-backup-*.yaml and ${PV_NAME}-backup-*.yaml${NC}"
     SCRIPT
     ```

  8. Make the script executable.
  
     ```bash
     chmod u+x change-pvc-access-mode.sh
     ```   

  9. Execute the script with the following parameters. Replace `<pvc-name>` and `<deployment-name>` with the values noted from steps 5 and 6.

     ```bash
     ./change-pvc-access-mode.sh zot-system <pvc-name> <deployment-name> ReadWriteOnce
     ```

     ```bash hideClipboard title="Example command"
     ./change-pvc-access-mode.sh zot-system zot-pvc zot ReadWriteOnce
     ```

     ```shell hideClipboard title="Example output"
     === PVC Access Mode Migration Script ===
     Namespace: zot-system
     PVC: zot-pvc
     Deployment: zot
     New Access Mode: ReadWriteOnce

     [1/9] Getting PV name...
     PV Name: pvc-6d603d91-d5f6-459a-b600-0a699cbb4936
     [2/9] Backing up current PVC and PV configuration...
     Backups created in current directory
     [3/9] Setting PV reclaim policy to Retain...
     Current reclaim policy: Delete
     persistentvolume/pvc-6d603d91-d5f6-459a-b600-0a699cbb4936 patched
     Reclaim policy changed to Retain
     [4/9] Getting current deployment replica count...
     Current replicas: 1
     [5/9] Scaling down deployment to 0...
     deployment.apps/zot scaled
     Waiting for pods to terminate...
     [6/9] Deleting PVC (data preserved in PV)...
     persistentvolumeclaim "zot-pvc" deleted from zot-system namespace
     Waiting for PV to be Released...
     PV Status: Released
     [7/9] Removing claimRef from PV...
     persistentvolume/pvc-6d603d91-d5f6-459a-b600-0a699cbb4936 patched
     [7/9] Updating PV access mode to ReadWriteOnce...
     persistentvolume/pvc-6d603d91-d5f6-459a-b600-0a699cbb4936 patched
     PV Status: Available
     [8/9] Creating new PVC with updated access mode...
     persistentvolumeclaim/zot-pvc created
     Waiting for PVC to bind...
     NAME      STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          VOLUMEATTRIBUTESCLASS   AGE
     zot-pvc   Pending   pvc-6d603d91-d5f6-459a-b600-0a699cbb4936   0                         linstor-lvm-storage   <unset>                 5s
     [9/9] Scaling deployment back to 1 replicas...
     deployment.apps/zot scaled

     === Migration Complete ===

     Verifying final state...
     NAME      STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          VOLUMEATTRIBUTESCLASS   AGE
     zot-pvc   Pending   pvc-6d603d91-d5f6-459a-b600-0a699cbb4936   0                         linstor-lvm-storage   <unset>                 7s

     NAME                READY   STATUS    RESTARTS   AGE
     zot-c96cb7b-hspd2   0/1     Pending   0          1s

     Note: Wait for pods to be Running and verify your application is working correctly
     Backup files created: zot-pvc-backup-*.yaml and pvc-6d603d91-d5f6-459a-b600-0a699cbb4936-backup-*.yaml
     ```

  10. Verify that the PVC status is `Bound` and the deployment pods are in the `Running` state before proceeding with the upgrade.

      ```bash
      kubectl get pvc --namespace zot-system
      ```

      ```shell hideClipboard title="Example output"
      NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          VOLUMEATTRIBUTESCLASS   AGE
      zot-pvc   Bound    pvc-6d603d91-d5f6-459a-b600-0a699cbb4936   250Gi      RWO            linstor-lvm-storage   <unset>                 5m30s
      ```

      ```bash
      kubectl get pods --namespace zot-system
      ```

      ```shell hideClipboard title="Example output"
      NAME                READY   STATUS    RESTARTS   AGE
      zot-c96cb7b-hspd2   1/1     Running   0          5m3s
      ```

  </details>

- If upgrading from version **4.7.27**, you must run an additional script before upgrading {props.version}. This script ensures the `linstor-lvm-storage` StorageClass has correct parameters and safely recreates the PVC while preserving data.

  <details>

  <summary> Click to expand the instructions for the 4.7.27 script </summary>

  1. Log in to Local UI of the leader node of your Palette management cluster. For example, `https://<palette-leader-node-ip>:5080`.

  2. From the left main menu, click **Cluster**.

  3. On the **Overview** tab, within the **Environment** section, click the link for the **Admin Kubeconfig File** to download the kubeconfig file.

  4. On your local machine, ensure you have `kubectl` installed and set the `KUBECONFIG` environment variable to point to the file.

     ```bash
     export KUBECONFIG=/path/to/downloaded/kubeconfig/file
     ```

  5. Use the following command to create a script named `recreate.sh`.

     The script ensures that the `linstor-lvm-storage` StorageClass has the correct parameters (`resourceGroup: lvm-ha` and `placementCount: 3`). If the StorageClass does not exist or has different parameters, it recreates it. Then, it safely recreates a specified PVC to bind to a new PV while preserving data, handling deployments with `volumeBindingMode=WaitForFirstConsumer` by scaling down the deployment first.

     ```bash
     cat > recreate.sh <<'SCRIPT'
     #!/bin/bash
     # Recreate a PVC with updated StorageClass configuration
     # Ensures the linstor-lvm-storage StorageClass exists with correct parameters,
     # then recreates the specified PVC which may bind to a new PV.
     # Safely handles volumeBindingMode=WaitForFirstConsumer by scaling down the deployment first.

     set -euo pipefail

     NAMESPACE=${1:-zot-system}
     PVC_NAME=${2:-zot-pvc}
     DEPLOY_NAME=${3:-zot}


     #!/bin/bash
     set -e

     SC_NAME="linstor-lvm-storage"

     echo "Checking for existing StorageClass '$SC_NAME'..."

     if kubectl get storageclass $SC_NAME >/dev/null 2>&1; then
       echo "StorageClass '$SC_NAME' exists. Checking parameters..."

       CURRENT_RG=$(kubectl get storageclass $SC_NAME -o jsonpath='{.parameters.resourceGroup}')
       CURRENT_PC=$(kubectl get storageclass $SC_NAME -o jsonpath='{.parameters.placementCount}')

       if [[ "$CURRENT_RG" == "lvm-ha" && "$CURRENT_PC" == "3" ]]; then
         echo "✅ StorageClass already has resourceGroup='lvm-ha' and placementCount='3'. Skipping re-creation."
       else
         echo "StorageClass parameters differ (resourceGroup=$CURRENT_RG, placementCount=$CURRENT_PC). Recreating..."
         kubectl delete storageclass $SC_NAME
         echo "Recreating StorageClass '$SC_NAME'..."
         cat <<EOF | kubectl apply -f -
     apiVersion: storage.k8s.io/v1
     kind: StorageClass
     metadata:
       name: $SC_NAME
       annotations:
         storageclass.kubernetes.io/is-default-class: "false"
         helm.sh/hook: post-install,post-upgrade
     provisioner: linstor.csi.linbit.com
     parameters:
       placementCount: "3"
       resourceGroup: "lvm-ha"
       storagePool: "lvm-thin"
     reclaimPolicy: Delete
     volumeBindingMode: WaitForFirstConsumer
     allowVolumeExpansion: true
     EOF
         echo "✅ StorageClass '$SC_NAME' updated."
       fi
     else
       echo "StorageClass not found. Creating new one..."
       cat <<EOF | kubectl apply -f -
     apiVersion: storage.k8s.io/v1
     kind: StorageClass
     metadata:
       name: $SC_NAME
       annotations:
         storageclass.kubernetes.io/is-default-class: "false"
         helm.sh/hook: post-install,post-upgrade
     provisioner: linstor.csi.linbit.com
     parameters:
       placementCount: "3"
       resourceGroup: "lvm-ha"
       storagePool: "lvm-thin"
     reclaimPolicy: Delete
     volumeBindingMode: WaitForFirstConsumer
     allowVolumeExpansion: true
     EOF
       echo "✅ StorageClass '$SC_NAME' created."
     fi

     echo "Getting PV bound to PVC '${PVC_NAME}' in namespace '${NAMESPACE}'..."
     PV_NAME=$(kubectl get pvc -n "${NAMESPACE}" "${PVC_NAME}" -o jsonpath='{.spec.volumeName}' 2>/dev/null || true)
     if [ -z "${PV_NAME}" ]; then
       echo "❌ PVC '${PVC_NAME}' not found or not bound to any PV."
       exit 1
     fi
     echo "Found PV: ${PV_NAME}"

     echo "Scaling down deployment '${DEPLOY_NAME}' to release PVC..."
     kubectl scale deploy "${DEPLOY_NAME}" -n "${NAMESPACE}" --replicas=0 || true

     echo "Deleting PVC '${PVC_NAME}' (PV data will be retained)..."
     kubectl delete pvc -n "${NAMESPACE}" "${PVC_NAME}" --ignore-not-found

     STORAGE=$(kubectl get pv "${PV_NAME}" -o jsonpath='{.spec.capacity.storage}')
     ACCESSMODES=$(kubectl get pv "${PV_NAME}" -o jsonpath='{.spec.accessModes[0]}')

     echo "Recreating PVC '${PVC_NAME}' bound to existing PV '${PV_NAME}'..."
     cat <<EOF | kubectl apply -f -
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: ${PVC_NAME}
       namespace: ${NAMESPACE}
     spec:
       accessModes:
         - ${ACCESSMODES}
       storageClassName: linstor-lvm-storage
       resources:
         requests:
           storage: ${STORAGE}
     EOF

     echo "Scaling deployment '${DEPLOY_NAME}' back up..."
     kubectl scale deploy "${DEPLOY_NAME}" -n "${NAMESPACE}" --replicas=1 || true

     echo "Waiting for PVC '${PVC_NAME}' to rebind..."
     for i in {1..30}; do
       STATUS=$(kubectl get pvc -n "${NAMESPACE}" "${PVC_NAME}" -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
       if [[ "$STATUS" == "Bound" ]]; then
         echo "✅ PVC successfully bound."
         break
       fi
       sleep 2
     done

     # Get the new PV name after PVC is bound
     NEW_PV_NAME=$(kubectl get pvc -n "${NAMESPACE}" "${PVC_NAME}" -o jsonpath='{.spec.volumeName}' 2>/dev/null || echo "")
     if [ -z "${NEW_PV_NAME}" ]; then
       echo "Warning: Could not determine new PV name."
       NEW_PV_NAME="<check manually>"
     fi

     echo "Done!"
     echo "✅ PVC '${PVC_NAME}' is bound to PV '${NEW_PV_NAME}' and data preserved."
     echo "
     Verify with:
       kubectl get pvc -n ${NAMESPACE} ${PVC_NAME}
       kubectl get pv ${NEW_PV_NAME}
     "
     SCRIPT
     ```

  6. Make the script executable.
  
     ```bash
     chmod u+x recreate.sh
     ```   

  7. Use the following command to execute the script.

     ```bash
     ./recreate.sh
     ```

     ```shell hideClipboard title="Example output"
     Checking for existing StorageClass 'linstor-lvm-storage'...
     StorageClass 'linstor-lvm-storage' exists. Checking parameters...
     ✅ StorageClass already has resourceGroup='lvm-ha' and placementCount='3'. Skipping re-creation.
     Getting PV bound to PVC 'zot-pvc' in namespace 'zot-system'...
     Found PV: pvc-3b7a8f9c-1d2e-4f5a-9b0c-1a2b3c4d5e6f
     Scaling down deployment 'zot' to release PVC...
     deployment.apps/zot scaled
     Deleting PVC 'zot-pvc' (PV data will be retained)...
     persistentvolumeclaim "zot-pvc" deleted
     Recreating PVC 'zot-pvc' bound to existing PV 'pvc-3b7a8f9c-1d2e-4f5a-9b0c-1a2b3c4d5e6f'...
     persistentvolumeclaim/zot-pvc created
     Scaling deployment 'zot' back up...
     deployment.apps/zot scaled
     Waiting for PVC 'zot-pvc' to rebind...
     ✅ PVC successfully bound.
     Done!
     ✅ PVC 'zot-pvc' is bound to PV 'pvc-7e9d2a10-4a6b-431f-b9e7-8c2d1f3a5b6c' and data preserved.

     Verify with:
       kubectl get pvc -n zot-system zot-pvc
       kubectl get pv pvc-7e9d2a10-4a6b-431f-b9e7-8c2d1f3a5b6c
     ```

  9. Verify that the PVC and PV status is `Bound`.

     ```bash
     kubectl get pvc --namespace zot-system zot-pvc
     ```

     ```shell hideClipboard title="Example output"
     NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          VOLUMEATTRIBUTESCLASS   AGE
     zot-pvc   Bound    pvc-7e9d2a10-4a6b-431f-b9e7-8c2d1f3a5b6c   300Gi      RWO            linstor-lvm-storage   <unset>                 5m30s
     ```

     ```bash
     kubectl get pv pvc-7e9d2a10-4a6b-431f-b9e7-8c2d1f3a5b6c
     ```

     ```shell hideClipboard title="Example output"
     NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS          REASON   AGE
     pvc-7e9d2a10-4a6b-431f-b9e7-8c2d1f3a5b6c   300Gi      RWO            Delete           Bound    zot-system/zot-pvc       linstor-lvm-storage            5m30s
     ```

  </details>