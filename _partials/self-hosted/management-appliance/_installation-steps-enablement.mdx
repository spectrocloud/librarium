---
partial_category: self-hosted
partial_name: installation-steps-enablement
---

1. Download the {props.iso} ISO from the [Artifact Studio](https://artifact-studio.spectrocloud.com/). Refer to the <VersionedLink text="Artifact Studio guide" url="/downloads/artifact-studio"/> for instructions on how to access and download the ISO.

2. Load the {props.iso} ISO to a bootable device, such as a USB stick, or upload the ISO to a datastore in your VMware environment. You can use several software tools to create a bootable USB drive, such as [balenaEtcher](https://etcher.balena.io/).

   - For VMware vSphere, you can upload the {props.iso} ISO to a datastore using the vSphere Client or the `govc` CLI tool. Refer to the [vSphere](https://techdocs.broadcom.com/us/en/vmware-cis/vsphere/vsphere/9-0/upload-iso-image-installation-media-for-a-guest-operating-system.html) or [govc](https://github.com/vmware/govmomi/tree/main/govc) documentation for more information.
   - For Bare Metal, you can use tools like `scp` or `rsync` to transfer the {props.iso} ISO to the nodes, or use a USB drive to
     boot the nodes from the ISO.
   - For Machine as a Service (MAAS), you can upload and deploy ISOs using Packer. Refer to the [MAAS documentation](https://maas.io/docs/about-images#p-17467-custom-images) for more information.

   Ensure that the {props.iso} ISO is accessible to all nodes that will be part of the {props.version} management cluster.

3. Attach the {props.iso} ISO to the nodes and ensure the boot order is set to boot from the {props.iso} ISO first. 

   For example, in VMware vSphere, the VMs will have the {props.iso} ISO in **CD/DVD drive 1**. Refer to the documentation of your infrastructure provider for specific instructions on how to attach and boot from an ISO.

4. Restart the nodes to start the installation process.

5. Once the nodes have rebooted and entered the GRand Unified Bootloader (GRUB) menu, select the **Palette eXtended Kubernetes Edge Install (manual)** option and press **ENTER**.

   :::caution

   Ensure that you select the option within the first five seconds of the GRUB menu appearing, as it will automatically proceed with the default installation option after this time.

   :::

6. Once the nodes have finished booting, in the terminal, issue the following command to list the block devices.

   ```bash
   lsblk --paths
   ```

   Use the output to identify the device name to use for the {props.version} ISO stack. For example, `/dev/sda`.

   ```shell hideClipboard title="Example output" {3}
   NAME       MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
   /dev/loop0   7:0    0     1G  1 loop /run/rootfsbase
   /dev/sda     8:0    0   250G  0 disk
   /dev/sdb     8:16   0  5000G  0 disk
   /dev/sr0    11:0    1  17.3G  0 rom  /run/initramfs/live
   ```

7. If there are any partitions on the device you plan to use for the installation, you must delete them before proceeding. For example, if the device is `/dev/sda`, issue the following command to delete all partitions on the device.

   ```bash
   wipefs --all /dev/sda
   ```

   :::danger

   Deleting partitions will erase all data on the device. Ensure that you back up any important data before proceeding.

   :::

8. Issue the following command to edit the installation manifest.

   ```bash
   vi /oem/stylus_config.yaml
   ```

9. Add the following `install.device` section to your manifest, replacing `<storage-drive>` with the device name identified in step 6.

   ```yaml hideClipboard {6}
   #cloud-config

   cosign: false
   verify: false
   install:
     device: <storage-drive>
     grub-entry-name: "Palette eXtended Kubernetes Edge"
     system:
       size: 8192
   ...
   ```

10. Save the changes and exit the editor.

11. Issue the following command to start the installation process.

    ```bash
    kairos-agent install
    ```

12. Wait for the installation process to complete. This will take at least 15 minutes, depending on the resources
    available on the nodes. After completion, the nodes will reboot and display the Palette TUI.

13. In the Palette TUI, provide credentials for the initial account. This account will be used to log in to Local UI and for SSH access to the node.

    | **Field**                | **Description**                            |
    |--------------------------|--------------------------------------------|
    | **Username**             | Provide a username to use for the account. |
    | **Password**             | Enter a password for the account.          |
    | **Confirm Password**     | Re-enter the password for confirmation.    |

    Press **ENTER** to continue.

14. In the Palette TUI, the available configuration options are displayed and are described in the next three steps.
    Use the **TAB** key or the up and down arrow keys to switch between fields. When you make a change, press **ENTER**
    to apply the change. Use **ESC** to go back.

15. In **Hostname**, check the existing hostname and, optionally, change it to a new one.

16. In **Host Network Adapters**, select a network adapter you would like to configure. By default, the network adapters
    request an IP automatically from the Dynamic Host Configuration Protocol (DHCP) server. The CIDR block of an
    adapter's possible IP address is displayed in the **Host Network Adapters** screen without selecting an individual
    adapter.

    In the configuration page for each adapter, you can change the IP addressing scheme of the adapter and choose a static
    IP instead of DHCP. In Static IP mode, you will need to provide a static IP address and subnet mask, as well as the
    address of the default gateway. Specifying a static IP will remove the existing DHCP settings.

    You can also specify the Maximum Transmission Unit (MTU) for your network adapter. The MTU defines the largest size,
    in bytes, of a packet that can be sent over a network interface without needing to be fragmented.

17. In **DNS Configuration**, specify the IP address of the primary and alternate name servers. You can optionally
    specify a search domain.

18. After you are satisfied with the configurations, navigate to **Quit** and press **ENTER** to finish the
    configuration. Press **ENTER** again on the confirmation prompt.

    After a few seconds, the terminal displays the **Device Info** and prompts you to provision the device through Local UI.

    :::tip

    If you need to access the Palette TUI again, issue the `palette-tui` command in the terminal.

    :::

19. Ensure you complete the configuration on each node before proceeding to the next step.

20. Decide on the host that you plan to use as the leader of the group. Refer to <VersionedLink text="Link Hosts" url="/clusters/edge/local-ui/cluster-management/link-hosts#leader-hosts"/> for more information about leader hosts.

21. Access the Local UI of the leader host. Local UI is used to manage the {props.version} nodes and perform administrative
    tasks. It provides a web-based interface for managing the {props.version} management cluster.

    In your web browser, go to `https://<node-ip>:5080`. Replace `<node-ip>` with the IP address of your node. If you
    have changed the default port of the console, replace `5080` with the Local UI port. The address of the Local UI console
    is also displayed on the terminal screen of the node.

    If you are accessing Local UI for the first time, a security warning may be displayed in your web browser. This
    is because Local UI uses a self-signed certificate. You can safely ignore this warning and proceed to Local
    UI.

22. Log in to Local UI using the credentials you provided in step 10.

23. (Optional) If you need to configure a HTTP proxy server for the node, follow the steps in the <VersionedLink text="Configure HTTP-Proxy in Local UI" url="/clusters/edge/local-ui/host-management/configure-proxy"/> guide. When done, proceed to the next step.

24. From the left main menu, click **Linked Edge Hosts**.

25. Click **Generate token**. The host begins generating tokens that you will use to link this host with other
    hosts. The Base64 encoded token contains the IP address of the host, as well as an OTP that will expire in two
    minutes. Once a token expires, the leader generates another token automatically.

26. Click the **Copy** button to copy the token.

27. Log in to Local UI on the host that you want to link to the leader host.

28. From the left main menu, click **Linked Edge Hosts**.

29. Click **Link this device to another**.

30. In the pop-up box that appears, enter the token you copied from the leader host.

31. Click **Confirm**.

32. Repeat steps 27-31 for every host you want to link to the leader host.

33. Confirm that all linked hosts appear in the **Linked Edge Hosts** table. The following columns should show the
    required statuses.

    | **Column**  | **Status** |
    | ----------- | ---------- |
    | **Status**  | Ready      |
    | **Content** | Synced     |
    | **Health**  | Healthy    |

    Content synchronization will take at least five minutes to complete, depending on your network resources.

34. On the left main menu, click **Cluster**.

35. Click **Create cluster**.

36. For **Basic Information**, provide a name for the cluster and optional tags in `key:value` format.

37. In **Cluster Profile**, the **Imported Applications preview** section displays the applications that are included
    with the {props.app}. These applications are pre-configured and used to deploy your {props.version} management
    cluster.

    Leave the default options in place and click **Next**.

38. In **Profile Config**, configure the cluster profile settings to your requirements. Review the following tables for
    the available options.

    #### Cluster Profile Options

    | **Option**                        | **Description**                                                                                                                                 | **Type**      | **Default**          |
    | --------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | ------------- | -------------------- |
    | **Pod CIDR**                      | The CIDR range for the pod network. This is used to allocate IP addresses to pods in the cluster.                                               | CIDR notation | **`100.64.0.0/18`** |
    | **Service CIDR**                  | The CIDR range for the service network. This is used to allocate IP addresses to services in the cluster.                                       | CIDR notation | **`100.64.64.0/18`** |
    | **Ubuntu Pro Token (Optional)**   | The token for your [Ubuntu Pro](https://ubuntu.com/pro) subscription.                                                                           | String        | _No default_         |
    | **Storage Pool Drive (Optional)** | The storage pool device to use for the cluster. As mentioned in the [Prerequisites](#prerequisites), assign this to your second storage device. | String        | **`/dev/sdb`**       |
    | **CSI Placement Count** | The number of replicas for the Container Storage Interface (CSI) Persistent Volumes (PVs). The accepted values are `1` or `3`. We recommend using **3** to provide high availability for the CSI volumes. _This value must match the **MongoDB Replicas** value._ | Integer       | **`3`**              |

    #### Registry Options

    | **Option**                                    | **Description**                                                                                                                                                                                                                                                                                              | **Type**              | **Default**                               |
    | --------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------- | ----------------------------------------- |
    | **In Cluster Registry (Optional)**            | - `True` - Use internal Zot registry <br /> - `False` - Use external registry.                                                                                                                                                                                                                                     | Boolean               | **True**                                  |
    | **Registry Endpoint**                         | The DNS/IP endpoint for the registry. Leave the default entry if using the internal Zot registry, which is a virtual IP address assigned by [kube-vip](https://kube-vip.io/). Adjust if using an external registry.                                                                                          | String                | **`{{.spectro.system.cluster.kubevip}}`** |
    | **Registry Port**                             | The port for the registry. The default value can be changed for the internal Zot registry. Adjust if using an external registry.                                                                                                                                                                             | Integer               | **`30003`**                               |
    | **OCI Registry Base Content Path (Optional)** | The base path for the registry content for the internal or external registry. {props.version} packs will be stored in this directory.                                                                                                                                                                                | String                | **`spectro-content`**                     |
    | **OCI Pack Registry Username**                         | If using the internal Zot registry, leave the default username or adjust to your requirements. If using an external registry, provide the appropriate username.                                                                                                                                          | String                | **`admin`**                               |
    | **OCI Pack Registry Password**                | If using the internal Zot registry, enter a password to your requirements. If using an external registry, provide the appropriate password.                                                                                                                                                               | String                | _No default - must be provided._                              |
    | **OCI Registry Storage Size (GiB) (Optional)** | The size of the storage for the OCI registry. This is used to store the images and packs in the registry. The default value is set to 100 GiB, but this should be increased to at least **250** GiB for production environments. | Integer               | **`100`**                                 |
    | **OCI Pack Registry Ca Cert (Optional)**      | - Internal Zot registry - Not required. <br /> - External registry - The CA certificate that was used to sign the external registry certificate.                                                                                                                                                                           | Base64 encoded string | _No default_                              |
    | **Image Replacement Rules (Optional)**        | Set rules for replacing image references when using an external registry. For example, `all: oci-registry-ip:oci-registry-port/spectro-content`. Leave empty if using the internal Zot registry.                                                                                                                                  | String                | _No default_                              |
    | **Root Domain (Optional)**                    | The root domain for the registry. The default is set for the internal Zot registry, which is a virtual IP address assigned by [kube-vip](https://kube-vip.io/). If using an external registry, adjust this to the appropriate domain. | String                | **`{{.spectro.system.cluster.kubevip}}`** |
    | **Mongo Replicas** | The number of MongoDB replicas to create for the cluster. The accepted values are `1` or `3`. We recommend using **3** to provide high availability for the MongoDB database. _This value must match the **CSI Placement Count** value._ | Integer               | **`3`**                                   |

39. Click **Next** when you are done.

40. In **Cluster Config**, configure the following options.

    #### Cluster Config Options

    | **Option**                                 | **Description**                                                                                | **Type** | **Default**  |
    | ------------------------------------------ | ---------------------------------------------------------------------------------------------- | -------- | ------------ |
    | **Network Time Protocol (NTP) (Optional)** | The NTP servers to synchronize time within the cluster.                                        | String   | _No default_ |
    | **SSH Keys (Optional)**                    | The public SSH keys to access the cluster nodes. Add additional keys by clicking **Add Item**. | String   | _No default_ |
    | **Virtual IP Address (VIP)**               | The virtual IP address for the cluster. This is used for load balancing and high availability. | String   | _No default_ |

    Click **Next** when you are done.

41. In **Node Config**, configure the following options.

    :::important

    You must assign at least three control plane nodes for high availability. You can remove the worker node
    pool as it is not required for the {props.version} management cluster. When doing this, ensure that the
    **Allow worker capability** option is enabled for the control plane node pool.

    :::

    #### Node Pool Options

    <Tabs>

    <TabItem value="control-plane-pool-options" label="Control Plane Pool Options">

    | **Option**                                       | **Description**                                                                                                                                                       | **Type**                                             | **Default**              |
    | ------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------- | ------------------------ |
    | **Node pool name**                               | The name of the control plane node pool. This will be used to identify the node pool in {props.version}.                                                                      | String                                               | **`control-plane-pool`** |
    | **Allow worker capability (Optional)**           | Whether to allow workloads to be scheduled on this control plane node pool. Ensure that this is enabled if no worker pool is assigned to the cluster.                 | Boolean                                              | **True**                 |
    | **Additional Kubernetes Node Labels (Optional)** | Tags for the node pool in `key:value` format. These tags can be used to filter and search for node pools in {props.version}.                                                  | String                                               | _No default_             |
    | **Taints**                                       | Taints for the node pool in `key=value:effect` format. Taints are used to prevent pods from being scheduled on the nodes in this pool unless they tolerate the taint. | - **Key** = string <br />- **Value** = string<br />- **Effect** = string (enum) | _No default_             |

    </TabItem>

    <TabItem value="worker-pool-options" label="Worker Pool Options">

    | **Option**                                       | **Description**                                                                                                                                                       | **Type**                                             | **Default**       |
    | ------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------- | ----------------- |
    | **Node pool name**                               | The name of the worker node pool. This will be used to identify the node pool in {props.version}.                                                                             | String                                               | **`worker-pool`** |
    | **Additional Kubernetes Node Labels (Optional)** | Tags for the node pool in `key:value` format. These tags can be used to filter and search for node pools in {props.version}.                                                  | String                                               | _No default_      |
    | **Taints**                                       | Taints for the node pool in `key=value:effect` format. Taints are used to prevent pods from being scheduled on the nodes in this pool unless they tolerate the taint. | - **Key** = string <br />- **Value** = string<br />- **Effect** = string (enum) | _No default_      |

    </TabItem>

    </Tabs>

    ##### Pool Configuration

    The following options are available for both the control plane and worker node pools. You can configure these
    options to your requirements. You can also remove worker pools if not needed.

    | **Option**               | **Description**                                                                                                                                                                           | **Type**      | **Default**                                                                                      |
    | ------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- | ------------------------------------------------------------------------------------------------ |
    | **Architecture**         | The CPU architecture of the nodes. This is used to ensure compatibility with the applications operating on the nodes.                                                                       | String (enum) | **`amd64`**                                                                                      |
    | **Add Edge Hosts**       | Click **Add Item** and select the other hosts that you installed using the {props.app} ISO. These hosts will be added to the node pool. Each pool must contain at least one node. | N/A           | - **Control Plane Pool** = _Current host selected_ <br /> - **Worker Pool** = _No host selected_ |
    | **NIC Name**             | The name of the network interface card (NIC) to use for the nodes. Leave on **Auto** to let the system choose the appropriate NIC, or select one manually from the drop-down menu.        | N/A           | **Auto**                                                                                         |
    | **Host Name (Optional)** | The hostname for the nodes. This is used to identify the nodes in the cluster. A generated hostname is provided automatically, which you can adjust to your requirements.                   | String        | **`edge-*`**                                                                                     |

42. Click **Next** when you are done.

43. In **Review**, check that your configuration is correct. If you need to make changes, click on any of the sections
    in the left sidebar to go back and edit the configuration.

    When you are satisfied with your configuration, click **Deploy Cluster**. This will start the cluster creation
    process.

    The cluster creation process will take 20 to 30 minutes to complete. You can monitor progress from the **Overview** tab
    on the **Cluster** page in the left main menu. The cluster is fully provisioned when the status changes to
    **Running** and the health status is **Healthy**.

44. Once the cluster is provisioned, access the {props.version} system console using the virtual IP address (VIP) you configured
    earlier. Open your web browser and go to `https://<vip-address>/system`. Replace `<vip-address>` with the VIP you
    configured for the cluster.

    The first time you visit the system console, a warning message about an untrusted TLS certificate may appear. This
    is expected, as you have not yet uploaded your TLS certificate. You can ignore this warning message and proceed.

45. You will be prompted to log in to {props.version} system console. Use `admin` as the username and `admin` as the password.
    You will be prompted to change the password after logging in.

46. In the **Account Info** window, provide the following information.

    | **Field**                | **Description**    |
    |--------------------------|--------------------|
    | **Email address**        | This is used for notifications and password recovery as well as logging in to the {props.version} system console. This will not be active until you <PaletteVertexUrlMapper edition={props.edition} text="configure SMTP settings" url="/system-management/smtp"/> in {props.version} system console and verify your email address. |
    | **Current password**     | Use `admin` as the current password. |
    | **New password**         | Enter a new password for the account. |
    | **Confirm new password** | Re-enter the new password for confirmation. |

    Refer to <PaletteVertexUrlMapper edition={props.edition} text="Password Requirements and Security" url="/system-management/account-management/credentials#password-requirements-and-security"/> to learn about password requirements.

After logging in, a summary page is displayed. You now have access to the {props.version} system console, where you can manage your
{props.version} environment.

If you are accessing the {props.version} system console for the first time, a security warning may be displayed in your web
browser. This is because {props.version} is configured with a self-signed certificate. You can replace the self-signed
certificate with your own SSL certificates as guided later in [Next Steps](#next-steps).
