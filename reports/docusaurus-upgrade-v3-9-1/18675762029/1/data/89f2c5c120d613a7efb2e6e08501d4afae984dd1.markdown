# Page snapshot

```yaml
- generic [active] [ref=e1]:
  - generic [ref=e2]:
    - region "Skip to main content":
      - link "Skip to main content" [ref=e3] [cursor=pointer]:
        - /url: "#__docusaurus_skipToContent_fallback"
    - navigation "Main" [ref=e4]:
      - generic [ref=e5]:
        - generic [ref=e6]:
          - link "Spectro cloud logo" [ref=e7] [cursor=pointer]:
            - /url: /
            - img "Spectro cloud logo" [ref=e9]
          - link "Docs" [ref=e10] [cursor=pointer]:
            - /url: /release-notes/
          - link "Tutorials" [ref=e11] [cursor=pointer]:
            - /url: /tutorials/
          - link "Downloads" [ref=e12] [cursor=pointer]:
            - /url: /downloads/
          - link "API" [ref=e13] [cursor=pointer]:
            - /url: /api/introduction/
          - button "latest" [ref=e15] [cursor=pointer]
        - generic [ref=e16]:
          - link "GitHub repository" [ref=e17] [cursor=pointer]:
            - /url: https://github.com/spectrocloud/librarium
          - button "Switch between dark and light mode (currently system mode)" [ref=e19] [cursor=pointer]:
            - img [ref=e20]
          - button "Search (Control+k)" [ref=e23] [cursor=pointer]:
            - generic [ref=e24]:
              - img [ref=e25]
              - generic [ref=e28]: Search
    - generic [ref=e32]:
      - complementary [ref=e33]:
        - generic [ref=e35]:
          - link "Spectro cloud logo" [ref=e36] [cursor=pointer]:
            - /url: /
            - img "Spectro cloud logo" [ref=e37]
          - navigation "Docs sidebar" [ref=e38]:
            - list [ref=e39]:
              - listitem [ref=e40]:
                - generic [ref=e41]:
                  - link "Release Notes" [ref=e42] [cursor=pointer]:
                    - /url: /release-notes/
                    - img [ref=e44]
                    - text: Release Notes
                  - button "Toggle the collapsible sidebar category 'Release Notes'" [ref=e46] [cursor=pointer]
              - listitem [ref=e47]:
                - generic [ref=e48]:
                  - link "Security Bulletins" [ref=e49] [cursor=pointer]:
                    - /url: /security-bulletins/
                    - img [ref=e51]
                    - text: Security Bulletins
                  - button "Toggle the collapsible sidebar category 'Security Bulletins'" [ref=e53] [cursor=pointer]
              - listitem [ref=e54]:
                - generic [ref=e55]:
                  - link "What is Palette?" [ref=e56] [cursor=pointer]:
                    - /url: /
                    - img [ref=e58]
                    - text: What is Palette?
                  - button "Toggle the collapsible sidebar category 'What is Palette?'" [ref=e60] [cursor=pointer]
              - listitem [ref=e61]:
                - link "Getting Started" [ref=e62] [cursor=pointer]:
                  - /url: /getting-started/
                  - img [ref=e64]
                  - text: Getting Started
              - listitem [ref=e66]:
                - generic [ref=e67]:
                  - link "Architecture" [ref=e68] [cursor=pointer]:
                    - /url: /architecture/
                    - img [ref=e70]
                    - text: Architecture
                  - button "Toggle the collapsible sidebar category 'Architecture'" [ref=e72] [cursor=pointer]
              - listitem [ref=e73]:
                - generic [ref=e74]:
                  - link "Profiles" [ref=e75] [cursor=pointer]:
                    - /url: /profiles/
                    - img [ref=e77]
                    - text: Profiles
                  - button "Toggle the collapsible sidebar category 'Profiles'" [ref=e81] [cursor=pointer]
              - listitem [ref=e82]:
                - generic [ref=e83]:
                  - link "Cluster Templates" [ref=e84] [cursor=pointer]:
                    - /url: /cluster-templates/
                    - img [ref=e86]
                    - text: Cluster Templates
                  - button "Toggle the collapsible sidebar category 'Cluster Templates'" [ref=e88] [cursor=pointer]
              - listitem [ref=e89]:
                - generic [ref=e90]:
                  - link "Deployment Modes" [ref=e91] [cursor=pointer]:
                    - /url: /deployment-modes/
                    - img [ref=e93]
                    - text: Deployment Modes
                  - button "Toggle the collapsible sidebar category 'Deployment Modes'" [ref=e95] [cursor=pointer]
              - listitem [ref=e96]:
                - generic [ref=e97]:
                  - link "Clusters" [ref=e98] [cursor=pointer]:
                    - /url: /clusters/
                    - img [ref=e100]
                    - text: Clusters
                  - button "Toggle the collapsible sidebar category 'Clusters'" [ref=e106] [cursor=pointer]
              - listitem [ref=e107]:
                - generic [ref=e108]:
                  - link "Bring Your Own OS (BYOOS)" [ref=e109] [cursor=pointer]:
                    - /url: /byoos/
                    - img [ref=e111]
                    - text: Bring Your Own OS (BYOOS)
                  - button "Toggle the collapsible sidebar category 'Bring Your Own OS (BYOOS)'" [ref=e113] [cursor=pointer]
              - listitem [ref=e114]:
                - generic [ref=e115]:
                  - link "Palette Dev Engine" [ref=e116] [cursor=pointer]:
                    - /url: /devx/
                    - img [ref=e118]
                    - text: Palette Dev Engine
                  - button "Toggle the collapsible sidebar category 'Palette Dev Engine'" [ref=e120] [cursor=pointer]
              - listitem [ref=e121]:
                - generic [ref=e122]:
                  - link "Virtual Machine Orchestrator" [ref=e123] [cursor=pointer]:
                    - /url: /vm-management/
                    - img [ref=e125]
                    - text: Virtual Machine Orchestrator
                  - button "Toggle the collapsible sidebar category 'Virtual Machine Orchestrator'" [ref=e127] [cursor=pointer]
              - listitem [ref=e128]:
                - generic [ref=e129]:
                  - link "Workspaces" [ref=e130] [cursor=pointer]:
                    - /url: /workspace/
                    - img [ref=e132]
                    - text: Workspaces
                  - button "Toggle the collapsible sidebar category 'Workspaces'" [ref=e134] [cursor=pointer]
              - listitem [ref=e135]:
                - generic [ref=e136]:
                  - link "Packs List" [ref=e137] [cursor=pointer]:
                    - /url: /integrations/
                    - img [ref=e139]
                    - text: Packs List
                  - button "Toggle the collapsible sidebar category 'Packs List'" [ref=e141] [cursor=pointer]
              - listitem [ref=e142]:
                - generic [ref=e143]:
                  - link "User & Role Management" [ref=e144] [cursor=pointer]:
                    - /url: /user-management/
                    - img [ref=e146]
                    - text: User & Role Management
                  - button "Toggle the collapsible sidebar category 'User & Role Management'" [ref=e148] [cursor=pointer]
              - listitem [ref=e149]:
                - generic [ref=e150]:
                  - link "Registries and Packs" [ref=e151] [cursor=pointer]:
                    - /url: /registries-and-packs/
                    - img [ref=e153]
                    - text: Registries and Packs
                  - button "Toggle the collapsible sidebar category 'Registries and Packs'" [ref=e155] [cursor=pointer]
              - listitem [ref=e156]:
                - generic [ref=e157]:
                  - link "Security" [ref=e158] [cursor=pointer]:
                    - /url: /security/
                    - img [ref=e160]
                    - text: Security
                  - button "Toggle the collapsible sidebar category 'Security'" [ref=e162] [cursor=pointer]
              - listitem [ref=e163]:
                - generic [ref=e164]:
                  - link "Audit Logs" [ref=e165] [cursor=pointer]:
                    - /url: /audit-logs/
                    - img [ref=e167]
                    - text: Audit Logs
                  - button "Toggle the collapsible sidebar category 'Audit Logs'" [ref=e169] [cursor=pointer]
              - listitem [ref=e170]:
                - generic [ref=e171]:
                  - link "Self-Hosted Palette" [ref=e172] [cursor=pointer]:
                    - /url: /enterprise-version/
                    - img [ref=e174]
                    - text: Self-Hosted Palette
                  - button "Toggle the collapsible sidebar category 'Self-Hosted Palette'" [ref=e176] [cursor=pointer]
              - listitem [ref=e177]:
                - generic [ref=e178]:
                  - link "Palette VerteX" [ref=e179] [cursor=pointer]:
                    - /url: /vertex/
                    - img [ref=e181]
                    - text: Palette VerteX
                  - button "Toggle the collapsible sidebar category 'Palette VerteX'" [ref=e183] [cursor=pointer]
              - listitem [ref=e184]:
                - generic [ref=e185]:
                  - link "Tenant Administration" [ref=e186] [cursor=pointer]:
                    - /url: /tenant-settings/
                    - img [ref=e188]
                    - text: Tenant Administration
                  - button "Toggle the collapsible sidebar category 'Tenant Administration'" [ref=e190] [cursor=pointer]
              - listitem [ref=e191]:
                - generic [ref=e192]:
                  - link "Automation" [ref=e193] [cursor=pointer]:
                    - /url: /automation/
                    - img [ref=e195]
                    - text: Automation
                  - button "Toggle the collapsible sidebar category 'Automation'" [ref=e197] [cursor=pointer]
              - listitem [ref=e198]:
                - generic [ref=e199]:
                  - link "Troubleshooting" [expanded] [ref=e200] [cursor=pointer]:
                    - /url: /troubleshooting/
                    - img [ref=e202]
                    - text: Troubleshooting
                  - button "Toggle the collapsible sidebar category 'Troubleshooting'" [ref=e204] [cursor=pointer]
                - list [ref=e205]:
                  - listitem [ref=e206]:
                    - link "Kubernetes Debugging" [ref=e207] [cursor=pointer]:
                      - /url: /troubleshooting/kubernetes-tips/
                  - listitem [ref=e208]:
                    - link "Automation" [ref=e209] [cursor=pointer]:
                      - /url: /troubleshooting/automation/
                  - listitem [ref=e210]:
                    - link "Cluster Deployment" [ref=e211] [cursor=pointer]:
                      - /url: /troubleshooting/cluster-deployment/
                  - listitem [ref=e212]:
                    - link "Nodes & Clusters" [ref=e213] [cursor=pointer]:
                      - /url: /troubleshooting/nodes/
                  - listitem [ref=e214]:
                    - link "Packs" [ref=e215] [cursor=pointer]:
                      - /url: /troubleshooting/pack-issues/
                  - listitem [ref=e216]:
                    - link "Palette Dev Engine" [ref=e217] [cursor=pointer]:
                      - /url: /troubleshooting/palette-dev-engine/
                  - listitem [ref=e218]:
                    - generic [ref=e219]:
                      - link "Edge" [ref=e220] [cursor=pointer]:
                        - /url: /troubleshooting/edge/
                      - button "Toggle the collapsible sidebar category 'Edge'" [ref=e221] [cursor=pointer]
                  - listitem [ref=e222]:
                    - link "Private Cloud Gateway" [ref=e223] [cursor=pointer]:
                      - /url: /troubleshooting/pcg/
                  - listitem [ref=e224]:
                    - link "Virtual Machine Orchestrator" [ref=e225] [cursor=pointer]:
                      - /url: /troubleshooting/vmo-issues/
                  - listitem [ref=e226]:
                    - link "Enterprise Install" [ref=e227] [cursor=pointer]:
                      - /url: /troubleshooting/enterprise-install/
                  - listitem [ref=e228]:
                    - link "Palette Upgrade" [ref=e229] [cursor=pointer]:
                      - /url: /troubleshooting/palette-upgrade/
                  - listitem [ref=e230]:
                    - link "Generate HAR Files" [ref=e231] [cursor=pointer]:
                      - /url: /troubleshooting/generate-har-files/
              - listitem [ref=e232]:
                - link "Glossary" [ref=e233] [cursor=pointer]:
                  - /url: /glossary-all/
                  - img [ref=e235]
                  - text: Glossary
              - listitem [ref=e237]:
                - generic [ref=e238]:
                  - link "Compliance & Legal" [ref=e239] [cursor=pointer]:
                    - /url: /legal-licenses/
                    - img [ref=e241]
                    - text: Compliance & Legal
                  - button "Toggle the collapsible sidebar category 'Compliance & Legal'" [ref=e243] [cursor=pointer]
              - listitem [ref=e244]:
                - button "Privacy Settings" [ref=e245] [cursor=pointer]:
                  - img [ref=e246]
                  - text: Privacy Settings
      - main [ref=e248]:
        - generic [ref=e250]:
          - generic [ref=e252]:
            - article [ref=e253]:
              - navigation "Breadcrumbs" [ref=e254]:
                - list [ref=e255]:
                  - listitem [ref=e256]:
                    - link "Home page" [ref=e257] [cursor=pointer]:
                      - /url: /
                      - img [ref=e258]
                  - listitem [ref=e260]:
                    - link "Troubleshooting" [ref=e261] [cursor=pointer]:
                      - /url: /troubleshooting/
                  - listitem [ref=e262]:
                    - generic [ref=e263]: Enterprise Install
              - generic [ref=e264]:
                - heading "Enterprise Install" [level=1] [ref=e266]
                - paragraph [ref=e267]: Refer to the following sections to troubleshoot errors encountered when installing an Enterprise Cluster.
                - heading "Scenario - VerteX Management Appliance Fails to Upgrade due to Stuck LINSTOR Satellite PodsDirect link to Scenario - VerteX Management Appliance Fails to Upgrade due to Stuck LINSTOR Satellite Pods" [level=2] [ref=e268]:
                  - text: Scenario - VerteX Management Appliance Fails to Upgrade due to Stuck LINSTOR Satellite Pods
                  - link "Direct link to Scenario - VerteX Management Appliance Fails to Upgrade due to Stuck LINSTOR Satellite Pods" [ref=e269] [cursor=pointer]:
                    - /url: "#scenario---vertex-management-appliance-fails-to-upgrade-due-to-stuck-linstor-satellite-pods"
                    - text: "#"
                - paragraph [ref=e270]:
                  - text: When attempting to upgrade the VerteX Management Appliance, the
                  - code [ref=e271]: linstor-satellite.*
                  - text: and
                  - code [ref=e272]: linstor-csi-node.*
                  - text: pods may become stuck, which causes the upgrade process to stall. This is because the
                  - code [ref=e273]: linstor-satellite.*
                  - text: pods may be using an incorrect
                  - link "Distributed Replicated Block Device (DRBD)" [ref=e274] [cursor=pointer]:
                    - /url: https://linbit.com/drbd-user-guide/drbd-guide-9_0-en/
                  - text: image for the
                  - code [ref=e275]: drbd-module-loader
                  - text: container.
                - paragraph [ref=e276]: To resolve this issue, you can check whether the pods are using an incorrect image and update them if necessary.
                - heading "Debug StepsDirect link to Debug Steps" [level=3] [ref=e277]:
                  - text: Debug Steps
                  - link "Direct link to Debug Steps" [ref=e278] [cursor=pointer]:
                    - /url: "#debug-steps"
                    - text: "#"
                - list [ref=e279]:
                  - listitem [ref=e280]:
                    - paragraph [ref=e281]:
                      - text: Log in to the Local UI of the leader node of your VerteX management cluster. By default, Local UI is accessible at
                      - code [ref=e282]: https://<node-ip>:5080
                      - text: . Replace
                      - code [ref=e283]: <node-ip>
                      - text: with the IP address of the leader node.
                  - listitem [ref=e284]:
                    - paragraph [ref=e285]:
                      - text: From the left main menu, click
                      - strong [ref=e286]: Cluster
                      - text: .
                  - listitem [ref=e287]:
                    - paragraph [ref=e288]:
                      - text: Under
                      - strong [ref=e289]: Environment
                      - text: ", download the"
                      - strong [ref=e290]: Admin Kubeconfig File
                      - text: by clicking on the
                      - code [ref=e291]: <cluster-name>.kubeconfig
                      - text: hyperlink.
                  - listitem [ref=e292]:
                    - paragraph [ref=e293]: Open a terminal session in an environment that has network access to the VerteX management cluster.
                  - listitem [ref=e294]:
                    - paragraph [ref=e295]:
                      - text: Issue the following command to set the
                      - code [ref=e296]: KUBECONFIG
                      - text: environment variable to the path of the kubeconfig file you downloaded in step 3.
                    - generic [ref=e298]:
                      - code [ref=e300]:
                        - generic [ref=e301]: export KUBECONFIG=<path-to-kubeconfig-file>
                      - button "Copy code to clipboard" [ref=e303] [cursor=pointer]:
                        - generic [ref=e304]:
                          - img [ref=e305]
                          - img [ref=e307]
                  - listitem [ref=e309]:
                    - paragraph [ref=e310]:
                      - text: Use the following command to check the status of these pods in the
                      - code [ref=e311]: piraeus-system
                      - text: namespace.
                    - generic [ref=e313]:
                      - code [ref=e315]:
                        - generic [ref=e316]: kubectl get pods --namespace piraeus-system
                      - button "Copy code to clipboard" [ref=e318] [cursor=pointer]:
                        - generic [ref=e319]:
                          - img [ref=e320]
                          - img [ref=e322]
                    - paragraph [ref=e324]:
                      - text: In the output, look for the status of the
                      - code [ref=e325]: linstor-satellite.*
                      - text: pods.
                    - generic [ref=e326]:
                      - generic [ref=e327]: Example output
                      - generic [ref=e328]:
                        - code [ref=e330]:
                          - generic [ref=e331]: NAME READY STATUS RESTARTS AGE
                          - generic [ref=e332]: ha-controller-2886l 1/1 Running 0 25h
                          - generic [ref=e333]: ha-controller-nnvqt 1/1 Running 1 (29m ago) 25h
                          - generic [ref=e334]: ha-controller-qhc26 1/1 Running 1 (36m ago) 25h
                          - generic [ref=e335]: linstor-controller-69b8ff6479-ccpzx 1/1 Running 0 31m
                          - generic [ref=e336]: linstor-csi-controller-78c8bc4d55-5gk2b 7/7 Running 4 (28m ago) 38m
                          - generic [ref=e337]: linstor-csi-node-dp8lm 0/3 Error 0 25h
                          - generic [ref=e338]: linstor-csi-node-r2hfv 0/3 Error 0 25h
                          - generic [ref=e339]: linstor-csi-node-tpt6h 3/3 Running 0 25h
                          - generic [ref=e340]: linstor-satellite.edge-1d3f3842cb0fdcef14b65cb510b5974f-5vkml 2/2 Running 0 25h
                          - generic [ref=e341]: linstor-satellite.edge-53583842350d90345a1f7251033cb228-8s7js 0/2 Init:CrashLoopBackOff 10 (2m54s ago) 26m
                          - generic [ref=e342]: linstor-satellite.edge-c0913842383ebd183d13d1458bb762c5-78q97 0/2 Init:CrashLoopBackOff 11 (3m46s ago) 33m
                          - generic [ref=e343]: piraeusoperator-piraeus-controller-manager-6f8988d598-b2v57 1/1 Running 1 (28m ago) 25h
                        - button "Toggle word wrap" [ref=e345] [cursor=pointer]:
                          - img [ref=e346]
                  - listitem [ref=e348]:
                    - paragraph [ref=e349]:
                      - text: If any of the
                      - code [ref=e350]: linstor-satellite.*
                      - text: pods are not in a
                      - strong [ref=e351]: Running
                      - text: state, use the following command to describe the pods. Replace
                      - code [ref=e352]: <pod-name>
                      - text: with the name of the LINSTOR satellite pod you want to inspect.
                    - generic [ref=e354]:
                      - code [ref=e356]:
                        - generic [ref=e357]: kubectl describe pod <pod-name> --namespace piraeus-system
                      - button "Copy code to clipboard" [ref=e359] [cursor=pointer]:
                        - generic [ref=e360]:
                          - img [ref=e361]
                          - img [ref=e363]
                    - paragraph [ref=e365]:
                      - text: Look for events indicating that the pod is attempting to use the
                      - code [ref=e366]: drbd9-jammy:v9.2.13
                      - text: image for the
                      - code [ref=e367]: drbd-module-loader
                      - text: container, such as the following example.
                    - generic [ref=e368]:
                      - generic [ref=e369]: Example output
                      - generic [ref=e370]:
                        - code [ref=e372]:
                          - generic [ref=e373]: ...
                          - generic [ref=e374]: "Events:"
                          - generic [ref=e375]: Type Reason Age From Message
                          - generic [ref=e376]: "---- ------ ---- ---- -------"
                          - generic [ref=e377]: Normal Scheduled 34m default-scheduler Successfully assigned piraeus-system/linstor-satellite.edge-c0913842383ebd183d13d1458bb762c5-78q97 to edge-c0913842383ebd183d13d1458bb762c5
                          - generic [ref=e378]: Warning BackOff 26m (x6 over 31m) kubelet Back-off restarting failed container drbd-module-loader in pod linstor-satellite.edge-c0913842383ebd183d13d1458bb762c5-78q97_piraeus-system(71ea7db5-cc2c-4585-b1f7-fcc19bf14891)
                          - generic [ref=e379]: Normal Pulled 26m (x5 over 34m) kubelet Container image "us-docker.pkg.dev/palette-images-fips/packs/piraeus-operator/2.8.1/drbd9-jammy:v9.2.13" already present on machine
                          - generic [ref=e380]: "Normal Created 26m (x5 over 34m) kubelet Created container: drbd-module-loader"
                          - generic [ref=e381]: Normal Started 26m (x5 over 34m) kubelet Started container drbd-module-loader
                          - generic [ref=e382]: Normal Pulled 5m58s (x7 over 25m) kubelet Container image "us-docker.pkg.dev/palette-images-fips/packs/piraeus-operator/2.8.1/drbd9-jammy:v9.2.13" already present on machine
                          - generic [ref=e383]: "Normal Created 5m58s (x7 over 25m) kubelet Created container: drbd-module-loader"
                          - generic [ref=e384]: Normal Started 5m58s (x7 over 25m) kubelet Started container drbd-module-loader
                          - generic [ref=e385]: Warning BackOff 3m41s (x53 over 23m) kubelet Back-off restarting failed container drbd-module-loader in pod linstor-satellite.edge-c0913842383ebd183d13d1458bb762c5-78q97_piraeus-system(71ea7db5-cc2c-4585-b1f7-fcc19bf14891)
                        - button "Toggle word wrap" [ref=e387] [cursor=pointer]:
                          - img [ref=e388]
                  - listitem [ref=e390]:
                    - paragraph [ref=e391]:
                      - text: If any of the
                      - code [ref=e392]: linstor-satellite.*
                      - text: pods are using the
                      - code [ref=e393]: drbd9-jammy:v9.2.13
                      - text: image, issue the following command to create a manifest that corrects the image reference for the
                      - code [ref=e394]: drbd-module-loader
                      - text: container.
                    - generic [ref=e396]:
                      - code [ref=e398]:
                        - generic [ref=e399]: kubectl apply --filename - <<EOF
                        - generic [ref=e400]: "apiVersion: piraeus.io/v1"
                        - generic [ref=e401]: "kind: LinstorSatelliteConfiguration"
                        - generic [ref=e402]: "metadata:"
                        - generic [ref=e403]: "name: custom-loader-image"
                        - generic [ref=e404]: "namespace: piraeus-system"
                        - generic [ref=e405]: "spec:"
                        - generic [ref=e406]: "podTemplate:"
                        - generic [ref=e407]: "spec:"
                        - generic [ref=e408]: "initContainers:"
                        - generic [ref=e409]: "- name: drbd-module-loader"
                        - generic [ref=e410]: "image: us-docker.pkg.dev/palette-images-fips/packs/piraeus-operator/2.8.1/dbrd-loader:v2.8.1"
                        - generic [ref=e411]: "imagePullPolicy: IfNotPresent"
                        - generic [ref=e412]: EOF
                      - generic [ref=e413]:
                        - button "Toggle word wrap" [ref=e414] [cursor=pointer]:
                          - img [ref=e415]
                        - button "Copy code to clipboard" [ref=e417] [cursor=pointer]:
                          - generic [ref=e418]:
                            - img [ref=e419]
                            - img [ref=e421]
                    - generic [ref=e423]:
                      - generic [ref=e424]: Expected output
                      - generic [ref=e425]:
                        - code [ref=e427]:
                          - generic [ref=e428]: linstorsatelliteconfiguration.piraeus.io/custom-loader-image created
                        - button "Toggle word wrap" [ref=e430] [cursor=pointer]:
                          - img [ref=e431]
                  - listitem [ref=e433]:
                    - paragraph [ref=e434]:
                      - text: Wait for the
                      - code [ref=e435]: linstor-satellite.*
                      - text: pods to be recreated with the new image.
                  - listitem [ref=e436]:
                    - paragraph [ref=e437]:
                      - text: Verify that the
                      - code [ref=e438]: drbd-module-loader
                      - text: container is using the new image by describing the
                      - code [ref=e439]: linstor-satellite.*
                      - text: pods. Replace
                      - code [ref=e440]: <pod-name>
                      - text: with the name of the pod you want to check. You may need to issue
                      - code [ref=e441]: kubectl get pods --namespace piraeus-system
                      - text: first as the pod names will have changed.
                    - generic [ref=e443]:
                      - code [ref=e445]:
                        - generic [ref=e446]: kubectl describe pods <pod-name> --namespace piraeus-system
                      - button "Copy code to clipboard" [ref=e448] [cursor=pointer]:
                        - generic [ref=e449]:
                          - img [ref=e450]
                          - img [ref=e452]
                    - paragraph [ref=e454]:
                      - text: Look for events indicating that the
                      - code [ref=e455]: drbd-module-loader
                      - text: container is now using the
                      - code [ref=e456]: dbrd-loader:v2.8.1
                      - text: image.
                    - generic [ref=e457]:
                      - generic [ref=e458]: Example output
                      - generic [ref=e459]:
                        - code [ref=e461]:
                          - generic [ref=e462]: ...
                          - generic [ref=e463]: "Events:"
                          - generic [ref=e464]: Type Reason Age From Message
                          - generic [ref=e465]: "---- ------ ---- ---- -------"
                          - generic [ref=e466]: Normal Scheduled 4m44s default-scheduler Successfully assigned piraeus-system/linstor-satellite.edge-c0913842383ebd183d13d1458bb762c5-wfd4q to edge-c0913842383ebd183d13d1458bb762c5
                          - generic [ref=e467]: Normal Pulled 4m45s kubelet Container image "us-docker.pkg.dev/palette-images-fips/packs/piraeus-operator/2.8.1/dbrd-loader:v2.8.1" already present on machine
                          - generic [ref=e468]: "Normal Created 4m45s kubelet Created container: drbd-module-loader"
                          - generic [ref=e469]: Normal Started 4m44s kubelet Started container drbd-module-loader
                        - button "Toggle word wrap" [ref=e471] [cursor=pointer]:
                          - img [ref=e472]
                - paragraph [ref=e474]: The VerteX Management Appliance upgrade process will then continue. You can monitor the upgrade progress in Local UI.
                - heading "Scenario - Palette/VerteX Management Appliance Installation Stalled due to piraeus-operator Pack in Error StateDirect link to Scenario - Palette/VerteX Management Appliance Installation Stalled due to piraeus-operator Pack in Error State" [level=2] [ref=e475]:
                  - text: Scenario - Palette/VerteX Management Appliance Installation Stalled due to piraeus-operator Pack in Error State
                  - link "Direct link to Scenario - Palette/VerteX Management Appliance Installation Stalled due to piraeus-operator Pack in Error State" [ref=e476] [cursor=pointer]:
                    - /url: "#scenario---palettevertex-management-appliance-installation-stalled-due-to-piraeus-operator-pack-in-error-state"
                    - text: "#"
                - paragraph [ref=e477]:
                  - text: During the installation of the
                  - link "Palette" [ref=e478] [cursor=pointer]:
                    - /url: /enterprise-version/install-palette/palette-management-appliance/
                  - text: or
                  - link "VerteX Management Appliance" [ref=e479] [cursor=pointer]:
                    - /url: /vertex/install-palette-vertex/vertex-management-appliance/
                  - text: ", the"
                  - code [ref=e480]: piraeus-operator
                  - text: pack can enter an error state in Local UI. This can be caused by stalled creation of Kubernetes secrets in the
                  - code [ref=e481]: piraeus-system
                  - text: namespace and can prevent the installation from completing successfully.
                - paragraph [ref=e482]:
                  - text: To resolve, you can manually delete any secrets in the
                  - code [ref=e483]: piraeus-system
                  - text: namespace that have a
                  - code [ref=e484]: pending-install
                  - text: status label. This will allow the
                  - code [ref=e485]: piraeus-operator
                  - text: pack to complete its deployment and the Palette/VerteX Management Appliance installation to proceed.
                - heading "Debug StepsDirect link to Debug Steps" [level=3] [ref=e486]:
                  - text: Debug Steps
                  - link "Direct link to Debug Steps" [ref=e487] [cursor=pointer]:
                    - /url: "#debug-steps-1"
                    - text: "#"
                - list [ref=e488]:
                  - listitem [ref=e489]:
                    - paragraph [ref=e490]:
                      - text: Log in to the Local UI of the leader node of your Palette/VerteX management cluster. By default, Local UI is accessible at
                      - code [ref=e491]: https://<node-ip>:5080
                      - text: . Replace
                      - code [ref=e492]: <node-ip>
                      - text: with the IP address of the leader node.
                  - listitem [ref=e493]:
                    - paragraph [ref=e494]:
                      - text: From the left main menu, click
                      - strong [ref=e495]: Cluster
                      - text: .
                  - listitem [ref=e496]:
                    - paragraph [ref=e497]:
                      - text: Download the
                      - strong [ref=e498]: Admin Kubeconfig File
                      - text: by clicking on the
                      - code [ref=e499]: <cluster-name>.kubeconfig
                      - text: hyperlink.
                  - listitem [ref=e500]:
                    - paragraph [ref=e501]: Open a terminal session in an environment that has network access to the Palette/VerteX management cluster.
                  - listitem [ref=e502]:
                    - paragraph [ref=e503]:
                      - text: Issue the following command to set the
                      - code [ref=e504]: KUBECONFIG
                      - text: environment variable to the path of the kubeconfig file you downloaded in step 3.
                    - generic [ref=e506]:
                      - code [ref=e508]:
                        - generic [ref=e509]: export KUBECONFIG=<path-to-kubeconfig-file>
                      - button "Copy code to clipboard" [ref=e511] [cursor=pointer]:
                        - generic [ref=e512]:
                          - img [ref=e513]
                          - img [ref=e515]
                  - listitem [ref=e517]:
                    - paragraph [ref=e518]:
                      - text: Use
                      - code [ref=e519]: kubectl
                      - text: to list all secrets in the
                      - code [ref=e520]: piraeus-system
                      - text: namespace.
                    - generic [ref=e522]:
                      - code [ref=e524]:
                        - generic [ref=e525]: kubectl get secrets --namespace piraeus-system
                      - button "Copy code to clipboard" [ref=e527] [cursor=pointer]:
                        - generic [ref=e528]:
                          - img [ref=e529]
                          - img [ref=e531]
                    - generic [ref=e533]:
                      - generic [ref=e534]: Example output
                      - generic [ref=e535]:
                        - code [ref=e537]:
                          - generic [ref=e538]: NAME TYPE DATA AGE
                          - generic [ref=e539]: sh.helm.release.v1.piraeusoperator-linstor-gui.v1 helm.sh/release.v1 1 1h
                          - generic [ref=e540]: sh.helm.release.v1.piraeusoperator-linstor-gui.v2 helm.sh/release.v1 1 1h
                          - generic [ref=e541]: sh.helm.release.v1.piraeusoperator-piraeus-cluster.v1 helm.sh/release.v1 1 1h
                          - generic [ref=e542]: sh.helm.release.v1.piraeusoperator-piraeus-dashboard.v1 helm.sh/release.v1 1 1h
                          - generic [ref=e543]: sh.helm.release.v1.piraeusoperator-piraeus.v1 helm.sh/release.v1 1 1h
                          - generic [ref=e544]: sh.helm.release.v1.piraeusoperator-piraeus.v2 helm.sh/release.v1 1 1h
                        - button "Toggle word wrap" [ref=e546] [cursor=pointer]:
                          - img [ref=e547]
                  - listitem [ref=e549]:
                    - paragraph [ref=e550]:
                      - text: Use the following command to check each secret for a
                      - code [ref=e551]: pending-install
                      - text: status label. Replace
                      - code [ref=e552]: <secret-name>
                      - text: with the name of the secret you want to check.
                    - generic [ref=e554]:
                      - code [ref=e556]:
                        - generic [ref=e557]: kubectl describe secrets <secret-name> --namespace piraeus-system
                      - generic [ref=e558]:
                        - button "Toggle word wrap" [ref=e559] [cursor=pointer]:
                          - img [ref=e560]
                        - button "Copy code to clipboard" [ref=e562] [cursor=pointer]:
                          - generic [ref=e563]:
                            - img [ref=e564]
                            - img [ref=e566]
                    - generic [ref=e568]:
                      - generic [ref=e569]: Example output
                      - generic [ref=e570]:
                        - code [ref=e572]:
                          - generic [ref=e573]: "Name: sh.helm.release.v1.piraeusoperator-piraeus-cluster.v1"
                          - generic [ref=e574]: "Namespace: piraeus-system"
                          - generic [ref=e575]: "Labels: modifiedAt=0123456789"
                          - generic [ref=e576]: name=piraeusoperator-piraeus-cluster
                          - generic [ref=e577]: owner=helm
                          - generic [ref=e578]: status=pending-install
                          - generic [ref=e579]: version=1
                          - generic [ref=e580]: "Annotations: <none>"
                          - generic [ref=e581]: "Type: helm.sh/release.v1"
                          - generic [ref=e582]: Data
                          - generic [ref=e583]: ====
                          - generic [ref=e584]: "release: 7156 bytes"
                        - button "Toggle word wrap" [ref=e586] [cursor=pointer]:
                          - img [ref=e587]
                    - generic [ref=e589]:
                      - generic [ref=e590]:
                        - img [ref=e592]
                        - text: tip
                      - generic [ref=e594]:
                        - paragraph [ref=e595]:
                          - text: You can also try using the following command to filter for secrets with a
                          - code [ref=e596]: pending-install
                          - text: status label.
                        - generic [ref=e598]:
                          - code [ref=e600]:
                            - generic [ref=e601]: kubectl describe secrets --namespace piraeus-system --selector status=pending-install
                          - generic [ref=e602]:
                            - button "Toggle word wrap" [ref=e603] [cursor=pointer]:
                              - img [ref=e604]
                            - button "Copy code to clipboard" [ref=e606] [cursor=pointer]:
                              - generic [ref=e607]:
                                - img [ref=e608]
                                - img [ref=e610]
                  - listitem [ref=e612]:
                    - paragraph [ref=e613]:
                      - text: If you find any secrets with this label, delete them using the following command. Replace
                      - code [ref=e614]: <secret-name>
                      - text: with the name of the secret you want to delete.
                    - generic [ref=e616]:
                      - code [ref=e618]:
                        - generic [ref=e619]: kubectl delete secrets <secret-name> --namespace piraeus-system
                      - button "Copy code to clipboard" [ref=e621] [cursor=pointer]:
                        - generic [ref=e622]:
                          - img [ref=e623]
                          - img [ref=e625]
                  - listitem [ref=e627]:
                    - paragraph [ref=e628]:
                      - text: After deleting any secrets with a
                      - code [ref=e629]: pending-install
                      - text: status label, wait for the
                      - code [ref=e630]: piraeus-operator
                      - text: pack to enter a
                      - strong [ref=e631]: Running
                      - text: status in Local UI. The installation of Palette/VerteX Management Appliance should then proceed successfully.
                - heading "Scenario - Unexpected Logouts in Tenant Console After Palette/VerteX Management Appliance InstallationDirect link to Scenario - Unexpected Logouts in Tenant Console After Palette/VerteX Management Appliance Installation" [level=2] [ref=e632]:
                  - text: Scenario - Unexpected Logouts in Tenant Console After Palette/VerteX Management Appliance Installation
                  - link "Direct link to Scenario - Unexpected Logouts in Tenant Console After Palette/VerteX Management Appliance Installation" [ref=e633] [cursor=pointer]:
                    - /url: "#scenario---unexpected-logouts-in-tenant-console-after-palettevertex-management-appliance-installation"
                    - text: "#"
                - paragraph [ref=e634]:
                  - text: After installing self-hosted Palette/Palette VerteX using the
                  - link "Palette Management Appliance" [ref=e635] [cursor=pointer]:
                    - /url: /enterprise-version/install-palette/palette-management-appliance/
                  - text: or
                  - link "VerteX Management Appliance" [ref=e636] [cursor=pointer]:
                    - /url: /vertex/install-palette-vertex/vertex-management-appliance/
                  - text: ", you may experience unexpected logouts when using the tenant console. This can be caused by a time skew on your Palette/VerteX management cluster nodes, which leads to authentication issues."
                - paragraph [ref=e637]: To verify the system time, open a terminal session on each node in your Palette/VerteX management cluster and issue the following command to check the system time.
                - generic [ref=e639]:
                  - code [ref=e641]:
                    - generic [ref=e642]: timedatectl
                  - button "Copy code to clipboard" [ref=e644] [cursor=pointer]:
                    - generic [ref=e645]:
                      - img [ref=e646]
                      - img [ref=e648]
                - paragraph [ref=e650]:
                  - text: An output similar to the following will be displayed. A time skew is indicated by the
                  - code [ref=e651]: Local time
                  - text: and
                  - code [ref=e652]: Universal time
                  - text: values being different across the nodes.
                - generic [ref=e653]:
                  - tablist [ref=e654]:
                    - tab "Example output from node 1" [selected] [ref=e655] [cursor=pointer]
                    - tab "Example output from node 2" [ref=e656] [cursor=pointer]
                    - tab "Example output from node 3" [ref=e657] [cursor=pointer]
                  - tabpanel [ref=e659]:
                    - code [ref=e663]:
                      - generic [ref=e664]: "Local time: Fri 2025-07-11 09:41:42 UTC"
                      - generic [ref=e665]: "Universal time: Fri 2025-07-11 09:41:42 UTC"
                      - generic [ref=e666]: "RTC time: Fri 2025-07-11 09:41:42"
                      - generic [ref=e667]: "Time zone: UTC (UTC, +0000)"
                      - generic [ref=e668]: "System clock synchronized: no"
                      - generic [ref=e669]: "NTP service: active"
                      - generic [ref=e670]: "RTC in local TZ: no"
                - paragraph [ref=e671]:
                  - text: You may also notice errors in the
                  - code [ref=e672]: auth-*
                  - text: pod logs in the
                  - code [ref=e673]: hubble-system
                  - text: namespace similar to the following.
                - generic [ref=e674]:
                  - generic [ref=e675]: Example command to extract logs from auth pod
                  - generic [ref=e676]:
                    - code [ref=e678]:
                      - generic [ref=e679]: kubectl logs --namespace hubble-system auth-5f95c77cb-49jtv
                    - button "Copy code to clipboard" [ref=e681] [cursor=pointer]:
                      - generic [ref=e682]:
                        - img [ref=e683]
                        - img [ref=e685]
                - generic [ref=e687]:
                  - generic [ref=e688]: Example output
                  - generic [ref=e689]:
                    - code [ref=e691]:
                      - generic [ref=e692]: auth-5f95c77cb-49jtv Jul 7 17:22:46.378 ERROR [hubble_token.go:426/hucontext.getClaimsFromToken] [Unable to parse the token 'abcd...1234' due to Token used before
                      - generic [ref=e693]: issued]
                      - generic [ref=e694]: auth-5f95c77cb-49jtv Jul 7 17:22:46.378 ERROR [auth_service.go:282/service.(*AuthService).Logout] [provided token 'xxxxx' is not valid Token used before issued]
                    - button "Toggle word wrap" [ref=e696] [cursor=pointer]:
                      - img [ref=e697]
                - paragraph [ref=e699]: This indicates that the system time on your Palette/VerteX management cluster nodes is not synchronized with a Network Time Protocol (NTP) server. To resolve this issue, you can configure an NTP server in the Palette/VerteX management cluster settings.
                - heading "Debug StepsDirect link to Debug Steps" [level=3] [ref=e700]:
                  - text: Debug Steps
                  - link "Direct link to Debug Steps" [ref=e701] [cursor=pointer]:
                    - /url: "#debug-steps-2"
                    - text: "#"
                - list [ref=e702]:
                  - listitem [ref=e703]:
                    - paragraph [ref=e704]: Log in to Local UI of the leader node of your Palette/VerteX management cluster.
                  - listitem [ref=e705]:
                    - paragraph [ref=e706]:
                      - text: On the left main menu, click
                      - strong [ref=e707]: Cluster
                      - text: .
                  - listitem [ref=e708]:
                    - paragraph [ref=e709]:
                      - text: Click
                      - strong [ref=e710]: Actions
                      - text: in the top-right corner and select
                      - strong [ref=e711]: Cluster Settings
                      - text: from the drop-down menu.
                  - listitem [ref=e712]:
                    - paragraph [ref=e713]:
                      - text: In the
                      - strong [ref=e714]: Network Time Protocol (NTP) (Optional)
                      - text: field, enter the NTP server that you want to use for your Palette/VerteX management cluster. For example, you can use
                      - code [ref=e715]: pool.ntp.org
                      - text: or any other NTP server that is accessible from your Palette/VerteX management cluster nodes.
                  - listitem [ref=e716]:
                    - paragraph [ref=e717]:
                      - text: Click
                      - strong [ref=e718]: Save Changes
                      - text: to apply the changes.
                - heading "Scenario - IP Pool Exhausted During Airgapped UpgradeDirect link to Scenario - IP Pool Exhausted During Airgapped Upgrade" [level=2] [ref=e719]:
                  - text: Scenario - IP Pool Exhausted During Airgapped Upgrade
                  - link "Direct link to Scenario - IP Pool Exhausted During Airgapped Upgrade" [ref=e720] [cursor=pointer]:
                    - /url: "#scenario---ip-pool-exhausted-during-airgapped-upgrade"
                    - text: "#"
                - paragraph [ref=e721]:
                  - text: When upgrading a self-hosted airgapped cluster to version 4.6.32, the IPAM controller may report an
                  - code [ref=e722]: Exhausted IP Pools
                  - text: error despite having available IP addresses. This is due to a race condition in CAPV version 1.12.0, which may lead to an orphaned IP claim when its associated VMware vSphere machine is deleted during the control plane rollout. When this occurs, the IP claim and IP address are not cleaned up, keeping the IP reserved and exhausting the IP pool. To complete the upgrade, you must manually release the orphaned claim holding the IP address.
                - heading "Debug StepsDirect link to Debug Steps" [level=3] [ref=e723]:
                  - text: Debug Steps
                  - link "Direct link to Debug Steps" [ref=e724] [cursor=pointer]:
                    - /url: "#debug-steps-3"
                    - text: "#"
                - list [ref=e725]:
                  - listitem [ref=e726]:
                    - paragraph [ref=e727]:
                      - text: Open up a terminal session in an environment that has network access to the cluster. Refer to
                      - link "Access Cluster with CLI" [ref=e728] [cursor=pointer]:
                        - /url: /clusters/cluster-management/palette-webctl/
                      - text: for additional guidance.
                  - listitem [ref=e729]:
                    - paragraph [ref=e730]: Issue the following command to list the IP addresses of the current nodes in the cluster.
                    - generic [ref=e731]:
                      - generic [ref=e732]:
                        - img [ref=e734]
                        - text: info
                      - paragraph [ref=e737]: The airgap support VM is not listed, only the nodes in the cluster.
                    - generic [ref=e739]:
                      - code [ref=e741]:
                        - generic [ref=e742]: kubectl get nodes \
                        - generic [ref=e743]: "--output jsonpath='{range .items[*]}{.status.addresses[?(@.type==\"InternalIP\")].address}{\"\\n\"}{end}'"
                      - generic [ref=e744]:
                        - button "Toggle word wrap" [ref=e745] [cursor=pointer]:
                          - img [ref=e746]
                        - button "Copy code to clipboard" [ref=e748] [cursor=pointer]:
                          - generic [ref=e749]:
                            - img [ref=e750]
                            - img [ref=e752]
                    - generic [ref=e754]:
                      - generic [ref=e755]: Example output
                      - code [ref=e758]:
                        - generic [ref=e759]: 10.10.227.13
                        - generic [ref=e760]: 10.10.227.11
                        - generic [ref=e761]: 10.10.227.14
                  - listitem [ref=e762]:
                    - paragraph [ref=e763]:
                      - text: List all IP claims in the
                      - code [ref=e764]: spectro-mgmt
                      - text: namespace. The base
                      - code [ref=e765]: spectro-mgmt-cluster
                      - text: claim belongs to the airgap support VM.
                    - generic [ref=e767]:
                      - code [ref=e769]:
                        - generic [ref=e770]: kubectl get ipclaim --namespace spectro-mgmt
                      - button "Copy code to clipboard" [ref=e772] [cursor=pointer]:
                        - generic [ref=e773]:
                          - img [ref=e774]
                          - img [ref=e776]
                    - generic [ref=e778]:
                      - generic [ref=e779]: Example output
                      - code [ref=e782]:
                        - generic [ref=e783]: NAMESPACE NAME AGE
                        - generic [ref=e784]: spectro-mgmt spectro-mgmt-cluster 29h
                        - generic [ref=e785]: spectro-mgmt spectro-mgmt-cluster-cp-43978-dw858-0 14h
                        - generic [ref=e786]: spectro-mgmt spectro-mgmt-cluster-cp-43978-p2bpg-0 29h
                        - generic [ref=e787]: spectro-mgmt spectro-mgmt-cluster-cp-dt44d-0 14h
                        - generic [ref=e788]: spectro-mgmt spectro-mgmt-cluster-cp-qx4vw-0 6m
                  - listitem [ref=e789]:
                    - paragraph [ref=e790]: Map each claim to its allocated IP.
                    - generic [ref=e792]:
                      - code [ref=e794]:
                        - generic [ref=e795]: kubectl get ipclaim --namespace spectro-mgmt \
                        - generic [ref=e796]: "--output jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.address.name}{\"\\n\"}{end}'"
                      - generic [ref=e797]:
                        - button "Toggle word wrap" [ref=e798] [cursor=pointer]:
                          - img [ref=e799]
                        - button "Copy code to clipboard" [ref=e801] [cursor=pointer]:
                          - generic [ref=e802]:
                            - img [ref=e803]
                            - img [ref=e805]
                    - paragraph [ref=e807]:
                      - text: Compare the IP addresses of the nodes in the cluster to the IP addresses in the claim list, ignoring the
                      - code [ref=e808]: spectro-mgmt-cluster
                      - text: claim of the airgap support VM. The IP that appears in the claim list that does not appear in the node list is the orphaned claim. In the below example, the orphaned claim is
                      - code [ref=e809]: spectro-mgmt-cluster-cp-qx4vw-0
                      - text: ", which is tied to the IP address 10.10.227.12 ("
                      - code [ref=e810]: spectro-mgmt-cluster-cluster1-10-10-227-12
                      - text: ).
                    - generic [ref=e811]:
                      - generic [ref=e812]: Example output
                      - generic [ref=e813]:
                        - code [ref=e815]:
                          - generic [ref=e816]: spectro-mgmt-cluster spectro-mgmt-cluster-cluster1-10-10-227-10
                          - generic [ref=e817]: spectro-mgmt-cluster-cp-43978-dw858-0 spectro-mgmt-cluster-cluster1-10-10-227-14
                          - generic [ref=e818]: spectro-mgmt-cluster-cp-43978-p2bpg-0 spectro-mgmt-cluster-cluster1-10-10-227-13
                          - generic [ref=e819]: spectro-mgmt-cluster-cp-dt44d-0 spectro-mgmt-cluster-cluster1-10-10-227-11
                          - generic [ref=e820]: spectro-mgmt-cluster-cp-qx4vw-0 spectro-mgmt-cluster-cluster1-10-10-227-12
                        - button "Toggle word wrap" [ref=e822] [cursor=pointer]:
                          - img [ref=e823]
                  - listitem [ref=e825]:
                    - paragraph [ref=e826]: Delete the orphaned claim.
                    - generic [ref=e828]:
                      - code [ref=e830]:
                        - generic [ref=e831]: kubectl delete ipclaim --namespace spectro-mgmt <claim-name>
                      - button "Copy code to clipboard" [ref=e833] [cursor=pointer]:
                        - generic [ref=e834]:
                          - img [ref=e835]
                          - img [ref=e837]
                    - generic [ref=e839]:
                      - generic [ref=e840]: Example command
                      - generic [ref=e841]:
                        - code [ref=e843]:
                          - generic [ref=e844]: kubectl delete ipclaim --namespace spectro-mgmt spectro-mgmt-cluster-cp-qx4vw-0
                        - button "Toggle word wrap" [ref=e846] [cursor=pointer]:
                          - img [ref=e847]
                  - listitem [ref=e849]:
                    - paragraph [ref=e850]:
                      - text: Re-run the upgrade. For guidance, refer to the applicable upgrade guide for your airgapped instance of
                      - link "Palette" [ref=e851] [cursor=pointer]:
                        - /url: /enterprise-version/upgrade/upgrade-vmware/airgap/
                      - text: or
                      - link "VerteX" [ref=e852] [cursor=pointer]:
                        - /url: /vertex/upgrade/upgrade-vmware/airgap/
                      - text: .
                - heading "Scenario - Self-Linking ErrorDirect link to Scenario - Self-Linking Error" [level=2] [ref=e853]:
                  - text: Scenario - Self-Linking Error
                  - link "Direct link to Scenario - Self-Linking Error" [ref=e854] [cursor=pointer]:
                    - /url: "#scenario---self-linking-error"
                    - text: "#"
                - paragraph [ref=e855]: When installing an Enterprise Cluster, you may encounter an error stating that the enterprise cluster is unable to self-link. Self-linking is the process of Palette or VerteX becoming aware of the Kubernetes cluster it is installed on. This error may occur if the self-hosted pack registry specified in the installation is missing the Certificate Authority (CA). This issue can be resolved by adding the CA to the pack registry.
                - heading "Debug StepsDirect link to Debug Steps" [level=3] [ref=e856]:
                  - text: Debug Steps
                  - link "Direct link to Debug Steps" [ref=e857] [cursor=pointer]:
                    - /url: "#debug-steps-4"
                    - text: "#"
                - list [ref=e858]:
                  - listitem [ref=e859]:
                    - paragraph [ref=e860]: Log in to the pack registry server that you specified in the Palette or VerteX installation.
                  - listitem [ref=e861]:
                    - paragraph [ref=e862]:
                      - text: Download the CA certificate from the pack registry server. Different OCI registries have different methods for downloading the CA certificate. For Harbor, check out the
                      - link "Download the Harbor Certificate" [ref=e863] [cursor=pointer]:
                        - /url: https://goharbor.io/docs/1.10/working-with-projects/working-with-images/pulling-pushing-images/#download-the-harbor-certificate
                      - text: guide.
                  - listitem [ref=e864]:
                    - paragraph [ref=e865]:
                      - text: Log in to the system console. Refer to
                      - link "Access Palette system console" [ref=e866] [cursor=pointer]:
                        - /url: /enterprise-version/system-management/#access-the-system-console
                      - text: or
                      - link "Access Vertex system console" [ref=e867] [cursor=pointer]:
                        - /url: /vertex/system-management/#access-the-system-console
                      - text: for additional guidance.
                  - listitem [ref=e868]:
                    - paragraph [ref=e869]:
                      - text: From the left navigation menu, select
                      - strong [ref=e870]: Administration
                      - text: and click on the
                      - strong [ref=e871]: Pack Registries
                      - text: tab.
                  - listitem [ref=e872]:
                    - paragraph [ref=e873]:
                      - text: Click on the
                      - strong [ref=e874]: three-dot Menu
                      - text: icon for the pack registry that you specified in the installation and select
                      - strong [ref=e875]: Edit
                      - text: .
                  - listitem [ref=e876]:
                    - paragraph [ref=e877]:
                      - text: Click on the
                      - strong [ref=e878]: Upload file
                      - text: button and upload the CA certificate that you downloaded in step 2.
                  - listitem [ref=e879]:
                    - paragraph [ref=e880]:
                      - text: Check the box
                      - strong [ref=e881]: Insecure Skip TLS Verify
                      - text: and click on
                      - strong [ref=e882]: Confirm
                      - text: .
                - paragraph [ref=e883]:
                  - img "A pack registry configuration screen." [ref=e884]
                - paragraph [ref=e885]:
                  - text: After a few moments, a system profile will be created and Palette or VerteX will be able to self-link successfully. If you continue to encounter issues, contact our support team by emailing
                  - link "support@spectrocloud.com" [ref=e886] [cursor=pointer]:
                    - /url: mailto:support@spectrocloud.com
                  - text: so that we can provide you with further guidance.
                - heading "Scenario - Enterprise Backup StuckDirect link to Scenario - Enterprise Backup Stuck" [level=2] [ref=e887]:
                  - text: Scenario - Enterprise Backup Stuck
                  - link "Direct link to Scenario - Enterprise Backup Stuck" [ref=e888] [cursor=pointer]:
                    - /url: "#scenario---enterprise-backup-stuck"
                    - text: "#"
                - paragraph [ref=e889]: In the scenario where an enterprise backup is stuck, a restart of the management pod may resolve the issue. Use the following steps to restart the management pod.
                - heading "Debug StepsDirect link to Debug Steps" [level=3] [ref=e890]:
                  - text: Debug Steps
                  - link "Direct link to Debug Steps" [ref=e891] [cursor=pointer]:
                    - /url: "#debug-steps-5"
                    - text: "#"
                - list [ref=e892]:
                  - listitem [ref=e893]:
                    - paragraph [ref=e894]:
                      - text: Open up a terminal session in an environment that has network access to the Kubernetes cluster. Refer to the
                      - link "Access Cluster with CLI" [ref=e895] [cursor=pointer]:
                        - /url: /clusters/cluster-management/palette-webctl/
                      - text: for additional guidance.
                  - listitem [ref=e896]:
                    - paragraph [ref=e897]:
                      - text: Identify the
                      - code [ref=e898]: mgmt
                      - text: pod in the
                      - code [ref=e899]: hubble-system
                      - text: namespace. Use the following command to list all pods in the
                      - code [ref=e900]: hubble-system
                      - text: namespace and filter for the
                      - code [ref=e901]: mgmt
                      - text: pod.
                    - generic [ref=e903]:
                      - code [ref=e905]:
                        - generic [ref=e906]: kubectl get pods --namespace hubble-system | grep mgmt
                      - button "Copy code to clipboard" [ref=e908] [cursor=pointer]:
                        - generic [ref=e909]:
                          - img [ref=e910]
                          - img [ref=e912]
                    - generic [ref=e915]:
                      - code [ref=e917]:
                        - generic [ref=e918]: mgmt-f7f97f4fd-lds69 1/1 Running 0 45m
                      - button "Toggle word wrap" [ref=e920] [cursor=pointer]:
                        - img [ref=e921]
                  - listitem [ref=e923]:
                    - paragraph [ref=e924]:
                      - text: Restart the
                      - code [ref=e925]: mgmt
                      - text: pod by deleting it. Use the following command to delete the
                      - code [ref=e926]: mgmt
                      - text: pod. Replace
                      - code [ref=e927]: <mgmt-pod-name>
                      - text: with the actual name of the
                      - code [ref=e928]: mgmt
                      - text: pod that you identified in step 2.
                    - generic [ref=e930]:
                      - code [ref=e932]:
                        - generic [ref=e933]: kubectl delete pod <mgmt-pod-name> --namespace hubble-system
                      - button "Copy code to clipboard" [ref=e935] [cursor=pointer]:
                        - generic [ref=e936]:
                          - img [ref=e937]
                          - img [ref=e939]
                    - code [ref=e944]:
                      - generic [ref=e945]: pod "mgmt-f7f97f4fd-lds69" deleted
                - heading "Scenario - Non-Unique vSphere CNS MappingDirect link to Scenario - Non-Unique vSphere CNS Mapping" [level=2] [ref=e946]:
                  - text: Scenario - Non-Unique vSphere CNS Mapping
                  - link "Direct link to Scenario - Non-Unique vSphere CNS Mapping" [ref=e947] [cursor=pointer]:
                    - /url: "#scenario---non-unique-vsphere-cns-mapping"
                    - text: "#"
                - paragraph [ref=e948]: In Palette and VerteX releases 4.4.8 and earlier, Persistent Volume Claims (PVCs) metadata do not use a unique identifier for self-hosted Palette clusters. This causes incorrect Cloud Native Storage (CNS) mappings in vSphere, potentially leading to issues during node operations and upgrades.
                - paragraph [ref=e949]: This issue is resolved in Palette and VerteX releases starting with 4.4.14. However, upgrading to 4.4.14 will not automatically resolve this issue. If you have self-hosted instances of Palette in your vSphere environment older than 4.4.14, you should execute the following utility script manually to make the CNS mapping unique for the associated PVC.
                - heading "Debug StepsDirect link to Debug Steps" [level=3] [ref=e950]:
                  - text: Debug Steps
                  - link "Direct link to Debug Steps" [ref=e951] [cursor=pointer]:
                    - /url: "#debug-steps-6"
                    - text: "#"
                - list [ref=e952]:
                  - listitem [ref=e953]:
                    - paragraph [ref=e954]:
                      - text: Ensure your machine has network access to your self-hosted Palette instance with
                      - code [ref=e955]: kubectl
                      - text: . Alternatively, establish an SSH connection to a machine where you can access your self-hosted Palette instance with
                      - code [ref=e956]: kubectl
                      - text: .
                  - listitem [ref=e957]:
                    - paragraph [ref=e958]: Log in to your self-hosted Palette instance System Console.
                  - listitem [ref=e959]:
                    - paragraph [ref=e960]:
                      - text: In the
                      - strong [ref=e961]: Main Menu
                      - text: ", click"
                      - strong [ref=e962]: Enterprise Cluster
                      - text: .
                  - listitem [ref=e963]:
                    - paragraph [ref=e964]:
                      - text: In the cluster details page, scroll down to the
                      - strong [ref=e965]: Kubernetes Config File
                      - text: field and download the kubeconfig file.
                  - listitem [ref=e966]:
                    - paragraph [ref=e967]: Issue the following command to download the utility script.
                    - generic [ref=e969]:
                      - code [ref=e971]:
                        - generic [ref=e972]: curl --output csi-helper https://software.spectrocloud.com/tools/csi-helper/csi-helper
                      - generic [ref=e973]:
                        - button "Toggle word wrap" [ref=e974] [cursor=pointer]:
                          - img [ref=e975]
                        - button "Copy code to clipboard" [ref=e977] [cursor=pointer]:
                          - generic [ref=e978]:
                            - img [ref=e979]
                            - img [ref=e981]
                  - listitem [ref=e983]:
                    - paragraph [ref=e984]: Adjust the permission of the script.
                    - generic [ref=e986]:
                      - code [ref=e988]:
                        - generic [ref=e989]: chmod +x csi-helper
                      - button "Copy code to clipboard" [ref=e991] [cursor=pointer]:
                        - generic [ref=e992]:
                          - img [ref=e993]
                          - img [ref=e995]
                  - listitem [ref=e997]:
                    - paragraph [ref=e998]: Issue the following command to execute the utility script. Replace the placeholder with the path to your kubeconfig file.
                    - generic [ref=e1000]:
                      - code [ref=e1002]:
                        - generic [ref=e1003]: ./csi-helper --kubeconfig=<PATH_TO_KUBECONFIG>
                      - button "Copy code to clipboard" [ref=e1005] [cursor=pointer]:
                        - generic [ref=e1006]:
                          - img [ref=e1007]
                          - img [ref=e1009]
                  - listitem [ref=e1011]:
                    - paragraph [ref=e1012]: Issue the following command to verify that the script has updated the cluster ID.
                    - generic [ref=e1014]:
                      - code [ref=e1016]:
                        - generic [ref=e1017]: kubectl describe configmap vsphere-cloud-config --namespace=kube-system
                      - generic [ref=e1018]:
                        - button "Toggle word wrap" [ref=e1019] [cursor=pointer]:
                          - img [ref=e1020]
                        - button "Copy code to clipboard" [ref=e1022] [cursor=pointer]:
                          - generic [ref=e1023]:
                            - img [ref=e1024]
                            - img [ref=e1026]
                    - paragraph [ref=e1028]:
                      - text: If the update is successful, the cluster ID in the ConfigMap will have a unique ID assigned instead of
                      - code [ref=e1029]: spectro-mgmt/spectro-mgmt-cluster
                      - text: .
                    - code [ref=e1033]:
                      - generic [ref=e1034]: "Name: vsphere-cloud-config"
                      - generic [ref=e1035]: "Namespace: kube-system"
                      - generic [ref=e1036]: "Labels: component=cloud-controller-manager"
                      - generic [ref=e1037]: vsphere-cpi-infra=config
                      - generic [ref=e1038]: "Annotations: cluster.spectrocloud.com/last-applied-hash: 17721994478134573986"
                      - generic [ref=e1039]: Data
                      - generic [ref=e1040]: ====
                      - generic [ref=e1041]: "vsphere.conf:"
                      - generic [ref=e1042]: "----"
                      - generic [ref=e1043]: "[Global]"
                      - generic [ref=e1044]: cluster-id = "896d25b9-bfac-414f-bb6f-52fd469d3a6c/spectro-mgmt-cluster"
                      - generic [ref=e1045]: "[VirtualCenter \"vcenter.spectrocloud.dev\"]"
                      - generic [ref=e1046]: insecure-flag = "true"
                      - generic [ref=e1047]: user = "example@vsphere.local"
                      - generic [ref=e1048]: password = "************"
                      - generic [ref=e1049]: "[Labels]"
                      - generic [ref=e1050]: zone = "k8s-zone"
                      - generic [ref=e1051]: region = "k8s-region"
                      - generic [ref=e1052]: BinaryData
                      - generic [ref=e1053]: ====
                      - generic [ref=e1054]: "Events: <none>"
                - heading "Scenario - \"Too Many Open Files\" in ClusterDirect link to Scenario - \"Too Many Open Files\" in Cluster" [level=2] [ref=e1055]:
                  - text: Scenario - "Too Many Open Files" in Cluster
                  - link "Direct link to Scenario - \"Too Many Open Files\" in Cluster" [ref=e1056] [cursor=pointer]:
                    - /url: "#scenario---too-many-open-files-in-cluster"
                    - text: "#"
                - paragraph [ref=e1057]:
                  - text: When viewing logs for Enterprise or
                  - link "Private Cloud Gateway" [ref=e1058] [cursor=pointer]:
                    - /url: /clusters/pcg/
                  - text: clusters, you may encounter a "too many open files" error, which prevents logs from tailing after a certain point. To resolve this issue, you must increase the maximum number of file descriptors for each node on your cluster.
                - heading "Debug StepsDirect link to Debug Steps" [level=3] [ref=e1059]:
                  - text: Debug Steps
                  - link "Direct link to Debug Steps" [ref=e1060] [cursor=pointer]:
                    - /url: "#debug-steps-7"
                    - text: "#"
                - paragraph [ref=e1061]: Repeat the following process for each node in your cluster.
                - list [ref=e1062]:
                  - listitem [ref=e1063]:
                    - paragraph [ref=e1064]: Log in to a node in your cluster.
                    - generic [ref=e1066]:
                      - code [ref=e1068]:
                        - generic [ref=e1069]: ssh -i <key-name> <spectro@hostname>
                      - button "Copy code to clipboard" [ref=e1071] [cursor=pointer]:
                        - generic [ref=e1072]:
                          - img [ref=e1073]
                          - img [ref=e1075]
                  - listitem [ref=e1077]:
                    - paragraph [ref=e1078]:
                      - text: Switch to
                      - code [ref=e1079]: sudo
                      - text: mode using the command that best fits your system and preferences.
                    - generic [ref=e1081]:
                      - code [ref=e1083]:
                        - generic [ref=e1084]: sudo --login
                      - button "Copy code to clipboard" [ref=e1086] [cursor=pointer]:
                        - generic [ref=e1087]:
                          - img [ref=e1088]
                          - img [ref=e1090]
                  - listitem [ref=e1092]:
                    - paragraph [ref=e1093]: Increase the maximum number of file descriptors that the kernel can allocate system-wide.
                    - generic [ref=e1095]:
                      - code [ref=e1097]:
                        - generic [ref=e1098]: echo "fs.file-max = 1000000" > /etc/sysctl.d/99-maxfiles.conf
                      - button "Copy code to clipboard" [ref=e1100] [cursor=pointer]:
                        - generic [ref=e1101]:
                          - img [ref=e1102]
                          - img [ref=e1104]
                  - listitem [ref=e1106]:
                    - paragraph [ref=e1107]:
                      - text: Apply the updated
                      - code [ref=e1108]: sysctl
                      - text: settings. The increased limit is returned.
                    - generic [ref=e1110]:
                      - code [ref=e1112]:
                        - generic [ref=e1113]: sysctl -p /etc/sysctl.d/99-maxfiles.conf
                      - button "Copy code to clipboard" [ref=e1115] [cursor=pointer]:
                        - generic [ref=e1116]:
                          - img [ref=e1117]
                          - img [ref=e1119]
                    - code [ref=e1124]:
                      - generic [ref=e1125]: fs.file-max = 1000000
                  - listitem [ref=e1126]:
                    - paragraph [ref=e1127]:
                      - text: Restart the
                      - code [ref=e1128]: kubelet
                      - text: and
                      - code [ref=e1129]: containerd
                      - text: services.
                    - generic [ref=e1131]:
                      - code [ref=e1133]:
                        - generic [ref=e1134]: systemctl restart kubelet containerd
                      - button "Copy code to clipboard" [ref=e1136] [cursor=pointer]:
                        - generic [ref=e1137]:
                          - img [ref=e1138]
                          - img [ref=e1140]
                  - listitem [ref=e1142]:
                    - paragraph [ref=e1143]: Confirm that the change was applied.
                    - generic [ref=e1145]:
                      - code [ref=e1147]:
                        - generic [ref=e1148]: sysctl fs.file-max
                      - button "Copy code to clipboard" [ref=e1150] [cursor=pointer]:
                        - generic [ref=e1151]:
                          - img [ref=e1152]
                          - img [ref=e1154]
                    - code [ref=e1159]:
                      - generic [ref=e1160]: fs.file-max = 1000000
                - heading "Scenario - MAAS and VMware vSphere Clusters Fail Image Resolution in Non-Airgap EnvironmentsDirect link to Scenario - MAAS and VMware vSphere Clusters Fail Image Resolution in Non-Airgap Environments" [level=2] [ref=e1161]:
                  - text: Scenario - MAAS and VMware vSphere Clusters Fail Image Resolution in Non-Airgap Environments
                  - link "Direct link to Scenario - MAAS and VMware vSphere Clusters Fail Image Resolution in Non-Airgap Environments" [ref=e1162] [cursor=pointer]:
                    - /url: "#scenario---maas-and-vmware-vsphere-clusters-fail-image-resolution-in-non-airgap-environments"
                    - text: "#"
                - paragraph [ref=e1163]: In Palette or VerteX non-airgap installations with versions 4.2.13 to 4.5.22, MAAS and VMware vSphere clusters may fail to provision due to image resolution errors. These environments have incorrectly configured default image endpoints. To resolve this issue, you must manually set these endpoints.
                - heading "Debug StepsDirect link to Debug Steps" [level=3] [ref=e1164]:
                  - text: Debug Steps
                  - link "Direct link to Debug Steps" [ref=e1165] [cursor=pointer]:
                    - /url: "#debug-steps-8"
                    - text: "#"
                - list [ref=e1166]:
                  - listitem [ref=e1167]:
                    - paragraph [ref=e1168]: Open a terminal with connectivity to your self-hosted environment.
                  - listitem [ref=e1169]:
                    - paragraph [ref=e1170]:
                      - text: Execute the following command to save the base URL of your Palette instance API to the
                      - code [ref=e1171]: BASE_URL
                      - text: environment value. Add your correct URL in place of
                      - code [ref=e1172]: REPLACE_ME
                      - text: .
                    - generic [ref=e1174]:
                      - code [ref=e1176]:
                        - generic [ref=e1177]: export BASE_URL="REPLACE ME"
                      - button "Copy code to clipboard" [ref=e1179] [cursor=pointer]:
                        - generic [ref=e1180]:
                          - img [ref=e1181]
                          - img [ref=e1183]
                  - listitem [ref=e1185]:
                    - paragraph [ref=e1186]:
                      - text: Use the following command to log in to the Palette System API by using the
                      - code [ref=e1187]: /v1/auth/syslogin
                      - text: endpoint. Ensure you replace the credentials below with your system console credentials.
                    - generic [ref=e1189]:
                      - code [ref=e1191]:
                        - generic [ref=e1192]: curl --location '$BASE_URL/v1/auth/syslogin' \
                        - generic [ref=e1193]: "--header 'Content-Type: application/json' \\"
                        - generic [ref=e1194]: "--data '{"
                        - generic [ref=e1195]: "\"password\": \"**********\","
                        - generic [ref=e1196]: "\"username\": \"**********\""
                        - generic [ref=e1197]: "}'"
                      - button "Copy code to clipboard" [ref=e1199] [cursor=pointer]:
                        - generic [ref=e1200]:
                          - img [ref=e1201]
                          - img [ref=e1203]
                    - paragraph [ref=e1205]: The output displays the authorization token.
                    - code [ref=e1209]:
                      - generic [ref=e1210]: "{"
                      - generic [ref=e1211]: "\"Authorization\": \"**********.\","
                      - generic [ref=e1212]: "\"IsPasswordReset\": true"
                      - generic [ref=e1213]: "}"
                  - listitem [ref=e1214]:
                    - paragraph [ref=e1215]: Copy the authorization token to your clipboard and assign it to an environment variable. Replace the placeholder below with the value from the output.
                    - code [ref=e1219]:
                      - generic [ref=e1220]: export TOKEN=**********
                  - listitem [ref=e1221]:
                    - paragraph [ref=e1222]:
                      - text: Execute the following command to set the MAAS image endpoint to
                      - code [ref=e1223]: https://maasgoldenimage.s3.amazonaws.com
                      - text: . Replace the
                      - code [ref=e1224]: caCert
                      - text: value below with the Certificate Authority (CA) certificate for your self-hosted environment.
                    - generic [ref=e1226]:
                      - code [ref=e1228]:
                        - generic [ref=e1229]: curl --request PUT '$BASE_URL/v1/system/config/maas/image' \
                        - generic [ref=e1230]: "--header 'Authorization: $TOKEN' \\"
                        - generic [ref=e1231]: "--header 'Content-Type: application/json' \\"
                        - generic [ref=e1232]: "--data '{"
                        - generic [ref=e1233]: "\"spec\": {"
                        - generic [ref=e1234]: "\"imagesHostEndpoint\": \"https://maasgoldenimage.s3.amazonaws.com\","
                        - generic [ref=e1235]: "\"insecureSkipVerify\": false,"
                        - generic [ref=e1236]: "\"caCert\": \"**********\""
                        - generic [ref=e1237]: "}"
                        - generic [ref=e1238]: "}'"
                      - generic [ref=e1239]:
                        - button "Toggle word wrap" [ref=e1240] [cursor=pointer]:
                          - img [ref=e1241]
                        - button "Copy code to clipboard" [ref=e1243] [cursor=pointer]:
                          - generic [ref=e1244]:
                            - img [ref=e1245]
                            - img [ref=e1247]
                  - listitem [ref=e1249]:
                    - paragraph [ref=e1250]:
                      - text: Execute the following command to set the VMware vSphere image endpoint to
                      - code [ref=e1251]: https://vmwaregoldenimage.s3.amazonaws.com
                      - text: . Replace the
                      - code [ref=e1252]: caCert
                      - text: value below with the Certificate Authority (CA) certificate for your self-hosted environment.
                    - generic [ref=e1254]:
                      - code [ref=e1256]:
                        - generic [ref=e1257]: curl --request PUT '$BASE_URL/v1/system/config/vsphere/image' \
                        - generic [ref=e1258]: "--header 'Authorization: $TOKEN' \\"
                        - generic [ref=e1259]: "--header 'Content-Type: application/json' \\"
                        - generic [ref=e1260]: "--data '{"
                        - generic [ref=e1261]: "\"spec\": {"
                        - generic [ref=e1262]: "\"imagesHostEndpoint\": \"https://vmwaregoldenimage.s3.amazonaws.com\","
                        - generic [ref=e1263]: "\"insecureSkipVerify\": false,"
                        - generic [ref=e1264]: "\"caCert\": \"**********\""
                        - generic [ref=e1265]: "}"
                        - generic [ref=e1266]: "}'"
                      - generic [ref=e1267]:
                        - button "Toggle word wrap" [ref=e1268] [cursor=pointer]:
                          - img [ref=e1269]
                        - button "Copy code to clipboard" [ref=e1271] [cursor=pointer]:
                          - generic [ref=e1272]:
                            - img [ref=e1273]
                            - img [ref=e1275]
                - paragraph [ref=e1277]: MAAS and VMware vSphere clusters will now be successfully provisioned on your self-hosted Palette environment.
              - generic [ref=e1278]:
                - generic [ref=e1280]:
                  - text: "Tags:"
                  - list [ref=e1281]:
                    - listitem [ref=e1282]:
                      - link "troubleshooting" [ref=e1283] [cursor=pointer]:
                        - /url: /tags/troubleshooting/
                    - listitem [ref=e1284]:
                      - link "self-hosted" [ref=e1285] [cursor=pointer]:
                        - /url: /tags/self-hosted/
                    - listitem [ref=e1286]:
                      - link "palette" [ref=e1287] [cursor=pointer]:
                        - /url: /tags/palette/
                    - listitem [ref=e1288]:
                      - link "vertex" [ref=e1289] [cursor=pointer]:
                        - /url: /tags/vertex/
                - link "Edit this page" [ref=e1292] [cursor=pointer]:
                  - /url: https://github.com/spectrocloud/librarium/blob/master/docs/docs-content/troubleshooting/enterprise-install.md
                  - img [ref=e1293]
                  - text: Edit this page
            - navigation "Docs pages" [ref=e1297]:
              - link "Previous  Virtual Machine Orchestrator" [ref=e1298] [cursor=pointer]:
                - /url: /troubleshooting/vmo-issues/
                - generic [ref=e1299]: Previous
                - generic [ref=e1300]:  Virtual Machine Orchestrator
              - link "Next Palette Upgrade " [ref=e1301] [cursor=pointer]:
                - /url: /troubleshooting/palette-upgrade/
                - generic [ref=e1302]: Next
                - generic [ref=e1303]: Palette Upgrade 
          - list [ref=e1306]:
            - listitem [ref=e1307]:
              - link "Scenario - VerteX Management Appliance Fails to Upgrade due to Stuck LINSTOR Satellite Pods" [ref=e1308] [cursor=pointer]:
                - /url: "#scenario---vertex-management-appliance-fails-to-upgrade-due-to-stuck-linstor-satellite-pods"
              - list [ref=e1309]:
                - listitem [ref=e1310]:
                  - link "Debug Steps" [ref=e1311] [cursor=pointer]:
                    - /url: "#debug-steps"
            - listitem [ref=e1312]:
              - link "Scenario - Palette/VerteX Management Appliance Installation Stalled due to piraeus-operator Pack in Error State" [ref=e1313] [cursor=pointer]:
                - /url: "#scenario---palettevertex-management-appliance-installation-stalled-due-to-piraeus-operator-pack-in-error-state"
              - list [ref=e1314]:
                - listitem [ref=e1315]:
                  - link "Debug Steps" [ref=e1316] [cursor=pointer]:
                    - /url: "#debug-steps-1"
            - listitem [ref=e1317]:
              - link "Scenario - Unexpected Logouts in Tenant Console After Palette/VerteX Management Appliance Installation" [ref=e1318] [cursor=pointer]:
                - /url: "#scenario---unexpected-logouts-in-tenant-console-after-palettevertex-management-appliance-installation"
              - list [ref=e1319]:
                - listitem [ref=e1320]:
                  - link "Debug Steps" [ref=e1321] [cursor=pointer]:
                    - /url: "#debug-steps-2"
            - listitem [ref=e1322]:
              - link "Scenario - IP Pool Exhausted During Airgapped Upgrade" [ref=e1323] [cursor=pointer]:
                - /url: "#scenario---ip-pool-exhausted-during-airgapped-upgrade"
              - list [ref=e1324]:
                - listitem [ref=e1325]:
                  - link "Debug Steps" [ref=e1326] [cursor=pointer]:
                    - /url: "#debug-steps-3"
            - listitem [ref=e1327]:
              - link "Scenario - Self-Linking Error" [ref=e1328] [cursor=pointer]:
                - /url: "#scenario---self-linking-error"
              - list [ref=e1329]:
                - listitem [ref=e1330]:
                  - link "Debug Steps" [ref=e1331] [cursor=pointer]:
                    - /url: "#debug-steps-4"
            - listitem [ref=e1332]:
              - link "Scenario - Enterprise Backup Stuck" [ref=e1333] [cursor=pointer]:
                - /url: "#scenario---enterprise-backup-stuck"
              - list [ref=e1334]:
                - listitem [ref=e1335]:
                  - link "Debug Steps" [ref=e1336] [cursor=pointer]:
                    - /url: "#debug-steps-5"
            - listitem [ref=e1337]:
              - link "Scenario - Non-Unique vSphere CNS Mapping" [ref=e1338] [cursor=pointer]:
                - /url: "#scenario---non-unique-vsphere-cns-mapping"
              - list [ref=e1339]:
                - listitem [ref=e1340]:
                  - link "Debug Steps" [ref=e1341] [cursor=pointer]:
                    - /url: "#debug-steps-6"
            - listitem [ref=e1342]:
              - link "Scenario - \"Too Many Open Files\" in Cluster" [ref=e1343] [cursor=pointer]:
                - /url: "#scenario---too-many-open-files-in-cluster"
              - list [ref=e1344]:
                - listitem [ref=e1345]:
                  - link "Debug Steps" [ref=e1346] [cursor=pointer]:
                    - /url: "#debug-steps-7"
            - listitem [ref=e1347]:
              - link "Scenario - MAAS and VMware vSphere Clusters Fail Image Resolution in Non-Airgap Environments" [ref=e1348] [cursor=pointer]:
                - /url: "#scenario---maas-and-vmware-vsphere-clusters-fail-image-resolution-in-non-airgap-environments"
              - list [ref=e1349]:
                - listitem [ref=e1350]:
                  - link "Debug Steps" [ref=e1351] [cursor=pointer]:
                    - /url: "#debug-steps-8"
  - button "Project Logo Ask AI" [ref=e1352] [cursor=pointer]:
    - generic [ref=e1355]:
      - img "Project Logo" [ref=e1356]
      - paragraph [ref=e1357]: Ask AI
```