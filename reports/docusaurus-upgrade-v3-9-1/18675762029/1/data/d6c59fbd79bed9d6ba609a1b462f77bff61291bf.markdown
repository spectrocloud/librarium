# Page snapshot

```yaml
- generic [active] [ref=e1]:
  - generic [ref=e2]:
    - region "Skip to main content":
      - link "Skip to main content" [ref=e3] [cursor=pointer]:
        - /url: "#__docusaurus_skipToContent_fallback"
    - navigation "Main" [ref=e4]:
      - generic [ref=e5]:
        - generic [ref=e6]:
          - link "Spectro cloud logo" [ref=e7] [cursor=pointer]:
            - /url: /
            - img "Spectro cloud logo" [ref=e9]
          - link "Docs" [ref=e10] [cursor=pointer]:
            - /url: /release-notes/
          - link "Tutorials" [ref=e11] [cursor=pointer]:
            - /url: /tutorials/
          - link "Downloads" [ref=e12] [cursor=pointer]:
            - /url: /downloads/
          - link "API" [ref=e13] [cursor=pointer]:
            - /url: /api/introduction/
          - button "latest" [ref=e15] [cursor=pointer]
        - generic [ref=e16]:
          - link "GitHub repository" [ref=e17] [cursor=pointer]:
            - /url: https://github.com/spectrocloud/librarium
          - button "Switch between dark and light mode (currently system mode)" [ref=e19] [cursor=pointer]:
            - img [ref=e20]
          - button "Search (Control+k)" [ref=e23] [cursor=pointer]:
            - generic [ref=e24]:
              - img [ref=e25]
              - generic [ref=e28]: Search
    - generic [ref=e32]:
      - complementary [ref=e33]:
        - generic [ref=e35]:
          - link "Spectro cloud logo" [ref=e36] [cursor=pointer]:
            - /url: /
            - img "Spectro cloud logo" [ref=e37]
          - navigation "Docs sidebar" [ref=e38]:
            - list [ref=e39]:
              - listitem [ref=e40]:
                - link "Palette Tutorials" [ref=e41] [cursor=pointer]:
                  - /url: /tutorials/
                  - img [ref=e43]
                  - text: Palette Tutorials
              - listitem [ref=e45]:
                - generic [ref=e46]:
                  - link "Getting Started" [expanded] [ref=e47] [cursor=pointer]:
                    - /url: /tutorials/getting-started/
                    - img [ref=e49]
                    - text: Getting Started
                  - button "Toggle the collapsible sidebar category 'Getting Started'" [ref=e51] [cursor=pointer]
                - list [ref=e52]:
                  - listitem [ref=e53]:
                    - generic [ref=e54]:
                      - link "Palette" [expanded] [ref=e55] [cursor=pointer]:
                        - /url: /tutorials/getting-started/palette/
                      - button "Toggle the collapsible sidebar category 'Palette'" [ref=e56] [cursor=pointer]
                    - list [ref=e57]:
                      - listitem [ref=e58]:
                        - link "Introduction to Palette" [ref=e59] [cursor=pointer]:
                          - /url: /tutorials/getting-started/palette/introduction-palette/
                      - listitem [ref=e60]:
                        - generic [ref=e61]:
                          - link "Deploy a Cluster to AWS" [ref=e62] [cursor=pointer]:
                            - /url: /tutorials/getting-started/palette/aws/
                          - button "Toggle the collapsible sidebar category 'Deploy a Cluster to AWS'" [ref=e63] [cursor=pointer]
                      - listitem [ref=e64]:
                        - generic [ref=e65]:
                          - link "Deploy a Cluster to Azure" [ref=e66] [cursor=pointer]:
                            - /url: /tutorials/getting-started/palette/azure/
                          - button "Toggle the collapsible sidebar category 'Deploy a Cluster to Azure'" [ref=e67] [cursor=pointer]
                      - listitem [ref=e68]:
                        - generic [ref=e69]:
                          - link "Deploy a Cluster to GCP" [ref=e70] [cursor=pointer]:
                            - /url: /tutorials/getting-started/palette/gcp/
                          - button "Toggle the collapsible sidebar category 'Deploy a Cluster to GCP'" [ref=e71] [cursor=pointer]
                      - listitem [ref=e72]:
                        - generic [ref=e73]:
                          - link "Deploy a Cluster to VMware" [expanded] [ref=e74] [cursor=pointer]:
                            - /url: /tutorials/getting-started/palette/vmware/
                          - button "Toggle the collapsible sidebar category 'Deploy a Cluster to VMware'" [ref=e75] [cursor=pointer]
                        - list [ref=e76]:
                          - listitem [ref=e77]:
                            - link "Set up Palette" [ref=e78] [cursor=pointer]:
                              - /url: /tutorials/getting-started/palette/vmware/setup/
                          - listitem [ref=e79]:
                            - link "Deploy a PCG" [ref=e80] [cursor=pointer]:
                              - /url: /tutorials/getting-started/palette/vmware/deploy-pcg/
                          - listitem [ref=e81]:
                            - link "Create a Cluster Profile" [ref=e82] [cursor=pointer]:
                              - /url: /tutorials/getting-started/palette/vmware/create-cluster-profile/
                          - listitem [ref=e83]:
                            - link "Deploy a Cluster" [ref=e84] [cursor=pointer]:
                              - /url: /tutorials/getting-started/palette/vmware/deploy-k8s-cluster/
                          - listitem [ref=e85]:
                            - link "Deploy Cluster Profile Updates" [ref=e86] [cursor=pointer]:
                              - /url: /tutorials/getting-started/palette/vmware/update-k8s-cluster/
                          - listitem [ref=e87]:
                            - link "Cluster Management with Terraform" [ref=e88] [cursor=pointer]:
                              - /url: /tutorials/getting-started/palette/vmware/deploy-manage-k8s-cluster-tf/
                          - listitem [ref=e89]:
                            - link "Scale, Upgrade, and Secure Clusters" [ref=e90] [cursor=pointer]:
                              - /url: /tutorials/getting-started/palette/vmware/scale-secure-cluster/
                  - listitem [ref=e91]:
                    - generic [ref=e92]:
                      - link "Palette Edge" [ref=e93] [cursor=pointer]:
                        - /url: /tutorials/getting-started/palette-edge/
                      - button "Toggle the collapsible sidebar category 'Palette Edge'" [ref=e94] [cursor=pointer]
                  - listitem [ref=e95]:
                    - generic [ref=e96]:
                      - link "Additional Capabilities" [ref=e97] [cursor=pointer]:
                        - /url: /tutorials/getting-started/additional-capabilities/
                      - button "Toggle the collapsible sidebar category 'Additional Capabilities'" [ref=e98] [cursor=pointer]
              - listitem [ref=e99]:
                - generic [ref=e100]:
                  - link "Packs and Registries" [ref=e101] [cursor=pointer]:
                    - /url: /tutorials/packs-registries/
                    - img [ref=e103]
                    - text: Packs and Registries
                  - button "Toggle the collapsible sidebar category 'Packs and Registries'" [ref=e105] [cursor=pointer]
              - listitem [ref=e106]:
                - generic [ref=e107]:
                  - link "Profiles" [ref=e108] [cursor=pointer]:
                    - /url: /tutorials/profiles/
                    - img [ref=e110]
                    - text: Profiles
                  - button "Toggle the collapsible sidebar category 'Profiles'" [ref=e114] [cursor=pointer]
              - listitem [ref=e115]:
                - generic [ref=e116]:
                  - link "Clusters" [ref=e117] [cursor=pointer]:
                    - /url: /tutorials/clusters/
                    - img [ref=e119]
                    - text: Clusters
                  - button "Toggle the collapsible sidebar category 'Clusters'" [ref=e125] [cursor=pointer]
              - listitem [ref=e126]:
                - generic [ref=e127]:
                  - link "Palette Dev Engine" [ref=e128] [cursor=pointer]:
                    - /url: /tutorials/pde/
                    - img [ref=e130]
                    - text: Palette Dev Engine
                  - button "Toggle the collapsible sidebar category 'Palette Dev Engine'" [ref=e132] [cursor=pointer]
              - listitem [ref=e133]:
                - button "Privacy Settings" [ref=e134] [cursor=pointer]
      - main [ref=e135]:
        - generic [ref=e137]:
          - generic [ref=e139]:
            - article [ref=e140]:
              - navigation "Breadcrumbs" [ref=e141]:
                - list [ref=e142]:
                  - listitem [ref=e143]:
                    - link "Home page" [ref=e144] [cursor=pointer]:
                      - /url: /
                      - img [ref=e145]
                  - listitem [ref=e147]:
                    - link "Getting Started" [ref=e148] [cursor=pointer]:
                      - /url: /tutorials/getting-started/
                  - listitem [ref=e149]:
                    - link "Palette" [ref=e150] [cursor=pointer]:
                      - /url: /tutorials/getting-started/palette/
                  - listitem [ref=e151]:
                    - link "Deploy a Cluster to VMware" [ref=e152] [cursor=pointer]:
                      - /url: /tutorials/getting-started/palette/vmware/
                  - listitem [ref=e153]:
                    - generic [ref=e154]: Scale, Upgrade, and Secure Clusters
              - generic [ref=e155]:
                - heading "Scale, Upgrade, and Secure Clusters" [level=1] [ref=e157]
                - paragraph [ref=e158]: Palette has in-built features to help with the automation of Day-2 operations. Upgrading and maintaining a deployed cluster is typically complex because you need to consider any possible impact on service availability. Palette provides out-of-the-box functionality for upgrades, observability, granular Role Based Access Control (RBAC), backup, and security scans.
                - paragraph [ref=e159]: This tutorial will teach you how to use the Palette UI to perform scale and maintenance tasks on your clusters. You will learn how to create Palette projects and teams, import a cluster profile, safely upgrade the Kubernetes version of a deployed cluster and scale up your cluster nodes. The concepts you learn about in the Getting Started section are centered around a fictional case study company, Spacetastic Ltd.
                - heading "ðŸ§‘â€ðŸš€ Back at Spacetastic HQDirect link to ðŸ§‘â€ðŸš€ Back at Spacetastic HQ" [level=2] [ref=e160]:
                  - text: ðŸ§‘â€ðŸš€ Back at Spacetastic HQ
                  - link "Direct link to ðŸ§‘â€ðŸš€ Back at Spacetastic HQ" [ref=e161] [cursor=pointer]:
                    - /url: "#-back-at-spacetastic-hq"
                    - text: "#"
                - paragraph [ref=e162]: The team have been impressed with Palette's capabilities and decide to become a Spectro Cloud customer. The last piece of the puzzle is to learn how to handle Day-2 operations, which become increasingly more important as the Spacetastic platform matures. They must ensure that their systems are patched, upgraded, scaled, and scanned for vulnerabilities. These maintenance tasks must be automated and applied on a schedule, as the entire team wants to focus on providing Spacetastic features.
                - blockquote [ref=e163]:
                  - paragraph [ref=e164]: "\"I've read your report on Palette adoption at Spacetastic,\" says Meera, who provides the security expertise at Spacetastic. \"I was impressed with the ability to roll out updates to all clusters using the same cluster profile. This will streamline our system upgrades and cluster patching. Keeping up with security best practices has never been more important, now that we are growing faster than ever!\""
                  - paragraph [ref=e165]: "\"I agree. No matter how safe our coding practices are, we need to periodically review, patch, and upgrade our dependencies,\" says Wren, who leads the engineering team at Spacetastic."
                  - paragraph [ref=e166]: Kai nods, scrolling through the Palette Docs. "Team, Palette has more security and Day-2 operation support than we have explored so far. I will continue their Getting Started section and report back with my findings."
                - heading "PrerequisitesDirect link to Prerequisites" [level=2] [ref=e167]:
                  - text: Prerequisites
                  - link "Direct link to Prerequisites" [ref=e168] [cursor=pointer]:
                    - /url: "#prerequisites"
                    - text: "#"
                - paragraph [ref=e169]:
                  - text: To complete this tutorial, follow the steps described in the
                  - link "Set up Palette with VMware" [ref=e170] [cursor=pointer]:
                    - /url: /tutorials/getting-started/palette/vmware/setup/
                  - text: guide to authenticate Palette for use with your VMware vSphere account.
                - paragraph [ref=e171]:
                  - text: Follow the steps described in the
                  - link "Deploy a PCG" [ref=e172] [cursor=pointer]:
                    - /url: /tutorials/getting-started/palette/vmware/deploy-pcg/
                  - text: tutorial to deploy a VMware vSphere Private Cloud Gateway (PCG).
                - paragraph [ref=e173]:
                  - text: Additionally, you should install kubectl locally. Use the Kubernetes
                  - link "Install Tools" [ref=e174] [cursor=pointer]:
                    - /url: https://kubernetes.io/docs/tasks/tools/
                  - text: page for further guidance.
                - heading "Create Palette ProjectsDirect link to Create Palette Projects" [level=2] [ref=e175]:
                  - text: Create Palette Projects
                  - link "Direct link to Create Palette Projects" [ref=e176] [cursor=pointer]:
                    - /url: "#create-palette-projects"
                    - text: "#"
                - paragraph [ref=e177]: Palette projects help you organize and manage cluster resources, providing logical groupings. They also allow you to manage user access control through RBAC. You can assign users and teams with specific roles to specific projects. All resources created within a project are scoped to that project and only available to that project, but a tenant can have multiple projects.
                - paragraph [ref=e178]:
                  - text: Log in to
                  - link "Palette" [ref=e179] [cursor=pointer]:
                    - /url: https://console.spectrocloud.com
                  - text: .
                - paragraph [ref=e180]:
                  - text: Click on the
                  - strong [ref=e181]: drop-down Menu
                  - text: at the top of the page and switch to the
                  - strong [ref=e182]: Tenant Admin
                  - text: scope. Palette provides the
                  - strong [ref=e183]: Default
                  - text: project out-of-the-box.
                - paragraph [ref=e184]:
                  - img "Image that shows how to select tenant admin scope" [ref=e185]
                - paragraph [ref=e186]:
                  - text: Navigate to the left
                  - strong [ref=e187]: Main Menu
                  - text: and click on
                  - strong [ref=e188]: Projects
                  - text: . Click on the
                  - strong [ref=e189]: Create Project
                  - text: button. The
                  - strong [ref=e190]: Create a new project
                  - text: dialog appears.
                - paragraph [ref=e191]: Fill out the input fields with values from the table below to create a project.
                - table [ref=e192]:
                  - rowgroup [ref=e193]:
                    - row "Field Description Value" [ref=e194]:
                      - cell "Field" [ref=e195]
                      - cell "Description" [ref=e196]
                      - cell "Value" [ref=e197]
                  - rowgroup [ref=e198]:
                    - row "Name The name of the project. Project-ScaleSecureTutorial" [ref=e199]:
                      - cell "Name" [ref=e200]
                      - cell "The name of the project." [ref=e201]
                      - cell "Project-ScaleSecureTutorial" [ref=e202]:
                        - code [ref=e203]: Project-ScaleSecureTutorial
                    - row "Description A brief description of the project. Project for Scale, Upgrade, and Secure Clusters tutorial." [ref=e204]:
                      - cell "Description" [ref=e205]
                      - cell "A brief description of the project." [ref=e206]
                      - cell "Project for Scale, Upgrade, and Secure Clusters tutorial." [ref=e207]
                    - row "Tags Add tags to the project. env:dev" [ref=e208]:
                      - cell "Tags" [ref=e209]
                      - cell "Add tags to the project." [ref=e210]
                      - cell "env:dev" [ref=e211]:
                        - code [ref=e212]: env:dev
                - paragraph [ref=e213]:
                  - text: Click
                  - strong [ref=e214]: Confirm
                  - text: to create the project. Once Palette finishes creating the project, a new card appears on the
                  - strong [ref=e215]: Projects
                  - text: page.
                - paragraph [ref=e216]:
                  - text: Navigate to the left
                  - strong [ref=e217]: Main Menu
                  - text: and click on
                  - strong [ref=e218]: Users & Teams
                  - text: .
                - paragraph [ref=e219]:
                  - text: Select the
                  - strong [ref=e220]: Teams
                  - text: tab. Then, click on
                  - strong [ref=e221]: Create Team
                  - text: .
                - paragraph [ref=e222]:
                  - text: Fill in the
                  - strong [ref=e223]: Team Name
                  - text: with
                  - strong [ref=e224]: scale-secure-tutorial-team
                  - text: . Click on
                  - strong [ref=e225]: Confirm
                  - text: .
                - paragraph [ref=e226]:
                  - text: Once Palette creates the team, select it from the
                  - strong [ref=e227]: Teams
                  - text: list. The
                  - strong [ref=e228]: Team Details
                  - text: pane opens.
                - paragraph [ref=e229]:
                  - text: On the
                  - strong [ref=e230]: Project Roles
                  - text: tab, click on
                  - strong [ref=e231]: New Project Role
                  - text: . The list of project roles appears.
                - paragraph [ref=e232]:
                  - text: Select the
                  - strong [ref=e233]: Project-ScaleSecureTutorial
                  - text: from the
                  - strong [ref=e234]: Projects
                  - text: drop-down. Then, select the
                  - strong [ref=e235]: Cluster Profile Viewer
                  - text: and
                  - strong [ref=e236]: Cluster Viewer
                  - text: roles. Click on
                  - strong [ref=e237]: Confirm
                  - text: .
                - paragraph [ref=e238]:
                  - img "Image that shows how to select team roles" [ref=e239]
                - paragraph [ref=e240]:
                  - text: Any users that you add to this team inherit the project roles assigned to it. Roles are the foundation of Palette's RBAC enforcement. They allow a single user to have different types of access control based on the resource being accessed. In this scenario, any user added to this team will have access to view any cluster profiles and clusters in the
                  - strong [ref=e241]: Project-ScaleSecureTutorial
                  - text: project, but not modify them. Check out the
                  - link "Palette RBAC" [ref=e242] [cursor=pointer]:
                    - /url: /user-management/palette-rbac/
                  - text: section for more details.
                - paragraph [ref=e243]:
                  - text: Navigate to the left
                  - strong [ref=e244]: Main Menu
                  - text: and click on
                  - strong [ref=e245]: Projects
                  - text: .
                - paragraph [ref=e246]:
                  - text: Click on
                  - strong [ref=e247]: Open project
                  - text: on the
                  - strong [ref=e248]: Project-ScaleSecureTutorial
                  - text: card.
                - paragraph [ref=e249]:
                  - img "Image that shows how to open the tutorial project" [ref=e250]
                - paragraph [ref=e251]:
                  - text: Your scope changes from
                  - strong [ref=e252]: Tenant Admin
                  - text: to
                  - strong [ref=e253]: Project-ScaleSecureTutorial
                  - text: . All further resources you create will be part of this project.
                - heading "Import a Cluster ProfileDirect link to Import a Cluster Profile" [level=2] [ref=e254]:
                  - text: Import a Cluster Profile
                  - link "Direct link to Import a Cluster Profile" [ref=e255] [cursor=pointer]:
                    - /url: "#import-a-cluster-profile"
                    - text: "#"
                - paragraph [ref=e256]: Palette provides three resource contexts. They help you customize your environment to your organizational needs, as well as control the scope of your settings.
                - table [ref=e257]:
                  - rowgroup [ref=e258]:
                    - row "Context Description" [ref=e259]:
                      - cell "Context" [ref=e260]
                      - cell "Description" [ref=e261]
                  - rowgroup [ref=e262]:
                    - row "System Resources are available at the system level and to all tenants in the system." [ref=e263]:
                      - cell "System" [ref=e264]
                      - cell "Resources are available at the system level and to all tenants in the system." [ref=e265]
                    - row "Tenant Resources are available at the tenant level and to all projects belonging to the tenant." [ref=e266]:
                      - cell "Tenant" [ref=e267]
                      - cell "Resources are available at the tenant level and to all projects belonging to the tenant." [ref=e268]
                    - row "Project Resources are available within a project and not available to other projects." [ref=e269]:
                      - cell "Project" [ref=e270]
                      - cell "Resources are available within a project and not available to other projects." [ref=e271]
                - paragraph [ref=e272]:
                  - text: All of the resources you have created as part of your Getting Started journey have used the
                  - strong [ref=e273]: Project
                  - text: context. They are only visible in the
                  - strong [ref=e274]: Default
                  - text: project. Therefore, you will need to create a new cluster profile in
                  - strong [ref=e275]: Project-ScaleSecureTutorial
                  - text: .
                - paragraph [ref=e276]:
                  - text: Navigate to the left
                  - strong [ref=e277]: Main Menu
                  - text: and click on
                  - strong [ref=e278]: Profiles
                  - text: . Click on
                  - strong [ref=e279]: Import Cluster Profile
                  - text: . The
                  - strong [ref=e280]: Import Cluster Profile
                  - text: pane opens.
                - paragraph [ref=e281]:
                  - text: Paste the following in the text editor. Click on
                  - strong [ref=e282]: Validate
                  - text: .
                - generic [ref=e284]:
                  - code [ref=e286]:
                    - generic [ref=e287]: "{"
                    - generic [ref=e288]: "\"metadata\": {"
                    - generic [ref=e289]: "\"name\": \"vmware-profile\","
                    - generic [ref=e290]: "\"description\": \"Cluster profile to deploy to VMware.\","
                    - generic [ref=e291]: "\"labels\": {}"
                    - generic [ref=e292]: "},"
                    - generic [ref=e293]: "\"spec\": {"
                    - generic [ref=e294]: "\"version\": \"1.0.0\","
                    - generic [ref=e295]: "\"template\": {"
                    - generic [ref=e296]: "\"type\": \"cluster\","
                    - generic [ref=e297]: "\"cloudType\": \"vsphere\","
                    - generic [ref=e298]: "\"packs\": ["
                    - generic [ref=e299]: "{"
                    - generic [ref=e300]: "\"name\": \"ubuntu-vsphere\","
                    - generic [ref=e301]: "\"type\": \"spectro\","
                    - generic [ref=e302]: "\"layer\": \"os\","
                    - generic [ref=e303]: "\"version\": \"22.04\","
                    - generic [ref=e304]: "\"tag\": \"22.04\","
                    - generic [ref=e305]: "\"values\": \"# Spectro Golden images includes most of the hardening as per CIS Ubuntu Linux 22.04 LTS Server L1 v1.0.0 standards\\n\\n# Uncomment below section to\\n# 1. Include custom files to be copied over to the nodes and/or\\n# 2. Execute list of commands before or after kubeadm init/join is executed\\n#\\n#kubeadmconfig:\\n# preKubeadmCommands:\\n# - echo \\\"Executing pre kube admin config commands\\\"\\n# - update-ca-certificates\\n# - 'systemctl restart containerd; sleep 3'\\n# - 'while [ ! -S /var/run/containerd/containerd.sock ]; do echo \\\"Waiting for containerd...\\\"; sleep 1; done'\\n# postKubeadmCommands:\\n# - echo \\\"Executing post kube admin config commands\\\"\\n# files:\\n# - targetPath: /usr/local/share/ca-certificates/mycom.crt\\n# targetOwner: \\\"root:root\\\"\\n# targetPermissions: \\\"0644\\\"\\n# content: |\\n# -----BEGIN CERTIFICATE-----\\n# MIICyzCCAbOgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl\\n# cm5ldGVzMB4XDTIwMDkyMjIzNDMyM1oXDTMwMDkyMDIzNDgyM1owFTETMBEGA1UE\\n# AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMdA\\n# nZYs1el/6f9PgV/aO9mzy7MvqaZoFnqO7Qi4LZfYzixLYmMUzi+h8/RLPFIoYLiz\\n# qiDn+P8c9I1uxB6UqGrBt7dkXfjrUZPs0JXEOX9U/6GFXL5C+n3AUlAxNCS5jobN\\n# fbLt7DH3WoT6tLcQefTta2K+9S7zJKcIgLmBlPNDijwcQsbenSwDSlSLkGz8v6N2\\n# 7SEYNCV542lbYwn42kbcEq2pzzAaCqa5uEPsR9y+uzUiJpv5tDHUdjbFT8tme3vL\\n# 9EdCPODkqtMJtCvz0hqd5SxkfeC2L+ypaiHIxbwbWe7GtliROvz9bClIeGY7gFBK\\n# jZqpLdbBVjo0NZBTJFUCAwEAAaMmMCQwDgYDVR0PAQH/BAQDAgKkMBIGA1UdEwEB\\n# /wQIMAYBAf8CAQAwDQYJKoZIhvcNAQELBQADggEBADIKoE0P+aVJGV9LWGLiOhki\\n# HFv/vPPAQ2MPk02rLjWzCaNrXD7aPPgT/1uDMYMHD36u8rYyf4qPtB8S5REWBM/Y\\n# g8uhnpa/tGsaqO8LOFj6zsInKrsXSbE6YMY6+A8qvv5lPWpJfrcCVEo2zOj7WGoJ\\n# ixi4B3fFNI+wih8/+p4xW+n3fvgqVYHJ3zo8aRLXbXwztp00lXurXUyR8EZxyR+6\\n# b+IDLmHPEGsY9KOZ9VLLPcPhx5FR9njFyXvDKmjUMJJgUpRkmsuU1mCFC+OHhj56\\n# IkLaSJf6z/p2a3YjTxvHNCqFMLbJ2FvJwYCRzsoT2wm2oulnUAMWPI10vdVM+Nc=\\n# -----END CERTIFICATE-----\","
                    - generic [ref=e306]: "\"registry\": {"
                    - generic [ref=e307]: "\"metadata\": {"
                    - generic [ref=e308]: "\"uid\": \"5eecc89d0b150045ae661cef\","
                    - generic [ref=e309]: "\"name\": \"Public Repo\","
                    - generic [ref=e310]: "\"kind\": \"pack\","
                    - generic [ref=e311]: "\"isPrivate\": false,"
                    - generic [ref=e312]: "\"providerType\": \"\""
                    - generic [ref=e313]: "}"
                    - generic [ref=e314]: "}"
                    - generic [ref=e315]: "},"
                    - generic [ref=e316]: "{"
                    - generic [ref=e317]: "\"name\": \"kubernetes\","
                    - generic [ref=e318]: "\"type\": \"spectro\","
                    - generic [ref=e319]: "\"layer\": \"k8s\","
                    - generic [ref=e320]: "\"version\": \"1.27.15\","
                    - generic [ref=e321]: "\"tag\": \"1.27.x\","
                    - generic [ref=e322]: "\"values\": \"# spectrocloud.com/enabled-presets: Kube Controller Manager:loopback-ctrlmgr,Kube Scheduler:loopback-scheduler\\npack:\\n content:\\n images:\\n - image: registry.k8s.io/coredns/coredns:v1.10.1\\n - image: registry.k8s.io/etcd:3.5.12-0\\n - image: registry.k8s.io/kube-apiserver:v1.27.15\\n - image: registry.k8s.io/kube-controller-manager:v1.27.15\\n - image: registry.k8s.io/kube-proxy:v1.27.15\\n - image: registry.k8s.io/kube-scheduler:v1.27.15\\n - image: registry.k8s.io/pause:3.9\\n - image: registry.k8s.io/pause:3.8\\n #CIDR Range for Pods in cluster\\n # Note : This must not overlap with any of the host or service network\\n podCIDR: \\\"192.168.0.0/16\\\"\\n #CIDR notation IP range from which to assign service cluster IPs\\n # Note : This must not overlap with any IP ranges assigned to nodes for pods.\\n serviceClusterIpRange: \\\"10.96.0.0/12\\\"\\n # serviceDomain: \\\"cluster.local\\\"\\n\\nkubeadmconfig:\\n apiServer:\\n extraArgs:\\n # Note : secure-port flag is used during kubeadm init. Do not change this flag on a running cluster\\n secure-port: \\\"6443\\\"\\n anonymous-auth: \\\"true\\\"\\n profiling: \\\"false\\\"\\n disable-admission-plugins: \\\"AlwaysAdmit\\\"\\n default-not-ready-toleration-seconds: \\\"60\\\"\\n default-unreachable-toleration-seconds: \\\"60\\\"\\n enable-admission-plugins: \\\"AlwaysPullImages,NamespaceLifecycle,ServiceAccount,NodeRestriction,PodSecurity\\\"\\n admission-control-config-file: \\\"/etc/kubernetes/pod-security-standard.yaml\\\"\\n audit-log-path: /var/log/apiserver/audit.log\\n audit-policy-file: /etc/kubernetes/audit-policy.yaml\\n audit-log-maxage: \\\"30\\\"\\n audit-log-maxbackup: \\\"10\\\"\\n audit-log-maxsize: \\\"100\\\"\\n authorization-mode: RBAC,Node\\n tls-cipher-suites: \\\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\\\"\\n extraVolumes:\\n - name: audit-log\\n hostPath: /var/log/apiserver\\n mountPath: /var/log/apiserver\\n pathType: DirectoryOrCreate\\n - name: audit-policy\\n hostPath: /etc/kubernetes/audit-policy.yaml\\n mountPath: /etc/kubernetes/audit-policy.yaml\\n readOnly: true\\n pathType: File\\n - name: pod-security-standard\\n hostPath: /etc/kubernetes/pod-security-standard.yaml\\n mountPath: /etc/kubernetes/pod-security-standard.yaml\\n readOnly: true\\n pathType: File\\n controllerManager:\\n extraArgs:\\n profiling: \\\"false\\\"\\n terminated-pod-gc-threshold: \\\"25\\\"\\n use-service-account-credentials: \\\"true\\\"\\n feature-gates: \\\"RotateKubeletServerCertificate=true\\\"\\n scheduler:\\n extraArgs:\\n profiling: \\\"false\\\"\\n kubeletExtraArgs:\\n read-only-port : \\\"0\\\"\\n event-qps: \\\"0\\\"\\n feature-gates: \\\"RotateKubeletServerCertificate=true\\\"\\n protect-kernel-defaults: \\\"true\\\"\\n rotate-server-certificates: \\\"true\\\"\\n tls-cipher-suites: \\\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\\\"\\n files:\\n - path: hardening/audit-policy.yaml\\n targetPath: /etc/kubernetes/audit-policy.yaml\\n targetOwner: \\\"root:root\\\"\\n targetPermissions: \\\"0600\\\"\\n - path: hardening/90-kubelet.conf\\n targetPath: /etc/sysctl.d/90-kubelet.conf\\n targetOwner: \\\"root:root\\\"\\n targetPermissions: \\\"0600\\\"\\n - targetPath: /etc/kubernetes/pod-security-standard.yaml\\n targetOwner: \\\"root:root\\\"\\n targetPermissions: \\\"0600\\\"\\n content: |\\n apiVersion: apiserver.config.k8s.io/v1\\n kind: AdmissionConfiguration\\n plugins:\\n - name: PodSecurity\\n configuration:\\n apiVersion: pod-security.admission.config.k8s.io/v1\\n kind: PodSecurityConfiguration\\n defaults:\\n enforce: \\\"baseline\\\"\\n enforce-version: \\\"v1.27\\\"\\n audit: \\\"baseline\\\"\\n audit-version: \\\"v1.27\\\"\\n warn: \\\"restricted\\\"\\n warn-version: \\\"v1.27\\\"\\n audit: \\\"restricted\\\"\\n audit-version: \\\"v1.27\\\"\\n exemptions:\\n # Array of authenticated usernames to exempt.\\n usernames: []\\n # Array of runtime class names to exempt.\\n runtimeClasses: []\\n # Array of namespaces to exempt.\\n namespaces: [kube-system]\\n\\n preKubeadmCommands:\\n # For enabling 'protect-kernel-defaults' flag to kubelet, kernel parameters changes are required\\n - 'echo \\\"====> Applying kernel parameters for Kubelet\\\"'\\n - 'sysctl -p /etc/sysctl.d/90-kubelet.conf'\\n postKubeadmCommands:\\n - 'chmod 600 /var/lib/kubelet/config.yaml'\\n #- 'echo \\\"List of post kubeadm commands to be executed\\\"'\\n\\n# Client configuration to add OIDC based authentication flags in kubeconfig\\n#clientConfig:\\n #oidc-issuer-url: \\\"{{ .spectro.pack.kubernetes.kubeadmconfig.apiServer.extraArgs.oidc-issuer-url }}\\\"\\n #oidc-client-id: \\\"{{ .spectro.pack.kubernetes.kubeadmconfig.apiServer.extraArgs.oidc-client-id }}\\\"\\n #oidc-client-secret: 1gsranjjmdgahm10j8r6m47ejokm9kafvcbhi3d48jlc3rfpprhv\\n #oidc-extra-scope: profile,email\","
                    - generic [ref=e323]: "\"registry\": {"
                    - generic [ref=e324]: "\"metadata\": {"
                    - generic [ref=e325]: "\"uid\": \"5eecc89d0b150045ae661cef\","
                    - generic [ref=e326]: "\"name\": \"Public Repo\","
                    - generic [ref=e327]: "\"kind\": \"pack\","
                    - generic [ref=e328]: "\"isPrivate\": false,"
                    - generic [ref=e329]: "\"providerType\": \"\""
                    - generic [ref=e330]: "}"
                    - generic [ref=e331]: "}"
                    - generic [ref=e332]: "},"
                    - generic [ref=e333]: "{"
                    - generic [ref=e334]: "\"name\": \"cni-calico\","
                    - generic [ref=e335]: "\"type\": \"spectro\","
                    - generic [ref=e336]: "\"layer\": \"cni\","
                    - generic [ref=e337]: "\"version\": \"3.27.2\","
                    - generic [ref=e338]: "\"tag\": \"3.27.x\","
                    - generic [ref=e339]: "\"values\": \"# spectrocloud.com/enabled-presets: Microk8s:microk8s-false\\npack:\\n content:\\n images:\\n - image: gcr.io/spectro-images-public/packs/calico/3.27.2/cni:v3.27.2\\n - image: gcr.io/spectro-images-public/packs/calico/3.27.2/node:v3.27.2\\n - image: gcr.io/spectro-images-public/packs/calico/3.27.2/kube-controllers:v3.27.2\\n\\nmanifests:\\n calico:\\n microk8s: \\\"false\\\"\\n images:\\n cni: \\\"\\\"\\n node: \\\"\\\"\\n kubecontroller: \\\"\\\"\\n # IPAM type to use. Supported types are calico-ipam, host-local\\n ipamType: \\\"calico-ipam\\\"\\n\\n calico_ipam:\\n assign_ipv4: true\\n assign_ipv6: false\\n\\n # Should be one of CALICO_IPV4POOL_IPIP or CALICO_IPV4POOL_VXLAN \\n encapsulationType: \\\"CALICO_IPV4POOL_IPIP\\\"\\n\\n # Should be one of Always, CrossSubnet, Never\\n encapsulationMode: \\\"Always\\\"\\n\\n env:\\n # Additional env variables for calico-node\\n calicoNode:\\n #IPV6: \\\"autodetect\\\"\\n #FELIX_IPV6SUPPORT: \\\"true\\\"\\n #CALICO_IPV6POOL_NAT_OUTGOING: \\\"true\\\"\\n #CALICO_IPV4POOL_CIDR: \\\"192.168.0.0/16\\\"\\n #IP_AUTODETECTION_METHOD: \\\"first-found\\\"\\n\\n # Additional env variables for calico-kube-controller deployment\\n calicoKubeControllers:\\n #LOG_LEVEL: \\\"info\\\"\\n #SYNC_NODE_LABELS: \\\"true\\\"\","
                    - generic [ref=e340]: "\"registry\": {"
                    - generic [ref=e341]: "\"metadata\": {"
                    - generic [ref=e342]: "\"uid\": \"5eecc89d0b150045ae661cef\","
                    - generic [ref=e343]: "\"name\": \"Public Repo\","
                    - generic [ref=e344]: "\"kind\": \"pack\","
                    - generic [ref=e345]: "\"isPrivate\": false,"
                    - generic [ref=e346]: "\"providerType\": \"\""
                    - generic [ref=e347]: "}"
                    - generic [ref=e348]: "}"
                    - generic [ref=e349]: "},"
                    - generic [ref=e350]: "{"
                    - generic [ref=e351]: "\"name\": \"csi-vsphere-csi\","
                    - generic [ref=e352]: "\"type\": \"spectro\","
                    - generic [ref=e353]: "\"layer\": \"csi\","
                    - generic [ref=e354]: "\"version\": \"3.1.2\","
                    - generic [ref=e355]: "\"tag\": \"3.1.x\","
                    - generic [ref=e356]: "\"values\": \"pack:\\n content:\\n images:\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/cpi-manager:v1.28.0\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/cpi-manager:v1.22.9\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/cpi-manager:v1.23.5\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/cpi-manager:v1.26.2\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/cpi-manager:v1.24.6\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/cpi-manager:v1.25.3\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/cpi-manager:v1.27.0\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/csi-attacher:v4.3.0\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/csi-resizer:v1.8.0\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/livenessprobe:v2.10.0\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/csi-provisioner:v3.5.0\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/csi-snapshotter:v6.2.2\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/csi-driver:v3.1.2\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/csi-syncer:v3.1.2\\n - image: gcr.io/spectro-images-public/packs/csi-vsphere-csi/3.1.2/csi-node-driver-registrar:v2.8.0\\n\\nmanifests:\\n #Storage class config\\n vsphere:\\n\\n #Toggle for Default class\\n isDefaultClass: \\\"true\\\"\\n\\n #Specifies file system type\\n fstype: \\\"ext4\\\"\\n\\n #Allowed reclaim policies are Delete, Retain\\n reclaimPolicy: \\\"Delete\\\"\\n\\n #Specifies the URL of the datastore on which the container volume needs to be provisioned.\\n datastoreURL: \\\"\\\"\\n\\n #Specifies the storage policy for datastores on which the container volume needs to be provisioned.\\n storagePolicyName: \\\"\\\"\\n\\n volumeBindingMode: \\\"WaitForFirstConsumer\\\"\\n\\n #Set this flag to true to enable volume expansion\\n allowVolumeExpansion: true\\n\\n vsphere-cloud-controller-manager:\\n k8sVersion: \\\"{{ .spectro.system.kubernetes.version }}\\\"\\n # Override CPI image\\n image: \\\"\\\"\\n extraArgs:\\n - \\\"--cloud-provider=vsphere\\\"\\n - \\\"--v=2\\\"\\n - \\\"--cloud-config=/etc/cloud/vsphere.conf\\\"\\n\\n vsphere-csi-driver:\\n replicas: 3\\n livenessProbe:\\n csiController:\\n initialDelaySeconds: 30\\n timeoutSeconds: 10\\n periodSeconds: 180\\n failureThreshold: 3\\n # Override CSI component images\\n csiAttacherImage: \\\"\\\"\\n csiResizerImage: \\\"\\\"\\n csiControllerImage: \\\"\\\"\\n csiLivenessProbeImage: \\\"\\\"\\n csiSyncerImage: \\\"\\\"\\n csiProvisionerImage: \\\"\\\"\\n csiSnapshotterImage: \\\"\\\"\\n nodeDriverRegistrarImage: \\\"\\\"\\n vsphereCsiNodeImage: \\\"\\\"\\n extraArgs:\\n csiAttacher:\\n - \\\"--v=4\\\"\\n - \\\"--timeout=300s\\\"\\n - \\\"--csi-address=$(ADDRESS)\\\"\\n - \\\"--leader-election\\\"\\n - \\\"--leader-election-lease-duration=120s\\\"\\n - \\\"--leader-election-renew-deadline=60s\\\"\\n - \\\"--leader-election-retry-period=30s\\\"\\n - \\\"--kube-api-qps=100\\\"\\n - \\\"--kube-api-burst=100\\\"\\n csiResizer:\\n - \\\"--v=4\\\"\\n - \\\"--timeout=300s\\\"\\n - \\\"--handle-volume-inuse-error=false\\\"\\n - \\\"--csi-address=$(ADDRESS)\\\"\\n - \\\"--kube-api-qps=100\\\"\\n - \\\"--kube-api-burst=100\\\"\\n - \\\"--leader-election\\\"\\n - \\\"--leader-election-lease-duration=120s\\\"\\n - \\\"--leader-election-renew-deadline=60s\\\"\\n - \\\"--leader-election-retry-period=30s\\\"\\n csiController:\\n - \\\"--fss-name=internal-feature-states.csi.vsphere.vmware.com\\\"\\n - \\\"--fss-namespace=$(CSI_NAMESPACE)\\\"\\n csiLivenessProbe:\\n - \\\"--v=4\\\"\\n - \\\"--csi-address=/csi/csi.sock\\\"\\n csiSyncer:\\n - \\\"--leader-election\\\"\\n - \\\"--leader-election-lease-duration=30s\\\"\\n - \\\"--leader-election-renew-deadline=20s\\\"\\n - \\\"--leader-election-retry-period=10s\\\"\\n - \\\"--fss-name=internal-feature-states.csi.vsphere.vmware.com\\\"\\n - \\\"--fss-namespace=$(CSI_NAMESPACE)\\\"\\n csiProvisioner:\\n - \\\"--v=4\\\"\\n - \\\"--timeout=300s\\\"\\n - \\\"--csi-address=$(ADDRESS)\\\"\\n - \\\"--kube-api-qps=100\\\"\\n - \\\"--kube-api-burst=100\\\"\\n - \\\"--leader-election\\\"\\n - \\\"--leader-election-lease-duration=120s\\\"\\n - \\\"--leader-election-renew-deadline=60s\\\"\\n - \\\"--leader-election-retry-period=30s\\\"\\n - \\\"--default-fstype=ext4\\\"\\n # needed only for topology aware setup\\n - \\\"--feature-gates=Topology=true\\\"\\n - \\\"--strict-topology\\\"\\n csiSnapshotter:\\n - \\\"--v=4\\\"\\n - \\\"--kube-api-qps=100\\\"\\n - \\\"--kube-api-burst=100\\\"\\n - \\\"--timeout=300s\\\"\\n - \\\"--csi-address=$(ADDRESS)\\\"\\n - \\\"--leader-election\\\"\\n - \\\"--leader-election-lease-duration=120s\\\"\\n - \\\"--leader-election-renew-deadline=60s\\\"\\n - \\\"--leader-election-retry-period=30s\\\"\","
                    - generic [ref=e357]: "\"registry\": {"
                    - generic [ref=e358]: "\"metadata\": {"
                    - generic [ref=e359]: "\"uid\": \"5eecc89d0b150045ae661cef\","
                    - generic [ref=e360]: "\"name\": \"Public Repo\","
                    - generic [ref=e361]: "\"kind\": \"pack\","
                    - generic [ref=e362]: "\"isPrivate\": false,"
                    - generic [ref=e363]: "\"providerType\": \"\""
                    - generic [ref=e364]: "}"
                    - generic [ref=e365]: "}"
                    - generic [ref=e366]: "},"
                    - generic [ref=e367]: "{"
                    - generic [ref=e368]: "\"name\": \"lb-metallb-helm\","
                    - generic [ref=e369]: "\"type\": \"spectro\","
                    - generic [ref=e370]: "\"layer\": \"addon\","
                    - generic [ref=e371]: "\"version\": \"0.14.8\","
                    - generic [ref=e372]: "\"tag\": \"0.14.8\","
                    - generic [ref=e373]: "\"values\": \"pack:\\n content:\\n images:\\n - image: gcr.io/spectro-images-public/packs/metallb/0.14.8/controller:v0.14.8\\n - image: gcr.io/spectro-images-public/packs/metallb/0.14.8/speaker:v0.14.8\\n - image: gcr.io/spectro-images-public/packs/metallb/0.14.8/frr:9.1.0\\n - image: gcr.io/spectro-images-public/packs/metallb/0.14.8/kube-rbac-proxy:v0.12.0\\n charts:\\n - repo: https://metallb.github.io/metallb\\n name: metallb\\n version: 0.14.8\\n namespace: metallb-system\\n namespaceLabels:\\n \\\"metallb-system\\\": \\\"pod-security.kubernetes.io/enforce=privileged,pod-security.kubernetes.io/enforce-version=v{{ .spectro.system.kubernetes.version | substr 0 4 }}\\\" # Do not change this namespace, since CRDs expect the namespace to be metallb-system\\n spectrocloud.com/install-priority: 0\\n\\ncharts:\\n metallb-full:\\n configuration:\\n ipaddresspools:\\n first-pool:\\n spec:\\n addresses:\\n - 192.168.10.0/24\\n # - 192.168.100.50-192.168.100.60\\n avoidBuggyIPs: true\\n autoAssign: true\\n\\n l2advertisements:\\n default:\\n spec:\\n ipAddressPools:\\n - first-pool\\n\\n bgpadvertisements: {}\\n # external:\\n # spec:\\n # ipAddressPools:\\n # - bgp-pool\\n # # communities:\\n # # - vpn-only\\n\\n bgppeers: {}\\n # bgp-peer-1:\\n # spec:\\n # myASN: 64512\\n # peerASN: 64512\\n # peerAddress: 172.30.0.3\\n # peerPort: 180\\n # # BFD profiles can only be used in FRR mode\\n # # bfdProfile: bfd-profile-1\\n\\n communities: {}\\n # community-1:\\n # spec:\\n # communities:\\n # - name: vpn-only\\n # value: 1234:1\\n\\n bfdprofiles: {}\\n # bfd-profile-1:\\n # spec:\\n # receiveInterval: 380\\n # transmitInterval: 270\\n\\n metallb:\\n # Default values for metallb.\\n # This is a YAML-formatted file.\\n # Declare variables to be passed into your templates.\\n\\n imagePullSecrets: []\\n nameOverride: \\\"\\\"\\n fullnameOverride: \\\"\\\"\\n loadBalancerClass: \\\"\\\"\\n\\n # To configure MetalLB, you must specify ONE of the following two\\n # options.\\n\\n rbac:\\n # create specifies whether to install and use RBAC rules.\\n create: true\\n\\n prometheus:\\n # scrape annotations specifies whether to add Prometheus metric\\n # auto-collection annotations to pods. See\\n # https://github.com/prometheus/prometheus/blob/release-2.1/documentation/examples/prometheus-kubernetes.yml\\n # for a corresponding Prometheus configuration. Alternatively, you\\n # may want to use the Prometheus Operator\\n # (https://github.com/coreos/prometheus-operator) for more powerful\\n # monitoring configuration. If you use the Prometheus operator, this\\n # can be left at false.\\n scrapeAnnotations: false\\n\\n # port both controller and speaker will listen on for metrics\\n metricsPort: 7472\\n\\n # if set, enables rbac proxy on the controller and speaker to expose\\n # the metrics via tls.\\n # secureMetricsPort: 9120\\n\\n # the name of the secret to be mounted in the speaker pod\\n # to expose the metrics securely. If not present, a self signed\\n # certificate to be used.\\n speakerMetricsTLSSecret: \\\"\\\"\\n\\n # the name of the secret to be mounted in the controller pod\\n # to expose the metrics securely. If not present, a self signed\\n # certificate to be used.\\n controllerMetricsTLSSecret: \\\"\\\"\\n\\n # prometheus doens't have the permission to scrape all namespaces so we give it permission to scrape metallb's one\\n rbacPrometheus: true\\n\\n # the service account used by prometheus\\n # required when \\\" .Values.prometheus.rbacPrometheus == true \\\" and \\\" .Values.prometheus.podMonitor.enabled=true or prometheus.serviceMonitor.enabled=true \\\"\\n serviceAccount: \\\"\\\"\\n\\n # the namespace where prometheus is deployed\\n # required when \\\" .Values.prometheus.rbacPrometheus == true \\\" and \\\" .Values.prometheus.podMonitor.enabled=true or prometheus.serviceMonitor.enabled=true \\\"\\n namespace: \\\"\\\"\\n\\n # the image to be used for the kuberbacproxy container\\n rbacProxy:\\n repository: gcr.io/spectro-images-public/packs/metallb/0.14.8/kube-rbac-proxy\\n tag: v0.12.0\\n pullPolicy:\\n\\n # Prometheus Operator PodMonitors\\n podMonitor:\\n # enable support for Prometheus Operator\\n enabled: false\\n\\n # optional additionnal labels for podMonitors\\n additionalLabels: {}\\n\\n # optional annotations for podMonitors\\n annotations: {}\\n\\n # Job label for scrape target\\n jobLabel: \\\"app.kubernetes.io/name\\\"\\n\\n # Scrape interval. If not set, the Prometheus default scrape interval is used.\\n interval:\\n\\n # \\tmetric relabel configs to apply to samples before ingestion.\\n metricRelabelings: []\\n # - action: keep\\n # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\\n # sourceLabels: [__name__]\\n\\n # \\trelabel configs to apply to samples before ingestion.\\n relabelings: []\\n # - sourceLabels: [__meta_kubernetes_pod_node_name]\\n # separator: ;\\n # regex: ^(.*)$\\n # target_label: nodename\\n # replacement: $1\\n # action: replace\\n\\n # Prometheus Operator ServiceMonitors. To be used as an alternative\\n # to podMonitor, supports secure metrics.\\n serviceMonitor:\\n # enable support for Prometheus Operator\\n enabled: false\\n\\n speaker:\\n # optional additional labels for the speaker serviceMonitor\\n additionalLabels: {}\\n # optional additional annotations for the speaker serviceMonitor\\n annotations: {}\\n # optional tls configuration for the speaker serviceMonitor, in case\\n # secure metrics are enabled.\\n tlsConfig:\\n insecureSkipVerify: true\\n\\n controller:\\n # optional additional labels for the controller serviceMonitor\\n additionalLabels: {}\\n # optional additional annotations for the controller serviceMonitor\\n annotations: {}\\n # optional tls configuration for the controller serviceMonitor, in case\\n # secure metrics are enabled.\\n tlsConfig:\\n insecureSkipVerify: true\\n\\n # Job label for scrape target\\n jobLabel: \\\"app.kubernetes.io/name\\\"\\n\\n # Scrape interval. If not set, the Prometheus default scrape interval is used.\\n interval:\\n\\n # \\tmetric relabel configs to apply to samples before ingestion.\\n metricRelabelings: []\\n # - action: keep\\n # regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\\n # sourceLabels: [__name__]\\n\\n # \\trelabel configs to apply to samples before ingestion.\\n relabelings: []\\n # - sourceLabels: [__meta_kubernetes_pod_node_name]\\n # separator: ;\\n # regex: ^(.*)$\\n # target_label: nodename\\n # replacement: $1\\n # action: replace\\n\\n # Prometheus Operator alertmanager alerts\\n prometheusRule:\\n # enable alertmanager alerts\\n enabled: false\\n\\n # optional additionnal labels for prometheusRules\\n additionalLabels: {}\\n\\n # optional annotations for prometheusRules\\n annotations: {}\\n\\n # MetalLBStaleConfig\\n staleConfig:\\n enabled: true\\n labels:\\n severity: warning\\n\\n # MetalLBConfigNotLoaded\\n configNotLoaded:\\n enabled: true\\n labels:\\n severity: warning\\n\\n # MetalLBAddressPoolExhausted\\n addressPoolExhausted:\\n enabled: true\\n labels:\\n severity: alert\\n\\n addressPoolUsage:\\n enabled: true\\n thresholds:\\n - percent: 75\\n labels:\\n severity: warning\\n - percent: 85\\n labels:\\n severity: warning\\n - percent: 95\\n labels:\\n severity: alert\\n\\n # MetalLBBGPSessionDown\\n bgpSessionDown:\\n enabled: true\\n labels:\\n severity: alert\\n\\n extraAlerts: []\\n\\n # controller contains configuration specific to the MetalLB cluster\\n # controller.\\n controller:\\n enabled: true\\n # -- Controller log level. Must be one of: `all`, `debug`, `info`, `warn`, `error` or `none`\\n logLevel: info\\n # command: /controller\\n # webhookMode: enabled\\n image:\\n repository: gcr.io/spectro-images-public/packs/metallb/0.14.8/controller\\n tag: v0.14.8\\n pullPolicy:\\n ## @param controller.updateStrategy.type Metallb controller deployment strategy type.\\n ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy\\n ## e.g:\\n ## strategy:\\n ## type: RollingUpdate\\n ## rollingUpdate:\\n ## maxSurge: 25%\\n ## maxUnavailable: 25%\\n ##\\n strategy:\\n type: RollingUpdate\\n serviceAccount:\\n # Specifies whether a ServiceAccount should be created\\n create: true\\n # The name of the ServiceAccount to use. If not set and create is\\n # true, a name is generated using the fullname template\\n name: \\\"\\\"\\n annotations: {}\\n securityContext:\\n runAsNonRoot: true\\n # nobody\\n runAsUser: 65534\\n fsGroup: 65534\\n resources: {}\\n # limits:\\n # cpu: 100m\\n # memory: 100Mi\\n nodeSelector: {}\\n tolerations: []\\n priorityClassName: \\\"\\\"\\n runtimeClassName: \\\"\\\"\\n affinity: {}\\n podAnnotations: {}\\n labels: {}\\n livenessProbe:\\n enabled: true\\n failureThreshold: 3\\n initialDelaySeconds: 10\\n periodSeconds: 10\\n successThreshold: 1\\n timeoutSeconds: 1\\n readinessProbe:\\n enabled: true\\n failureThreshold: 3\\n initialDelaySeconds: 10\\n periodSeconds: 10\\n successThreshold: 1\\n timeoutSeconds: 1\\n tlsMinVersion: \\\"VersionTLS12\\\"\\n tlsCipherSuites: \\\"\\\"\\n\\n extraContainers: []\\n\\n # speaker contains configuration specific to the MetalLB speaker\\n # daemonset.\\n speaker:\\n enabled: true\\n # command: /speaker\\n # -- Speaker log level. Must be one of: `all`, `debug`, `info`, `warn`, `error` or `none`\\n logLevel: info\\n tolerateMaster: true\\n memberlist:\\n enabled: true\\n mlBindPort: 7946\\n mlBindAddrOverride: \\\"\\\"\\n mlSecretKeyPath: \\\"/etc/ml_secret_key\\\"\\n excludeInterfaces:\\n enabled: true\\n # ignore the exclude-from-external-loadbalancer label (required for 1-node clusters are all-control-plane clusters)\\n ignoreExcludeLB: false\\n\\n image:\\n repository: gcr.io/spectro-images-public/packs/metallb/0.14.8/speaker\\n tag: v0.14.8\\n pullPolicy:\\n ## @param speaker.updateStrategy.type Speaker daemonset strategy type\\n ## ref: https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/\\n ##\\n updateStrategy:\\n ## StrategyType\\n ## Can be set to RollingUpdate or OnDelete\\n ##\\n type: RollingUpdate\\n serviceAccount:\\n # Specifies whether a ServiceAccount should be created\\n create: true\\n # The name of the ServiceAccount to use. If not set and create is\\n # true, a name is generated using the fullname template\\n name: \\\"\\\"\\n annotations: {}\\n securityContext: {}\\n ## Defines a secret name for the controller to generate a memberlist encryption secret\\n ## By default secretName: {{ \\\"metallb.fullname\\\" }}-memberlist\\n ##\\n # secretName:\\n resources: {}\\n # limits:\\n # cpu: 100m\\n # memory: 100Mi\\n nodeSelector: {}\\n tolerations: []\\n priorityClassName: \\\"\\\"\\n affinity: {}\\n ## Selects which runtime class will be used by the pod.\\n runtimeClassName: \\\"\\\"\\n podAnnotations: {}\\n labels: {}\\n livenessProbe:\\n enabled: true\\n failureThreshold: 3\\n initialDelaySeconds: 10\\n periodSeconds: 10\\n successThreshold: 1\\n timeoutSeconds: 1\\n readinessProbe:\\n enabled: true\\n failureThreshold: 3\\n initialDelaySeconds: 10\\n periodSeconds: 10\\n successThreshold: 1\\n timeoutSeconds: 1\\n startupProbe:\\n enabled: true\\n failureThreshold: 30\\n periodSeconds: 5\\n # frr contains configuration specific to the MetalLB FRR container,\\n # for speaker running alongside FRR.\\n frr:\\n enabled: false\\n image:\\n repository: gcr.io/spectro-images-public/packs/metallb/0.14.8/frr\\n tag: 9.1.0\\n pullPolicy:\\n metricsPort: 7473\\n resources: {}\\n # if set, enables a rbac proxy sidecar container on the speaker to\\n # expose the frr metrics via tls.\\n # secureMetricsPort: 9121\\n\\n\\n reloader:\\n resources: {}\\n\\n frrMetrics:\\n resources: {}\\n\\n extraContainers: []\\n\\n crds:\\n enabled: true\\n validationFailurePolicy: Fail\\n\\n # frrk8s contains the configuration related to using an frrk8s instance\\n # (github.com/metallb/frr-k8s) as the backend for the BGP implementation.\\n # This allows configuring additional frr parameters in combination to those\\n # applied by MetalLB.\\n frrk8s:\\n # if set, enables frrk8s as a backend. This is mutually exclusive to frr\\n # mode.\\n enabled: false\\n external: false\\n namespace: \\\"\\\"\","
                    - generic [ref=e374]: "\"registry\": {"
                    - generic [ref=e375]: "\"metadata\": {"
                    - generic [ref=e376]: "\"uid\": \"5eecc89d0b150045ae661cef\","
                    - generic [ref=e377]: "\"name\": \"Public Repo\","
                    - generic [ref=e378]: "\"kind\": \"pack\","
                    - generic [ref=e379]: "\"isPrivate\": false,"
                    - generic [ref=e380]: "\"providerType\": \"\""
                    - generic [ref=e381]: "}"
                    - generic [ref=e382]: "}"
                    - generic [ref=e383]: "},"
                    - generic [ref=e384]: "{"
                    - generic [ref=e385]: "\"name\": \"hello-universe\","
                    - generic [ref=e386]: "\"type\": \"oci\","
                    - generic [ref=e387]: "\"layer\": \"addon\","
                    - generic [ref=e388]: "\"version\": \"1.2.0\","
                    - generic [ref=e389]: "\"tag\": \"1.2.0\","
                    - generic [ref=e390]: "\"values\": \"# spectrocloud.com/enabled-presets: Backend:disable-api\\npack:\\n content:\\n images:\\n - image: ghcr.io/spectrocloud/hello-universe:1.2.0\\n spectrocloud.com/install-priority: 0\\n\\nmanifests:\\n hello-universe:\\n images:\\n hellouniverse: ghcr.io/spectrocloud/hello-universe:1.2.0\\n apiEnabled: false\\n namespace: hello-universe\\n port: 8080\\n replicas: 1\","
                    - generic [ref=e391]: "\"registry\": {"
                    - generic [ref=e392]: "\"metadata\": {"
                    - generic [ref=e393]: "\"uid\": \"64eaff5630402973c4e1856a\","
                    - generic [ref=e394]: "\"name\": \"Palette Community Registry\","
                    - generic [ref=e395]: "\"kind\": \"oci\","
                    - generic [ref=e396]: "\"isPrivate\": true,"
                    - generic [ref=e397]: "\"providerType\": \"pack\""
                    - generic [ref=e398]: "}"
                    - generic [ref=e399]: "}"
                    - generic [ref=e400]: "}"
                    - generic [ref=e401]: "]"
                    - generic [ref=e402]: "},"
                    - generic [ref=e403]: "\"variables\": []"
                    - generic [ref=e404]: "}"
                    - generic [ref=e405]: "}"
                  - generic [ref=e406]:
                    - button "Toggle word wrap" [ref=e407] [cursor=pointer]:
                      - img [ref=e408]
                    - button "Copy code to clipboard" [ref=e410] [cursor=pointer]:
                      - generic [ref=e411]:
                        - img [ref=e412]
                        - img [ref=e414]
                - paragraph [ref=e416]:
                  - text: Click on
                  - strong [ref=e417]: Confirm
                  - text: . Palette creates a new cluster profile named
                  - strong [ref=e418]: vmware-profile
                  - text: .
                - paragraph [ref=e419]:
                  - text: On the
                  - strong [ref=e420]: Profiles
                  - text: list, select
                  - strong [ref=e421]: Project
                  - text: from the
                  - strong [ref=e422]: Contexts
                  - text: drop-down. Your newly created cluster profile displays. The Palette UI confirms that the cluster profile was created in the scope of the
                  - strong [ref=e423]: Project-ScaleSecureTutorial
                  - text: .
                - paragraph [ref=e424]:
                  - img "Image that shows the cluster profile" [ref=e425]
                - paragraph [ref=e426]: Select the cluster profile to view its details. The cluster profile summary appears.
                - paragraph [ref=e427]:
                  - text: This cluster profile deploys the
                  - link "Hello Universe" [ref=e428] [cursor=pointer]:
                    - /url: https://github.com/spectrocloud/hello-universe
                  - text: application using a pack. Click on the
                  - code [ref=e429]: hellouniverse 1.2.0
                  - text: layer. The pack manifest editor appears.
                - paragraph [ref=e430]:
                  - text: Click on
                  - strong [ref=e431]: Presets
                  - text: on the right-hand side. You can learn more about the pack presets on the pack README, which is available in the Palette UI. Select the
                  - strong [ref=e432]: Enable Hello Universe API
                  - text: preset. The pack manifest changes accordingly.
                - paragraph [ref=e433]:
                  - img "Screenshot of pack presets" [ref=e434]
                - paragraph [ref=e435]:
                  - text: The pack requires two values to be replaced for the authorization token and for the database password when using this preset. Replace these values with your own base64 encoded values. The
                  - link "hello-universe" [ref=e436] [cursor=pointer]:
                    - /url: https://github.com/spectrocloud/hello-universe?tab=readme-ov-file#single-load-balancer
                    - emphasis [ref=e437]: hello-universe
                  - text: repository provides a token that you can use.
                - paragraph [ref=e438]:
                  - text: Click on
                  - strong [ref=e439]: Confirm Updates
                  - text: . The manifest editor closes.
                - paragraph [ref=e440]:
                  - text: Click on the
                  - strong [ref=e441]: lb-metallb-helm
                  - text: layer. The pack manifest editor appears.
                - paragraph [ref=e442]:
                  - text: Replace the predefined
                  - code [ref=e443]: 192.168.10.0/24
                  - text: IP CIDR listed below the
                  - strong [ref=e444]: addresses
                  - text: line with a valid IP address or IP range from your VMware environment to be assigned to your load balancer.
                - paragraph [ref=e445]:
                  - img "MetalLb Helm-based pack." [ref=e446]
                - paragraph [ref=e447]:
                  - text: Click on
                  - strong [ref=e448]: Confirm Updates
                  - text: . The manifest editor closes. Then, click on
                  - strong [ref=e449]: Save Changes
                  - text: to save your updates.
                - heading "Deploy a ClusterDirect link to Deploy a Cluster" [level=2] [ref=e450]:
                  - text: Deploy a Cluster
                  - link "Direct link to Deploy a Cluster" [ref=e451] [cursor=pointer]:
                    - /url: "#deploy-a-cluster"
                    - text: "#"
                - paragraph [ref=e452]:
                  - text: Navigate to the left
                  - strong [ref=e453]: Main Menu
                  - text: and select
                  - strong [ref=e454]: Clusters
                  - text: . Click on
                  - strong [ref=e455]: Create Cluster
                  - text: .
                - paragraph [ref=e456]:
                  - text: Palette will prompt you to select the type of cluster. Select
                  - strong [ref=e457]: VMware
                  - text: and click on
                  - strong [ref=e458]: Start VMware Configuration
                  - text: .
                - paragraph [ref=e459]:
                  - text: Continue with the rest of the cluster deployment flow using the cluster profile you created in the
                  - link "Import a Cluster Profile" [ref=e460] [cursor=pointer]:
                    - /url: "#import-a-cluster-profile"
                  - text: section, named
                  - strong [ref=e461]: vmware-profile
                  - text: . Refer to the
                  - link "Deploy a Cluster" [ref=e462] [cursor=pointer]:
                    - /url: /tutorials/getting-started/palette/vmware/deploy-k8s-cluster/#deploy-a-cluster
                  - text: tutorial for additional guidance or if you need a refresher of the Palette deployment flow.
                - heading "Verify the ApplicationDirect link to Verify the Application" [level=3] [ref=e463]:
                  - text: Verify the Application
                  - link "Direct link to Verify the Application" [ref=e464] [cursor=pointer]:
                    - /url: "#verify-the-application"
                    - text: "#"
                - paragraph [ref=e465]:
                  - text: Navigate to the left
                  - strong [ref=e466]: Main Menu
                  - text: and select
                  - strong [ref=e467]: Clusters
                  - text: .
                - paragraph [ref=e468]:
                  - text: Select your cluster to view its
                  - strong [ref=e469]: Overview
                  - text: tab.
                - paragraph [ref=e470]:
                  - text: When the application is deployed and ready for network traffic, Palette exposes the service URL in the
                  - strong [ref=e471]: Services
                  - text: field. Click on the URL for port
                  - strong [ref=e472]: :8080
                  - text: to access the Hello Universe application.
                - paragraph [ref=e473]:
                  - img "Cluster details page with service URL highlighted" [ref=e474]
                - heading "Upgrade Kubernetes VersionsDirect link to Upgrade Kubernetes Versions" [level=2] [ref=e475]:
                  - text: Upgrade Kubernetes Versions
                  - link "Direct link to Upgrade Kubernetes Versions" [ref=e476] [cursor=pointer]:
                    - /url: "#upgrade-kubernetes-versions"
                    - text: "#"
                - paragraph [ref=e477]: Regularly upgrading your Kubernetes version is an important part of maintaining a good security posture. New versions may contain important patches to security vulnerabilities and bugs that could affect the integrity and availability of your clusters.
                - paragraph [ref=e478]: Palette supports three minor Kubernetes versions at any given time. We support the current release and the three previous minor version releases, also known as N-3. For example, if the current release is 1.29, we support 1.28, 1.27, and 1.26.
                - generic [ref=e479]:
                  - generic [ref=e480]:
                    - img [ref=e482]
                    - text: warning
                  - paragraph [ref=e485]: Once you upgrade your cluster to a new Kubernetes version, you will not be able to downgrade.
                - paragraph [ref=e486]: We recommend using cluster profile versions to safely upgrade any layer of your cluster profile and maintain the security of your clusters. Expand the following section to learn how to create a new cluster profile version with a Kubernetes upgrade.
                - group [ref=e487]:
                  - generic "Upgrade Kubernetes using Cluster Profile Versions" [ref=e488] [cursor=pointer]
                - paragraph [ref=e489]:
                  - text: Upgrading the Kubernetes version of your cluster modifies an infrastructure layer. Therefore, Kubernetes needs to replace its nodes. This is known as a repave. Check out the
                  - link "Node Pools" [ref=e490] [cursor=pointer]:
                    - /url: /clusters/cluster-management/node-pool/#repave-behavior-and-configuration
                  - text: page to learn more about the repave behavior and configuration.
                - paragraph [ref=e491]:
                  - text: Click on the
                  - strong [ref=e492]: Nodes
                  - text: tab. You can follow along with the node upgrades on this screen. Palette replaces the nodes configured with the old Kubernetes version with newly upgraded ones. This may affect the performance of your application, as Kubernetes swaps the workloads to the upgraded nodes.
                - paragraph [ref=e493]:
                  - img "Node repaves in progress" [ref=e494]
                - heading "Verify the ApplicationDirect link to Verify the Application" [level=3] [ref=e495]:
                  - text: Verify the Application
                  - link "Direct link to Verify the Application" [ref=e496] [cursor=pointer]:
                    - /url: "#verify-the-application-1"
                    - text: "#"
                - paragraph [ref=e497]:
                  - text: The cluster update completes when the Palette UI marks the cluster profile layers as green and the cluster is in a
                  - strong [ref=e498]: Healthy
                  - text: state. The cluster
                  - strong [ref=e499]: Overview
                  - text: page also displays the Kubernetes version as
                  - strong [ref=e500]: "1.28"
                  - text: . Click on the URL for port
                  - strong [ref=e501]: :8080
                  - text: to access the application and verify that your upgraded cluster is functional.
                - paragraph [ref=e502]:
                  - img "Kubernetes upgrade applied" [ref=e503]
                - heading "Scan ClustersDirect link to Scan Clusters" [level=2] [ref=e504]:
                  - text: Scan Clusters
                  - link "Direct link to Scan Clusters" [ref=e505] [cursor=pointer]:
                    - /url: "#scan-clusters"
                    - text: "#"
                - paragraph [ref=e506]: Palette provides compliance, security, conformance, and Software Bill of Materials (SBOM) scans on tenant clusters. These scans ensure cluster adherence to specific compliance and security standards, as well as detect potential vulnerabilities. You can perform four types of scans on your cluster.
                - table [ref=e507]:
                  - rowgroup [ref=e508]:
                    - row "Scan Description" [ref=e509]:
                      - cell "Scan" [ref=e510]
                      - cell "Description" [ref=e511]
                  - rowgroup [ref=e512]:
                    - row "Kubernetes Configuration Security This scan examines the compliance of deployed security features against the CIS Kubernetes Benchmarks, which are consensus-driven security guidelines for Kubernetes. By default, the test set will execute based on the cluster Kubernetes version." [ref=e513]:
                      - cell "Kubernetes Configuration Security" [ref=e514]
                      - cell "This scan examines the compliance of deployed security features against the CIS Kubernetes Benchmarks, which are consensus-driven security guidelines for Kubernetes. By default, the test set will execute based on the cluster Kubernetes version." [ref=e515]
                    - row "Kubernetes Penetration Testing This scan evaluates Kubernetes-related open-ports for any configuration issues that can leave the tenant clusters exposed to attackers. It hunts for security issues in your clusters and increases visibility of the security controls in your Kubernetes environments." [ref=e516]:
                      - cell "Kubernetes Penetration Testing" [ref=e517]
                      - cell "This scan evaluates Kubernetes-related open-ports for any configuration issues that can leave the tenant clusters exposed to attackers. It hunts for security issues in your clusters and increases visibility of the security controls in your Kubernetes environments." [ref=e518]
                    - row "Kubernetes Conformance Testing This scan validates your Kubernetes configuration to ensure that it conforms to CNCF specifications. Palette leverages an open source tool called Sonobuoy to perform this scan." [ref=e519]:
                      - cell "Kubernetes Conformance Testing" [ref=e520]
                      - cell "This scan validates your Kubernetes configuration to ensure that it conforms to CNCF specifications. Palette leverages an open source tool called Sonobuoy to perform this scan." [ref=e521]:
                        - text: This scan validates your Kubernetes configuration to ensure that it conforms to CNCF specifications. Palette leverages an open source tool called
                        - link "Sonobuoy" [ref=e522] [cursor=pointer]:
                          - /url: https://sonobuoy.io
                        - text: to perform this scan.
                    - row "Software Bill of Materials (SBOM) This scan details the various third-party components and dependencies used by your workloads and helps to manage security and compliance risks associated with those components." [ref=e523]:
                      - cell "Software Bill of Materials (SBOM)" [ref=e524]
                      - cell "This scan details the various third-party components and dependencies used by your workloads and helps to manage security and compliance risks associated with those components." [ref=e525]
                - paragraph [ref=e526]:
                  - text: Navigate to the left
                  - strong [ref=e527]: Main Menu
                  - text: and select
                  - strong [ref=e528]: Clusters
                  - text: . Select your cluster to view its
                  - strong [ref=e529]: Overview
                  - text: tab.
                - paragraph [ref=e530]:
                  - text: Select the
                  - strong [ref=e531]: Scan
                  - text: tab. The list of all the available cluster scans appears. Palette indicates that you have never scanned your cluster.
                - paragraph [ref=e532]:
                  - img "Scans never performed on the cluster" [ref=e533]
                - paragraph [ref=e534]:
                  - text: Click
                  - strong [ref=e535]: Run Scan
                  - text: on the
                  - strong [ref=e536]: Kubernetes configuration security
                  - text: and
                  - strong [ref=e537]: Kubernetes penetration testing
                  - text: scans. Palette schedules and executes these scans on your cluster, which may take a few minutes. Once they complete, you can download the report in PDF, CSV or view the results directly in the Palette UI.
                - paragraph [ref=e538]:
                  - img "Scans completed on the cluster" [ref=e539]
                - paragraph [ref=e540]:
                  - text: Click on
                  - strong [ref=e541]: Configure Scan
                  - text: on the
                  - strong [ref=e542]: Software Bill of Materials (SBOM)
                  - text: scan. The
                  - strong [ref=e543]: Configure SBOM Scan
                  - text: dialog appears.
                - paragraph [ref=e544]:
                  - text: Leave the default selections on this screen and click on
                  - strong [ref=e545]: Confirm
                  - text: . Optionally, you can configure an S3 bucket to save your report into. Refer to the
                  - link "Configure an SBOM Scan" [ref=e546] [cursor=pointer]:
                    - /url: /clusters/cluster-management/compliance-scan/#configure-an-sbom-scan
                  - text: guide to learn more about the configuration options of this scan.
                - paragraph [ref=e547]: Once the scan completes, click on the report to view it within the Palette UI. The third-party dependencies that your workloads rely on are evaluated for potential security vulnerabilities. Reviewing the SBOM enables organizations to track vulnerabilities, perform regular software maintenance, and ensure compliance with regulatory requirements.
                - generic [ref=e548]:
                  - generic [ref=e549]:
                    - img [ref=e551]
                    - text: info
                  - paragraph [ref=e554]: The scan reports highlight any failed checks, based on Kubernetes community standards and CNCF requirements. We recommend that you prioritize the rectification of any identified issues.
                - paragraph [ref=e555]: As you have seen so far, Palette scans are crucial when maintaining your security posture. Palette provides the ability to schedule your scans and periodically evaluate your clusters. In addition, it keeps a history of previous scans for comparison purposes. Expand the following section to learn how to configure scan schedules for your cluster.
                - group [ref=e556]:
                  - generic "Configure Cluster Scan Schedules" [ref=e557] [cursor=pointer]
                - heading "Scale a ClusterDirect link to Scale a Cluster" [level=2] [ref=e558]:
                  - text: Scale a Cluster
                  - link "Direct link to Scale a Cluster" [ref=e559] [cursor=pointer]:
                    - /url: "#scale-a-cluster"
                    - text: "#"
                - paragraph [ref=e560]: A node pool is a group of nodes within a cluster that all have the same configuration. You can use node pools for different workloads. For example, you can create a node pool for your production workloads and another for your development workloads. You can update node pools for active clusters or create a new one for the cluster.
                - paragraph [ref=e561]:
                  - text: Navigate to the left
                  - strong [ref=e562]: Main Menu
                  - text: and select
                  - strong [ref=e563]: Clusters
                  - text: . Select your cluster to view its
                  - strong [ref=e564]: Overview
                  - text: tab.
                - paragraph [ref=e565]:
                  - text: Select the
                  - strong [ref=e566]: Nodes
                  - text: tab. Your cluster has a
                  - strong [ref=e567]: control-plane-pool
                  - text: and a
                  - strong [ref=e568]: worker-pool
                  - text: . Each pool contains one node.
                - paragraph [ref=e569]:
                  - text: Select the
                  - strong [ref=e570]: Overview
                  - text: tab. Download the
                  - link "kubeconfig" [ref=e571] [cursor=pointer]:
                    - /url: /clusters/cluster-management/kubeconfig/
                  - text: file.
                - paragraph [ref=e572]:
                  - img "kubeconfig download" [ref=e573]
                - paragraph [ref=e574]:
                  - text: Open a terminal window and set the environment variable
                  - code [ref=e575]: KUBECONFIG
                  - text: to point to the file you downloaded.
                - generic [ref=e577]:
                  - code [ref=e579]:
                    - generic [ref=e580]: export KUBECONFIG=~/Downloads/admin.vmware-cluster.kubeconfig
                  - button "Copy code to clipboard" [ref=e582] [cursor=pointer]:
                    - generic [ref=e583]:
                      - img [ref=e584]
                      - img [ref=e586]
                - paragraph [ref=e588]: Execute the following command in your terminal to view the nodes of your cluster.
                - generic [ref=e590]:
                  - code [ref=e592]:
                    - generic [ref=e593]: kubectl get nodes
                  - button "Copy code to clipboard" [ref=e595] [cursor=pointer]:
                    - generic [ref=e596]:
                      - img [ref=e597]
                      - img [ref=e599]
                - paragraph [ref=e601]:
                  - text: The output reveals two nodes, one for the worker pool and one for the control plane. Make a note of the name of your worker node, which is the node that does not have the
                  - code [ref=e602]: control-plane
                  - text: role. In the example below,
                  - code [ref=e603]: vmware-cluster-worker-pool-7d6d76b55b-dhffq
                  - text: is the name of the worker node.
                - generic [ref=e605]:
                  - code [ref=e607]:
                    - generic [ref=e608]: NAME STATUS ROLES AGE VERSION
                    - generic [ref=e609]: vmware-cluster-cp-xcqlw Ready control-plane 28m v1.28.13
                    - generic [ref=e610]: vmware-cluster-worker-pool-7d6d76b55b-dhffq Ready <none> 28m v1.28.13
                  - generic [ref=e611]:
                    - button "Toggle word wrap" [ref=e612] [cursor=pointer]:
                      - img [ref=e613]
                    - button "Copy code to clipboard" [ref=e615] [cursor=pointer]:
                      - generic [ref=e616]:
                        - img [ref=e617]
                        - img [ref=e619]
                - paragraph [ref=e621]:
                  - text: The Hello Universe pack deploys three pods in the
                  - code [ref=e622]: hello-universe
                  - text: namespace. Execute the following command to verify where these pods have been scheduled.
                - generic [ref=e624]:
                  - code [ref=e626]:
                    - generic [ref=e627]: kubectl get pods --namespace hello-universe --output wide
                  - button "Copy code to clipboard" [ref=e629] [cursor=pointer]:
                    - generic [ref=e630]:
                      - img [ref=e631]
                      - img [ref=e633]
                - paragraph [ref=e635]: The output verifies that all of the pods have been scheduled on the worker node you made a note of previously.
                - generic [ref=e637]:
                  - code [ref=e639]:
                    - generic [ref=e640]: NAME READY STATUS AGE NODE
                    - generic [ref=e641]: api-7db799cf85-5w5l6 1/1 Running 20m vmware-cluster-worker-pool-7d6d76b55b-dhffq
                    - generic [ref=e642]: postgres-698d7ff8f4-vbktf 1/1 Running 20m vmware-cluster-worker-pool-7d6d76b55b-dhffq
                    - generic [ref=e643]: ui-5f777c76df-pplcv 1/1 Running 20m vmware-cluster-worker-pool-7d6d76b55b-dhffq
                  - generic [ref=e644]:
                    - button "Toggle word wrap" [ref=e645] [cursor=pointer]:
                      - img [ref=e646]
                    - button "Copy code to clipboard" [ref=e648] [cursor=pointer]:
                      - generic [ref=e649]:
                        - img [ref=e650]
                        - img [ref=e652]
                - paragraph [ref=e654]:
                  - text: Navigate back to the Palette UI in your browser. Select the
                  - strong [ref=e655]: Nodes
                  - text: tab.
                - paragraph [ref=e656]:
                  - text: Click on
                  - strong [ref=e657]: New Node Pool
                  - text: . The
                  - strong [ref=e658]: Add node pool
                  - text: dialog appears. This workflow allows you to create a new worker pool for your cluster. Fill in the following configuration.
                - table [ref=e659]:
                  - rowgroup [ref=e660]:
                    - row "Field Value Description" [ref=e661]:
                      - cell "Field" [ref=e662]
                      - cell "Value" [ref=e663]
                      - cell "Description" [ref=e664]
                  - rowgroup [ref=e665]:
                    - row "Node pool name worker-pool-2 The name of your worker pool." [ref=e666]:
                      - cell "Node pool name" [ref=e667]:
                        - strong [ref=e668]: Node pool name
                      - cell "worker-pool-2" [ref=e669]:
                        - code [ref=e670]: worker-pool-2
                      - cell "The name of your worker pool." [ref=e671]
                    - row "Enable Autoscaler Enabled Scale the worker pool horizontally based on its per-node workload counts. The Minimum size specifies the lower bound of nodes in the pool, and the Maximum size specifies the upper bound." [ref=e672]:
                      - cell "Enable Autoscaler" [ref=e673]:
                        - strong [ref=e674]: Enable Autoscaler
                      - cell "Enabled" [ref=e675]
                      - cell "Scale the worker pool horizontally based on its per-node workload counts. The Minimum size specifies the lower bound of nodes in the pool, and the Maximum size specifies the upper bound." [ref=e676]:
                        - text: Scale the worker pool horizontally based on its per-node workload counts. The
                        - strong [ref=e677]: Minimum size
                        - text: specifies the lower bound of nodes in the pool, and the
                        - strong [ref=e678]: Maximum size
                        - text: specifies the upper bound.
                    - row "CPU 4 cores Set the number of CPUs equal to the already provisioned nodes." [ref=e679]:
                      - cell "CPU" [ref=e680]:
                        - strong [ref=e681]: CPU
                      - cell "4 cores" [ref=e682]
                      - cell "Set the number of CPUs equal to the already provisioned nodes." [ref=e683]
                    - row "Memory 8 GB Set the memory allocation equal to the already provisioned nodes." [ref=e684]:
                      - cell "Memory" [ref=e685]:
                        - strong [ref=e686]: Memory
                      - cell "8 GB" [ref=e687]
                      - cell "Set the memory allocation equal to the already provisioned nodes." [ref=e688]
                    - row "Disk 60 GB Set the disk space equal to the already provisioned nodes." [ref=e689]:
                      - cell "Disk" [ref=e690]:
                        - strong [ref=e691]: Disk
                      - cell "60 GB" [ref=e692]
                      - cell "Set the disk space equal to the already provisioned nodes." [ref=e693]
                - paragraph [ref=e694]:
                  - text: Next, populate the
                  - strong [ref=e695]: Compute cluster
                  - text: ","
                  - strong [ref=e696]: Resource Pool
                  - text: ","
                  - strong [ref=e697]: Datastore
                  - text: ", and"
                  - strong [ref=e698]: Network
                  - text: fields according to your VMware vSphere environment.
                - paragraph [ref=e699]:
                  - text: Click on
                  - strong [ref=e700]: Confirm
                  - text: . The dialog closes. Palette begins provisioning your node pool. Once the process completes, your three node pools appear in a healthy state.
                - paragraph [ref=e701]:
                  - img "New worker pool provisioned" [ref=e702]
                - paragraph [ref=e703]: Navigate back to your terminal and execute the following command in your terminal to view the nodes of your cluster.
                - generic [ref=e705]:
                  - code [ref=e707]:
                    - generic [ref=e708]: kubectl get nodes
                  - button "Copy code to clipboard" [ref=e710] [cursor=pointer]:
                    - generic [ref=e711]:
                      - img [ref=e712]
                      - img [ref=e714]
                - paragraph [ref=e716]:
                  - text: The output reveals three nodes, two for worker pools and one for the control plane. Make a note of the names of your worker nodes. In the example below,
                  - code [ref=e717]: vmware-cluster-worker-pool-7d6d76b55b-dhffq
                  - text: and
                  - code [ref=e718]: vmware-cluster-worker-pool-2-5b4b559f6d-znbtm
                  - text: are the worker nodes.
                - generic [ref=e720]:
                  - code [ref=e722]:
                    - generic [ref=e723]: NAME STATUS ROLES AGE VERSION
                    - generic [ref=e724]: vmware-cluster-cp-xcqlw Ready control-plane 58m v1.28.13
                    - generic [ref=e725]: vmware-cluster-worker-pool-2-5b4b559f6d-znbtm Ready <none> 30m v1.28.13
                    - generic [ref=e726]: vmware-cluster-worker-pool-7d6d76b55b-dhffq Ready <none> 58m v1.28.13
                  - generic [ref=e727]:
                    - button "Toggle word wrap" [ref=e728] [cursor=pointer]:
                      - img [ref=e729]
                    - button "Copy code to clipboard" [ref=e731] [cursor=pointer]:
                      - generic [ref=e732]:
                        - img [ref=e733]
                        - img [ref=e735]
                - paragraph [ref=e737]: It is common to dedicate node pools to a particular type of workload. One way to specify this is through the use of Kubernetes taints and tolerations.
                - paragraph [ref=e738]: Taints provide nodes with the ability to repel a set of pods, allowing you to mark nodes as unavailable for certain pods. Tolerations are applied to pods and allow the pods to schedule onto nodes with matching taints. Once configured, nodes do not accept any pods that do not tolerate the taints.
                - paragraph [ref=e739]: The animation below provides a visual representation of how taints and tolerations can be used to specify which workloads execute on which nodes.
                - paragraph [ref=e740]
                - paragraph [ref=e741]:
                  - text: Switch back to Palette in your web browser. Navigate to the left
                  - strong [ref=e742]: Main Menu
                  - text: and select
                  - strong [ref=e743]: Profiles
                  - text: . Select the cluster profile deployed to your cluster, named
                  - code [ref=e744]: vmware-profile
                  - text: . Ensure that the
                  - strong [ref=e745]: 1.1.0
                  - text: version is selected.
                - paragraph [ref=e746]:
                  - text: Click on the
                  - code [ref=e747]: hellouniverse 1.2.0
                  - text: layer. The manifest editor appears. Set the
                  - code [ref=e748]: manifests.hello-universe.ui.useTolerations
                  - text: field on line 19 to
                  - code [ref=e749]: "true"
                  - text: . Then, set the
                  - code [ref=e750]: manifests.hello-universe.ui.effect
                  - text: field on line 21 to
                  - code [ref=e751]: NoExecute
                  - text: . This toleration describes that the UI pods of Hello Universe will tolerate the taint with the effect
                  - code [ref=e752]: NoExecute
                  - text: ", key"
                  - code [ref=e753]: app
                  - text: ", and value"
                  - code [ref=e754]: ui
                  - text: . The tolerations of the UI pods should be as below.
                - generic [ref=e756]:
                  - code [ref=e758]:
                    - generic [ref=e759]: "ui:"
                    - generic [ref=e760]: "useTolerations: true"
                    - generic [ref=e761]: "tolerations:"
                    - generic [ref=e762]: "effect: NoExecute"
                    - generic [ref=e763]: "key: app"
                    - generic [ref=e764]: "value: ui"
                  - button "Copy code to clipboard" [ref=e766] [cursor=pointer]:
                    - generic [ref=e767]:
                      - img [ref=e768]
                      - img [ref=e770]
                - paragraph [ref=e772]:
                  - text: Click on
                  - strong [ref=e773]: Confirm Updates
                  - text: . The manifest editor closes. Then, click on
                  - strong [ref=e774]: Save Changes
                  - text: to persist your changes.
                - paragraph [ref=e775]:
                  - text: Navigate to the left
                  - strong [ref=e776]: Main Menu
                  - text: and select
                  - strong [ref=e777]: Clusters
                  - text: . Select your deployed cluster, named
                  - strong [ref=e778]: vmware-cluster
                  - text: .
                - paragraph [ref=e779]:
                  - text: Due to the changes you have made to the cluster profile, this cluster has a pending update. Click on
                  - strong [ref=e780]: Updates
                  - text: . The
                  - strong [ref=e781]: Changes Summary
                  - text: dialog appears.
                - paragraph [ref=e782]:
                  - text: Click on
                  - strong [ref=e783]: Review Changes in Editor
                  - text: . The
                  - strong [ref=e784]: Review Update Changes
                  - text: dialog appears. The toleration changes appear as incoming configuration.
                - paragraph [ref=e785]:
                  - text: Click on
                  - strong [ref=e786]: Apply Changes
                  - text: to apply the update to your cluster.
                - paragraph [ref=e787]:
                  - text: Select the
                  - strong [ref=e788]: Nodes
                  - text: tab. Click on
                  - strong [ref=e789]: Edit
                  - text: on the first worker pool, named
                  - strong [ref=e790]: worker-pool
                  - text: . The
                  - strong [ref=e791]: Edit node pool
                  - text: dialog appears.
                - paragraph [ref=e792]:
                  - text: Click on
                  - strong [ref=e793]: Add New Taint
                  - text: in the
                  - strong [ref=e794]: Taints
                  - text: section. Fill in
                  - code [ref=e795]: app
                  - text: for the
                  - strong [ref=e796]: Key
                  - text: ","
                  - code [ref=e797]: ui
                  - text: for the
                  - strong [ref=e798]: Value
                  - text: and select
                  - code [ref=e799]: NoExecute
                  - text: for the
                  - strong [ref=e800]: Effect
                  - text: . These values match the toleration you specified in your cluster profile earlier.
                - paragraph [ref=e801]:
                  - img "Add taint to worker pool" [ref=e802]
                - paragraph [ref=e803]:
                  - text: Click on
                  - strong [ref=e804]: Confirm
                  - text: to save your changes. The nodes in the
                  - code [ref=e805]: worker-pool
                  - text: can now only execute the UI pods that have a toleration matching the configured taint.
                - paragraph [ref=e806]: Switch back to your terminal. Execute the following command again to verify where the Hello Universe pods have been scheduled.
                - generic [ref=e808]:
                  - code [ref=e810]:
                    - generic [ref=e811]: kubectl get pods --namespace hello-universe --output wide
                  - button "Copy code to clipboard" [ref=e813] [cursor=pointer]:
                    - generic [ref=e814]:
                      - img [ref=e815]
                      - img [ref=e817]
                - paragraph [ref=e819]:
                  - text: The output verifies that the UI pods have remained scheduled on their original node named
                  - code [ref=e820]: vmware-cluster-worker-pool-7d6d76b55b-dhffq
                  - text: ", while the other two pods have been moved to the node of the second worker pool named"
                  - code [ref=e821]: vmware-cluster-worker-pool-2-5b4b559f6d-znbtm
                  - text: .
                - generic [ref=e823]:
                  - code [ref=e825]:
                    - generic [ref=e826]: NAME READY STATUS AGE NODE
                    - generic [ref=e827]: api-7db799cf85-5w5l6 1/1 Running 20m vmware-cluster-worker-pool-2-5b4b559f6d-znbtm
                    - generic [ref=e828]: postgres-698d7ff8f4-vbktf 1/1 Running 20m vmware-cluster-worker-pool-2-5b4b559f6d-znbtm
                    - generic [ref=e829]: ui-5f777c76df-pplcv 1/1 Running 20m vmware-cluster-worker-pool-7d6d76b55b-dhffq
                  - generic [ref=e830]:
                    - button "Toggle word wrap" [ref=e831] [cursor=pointer]:
                      - img [ref=e832]
                    - button "Copy code to clipboard" [ref=e834] [cursor=pointer]:
                      - generic [ref=e835]:
                        - img [ref=e836]
                        - img [ref=e838]
                - paragraph [ref=e840]:
                  - text: Taints and tolerations are a common way of creating nodes dedicated to certain workloads, once the cluster has scaled accordingly through its provisioned node pools. Refer to the
                  - link "Taints and Tolerations" [ref=e841] [cursor=pointer]:
                    - /url: /clusters/cluster-management/taints/
                  - text: guide to learn more.
                - heading "Verify the ApplicationDirect link to Verify the Application" [level=3] [ref=e842]:
                  - text: Verify the Application
                  - link "Direct link to Verify the Application" [ref=e843] [cursor=pointer]:
                    - /url: "#verify-the-application-2"
                    - text: "#"
                - paragraph [ref=e844]:
                  - text: Select the
                  - strong [ref=e845]: Overview
                  - text: tab. Click on the URL for port
                  - strong [ref=e846]: :8080
                  - text: to access the Hello Universe application and verify that the application is functioning correctly.
                - heading "CleanupDirect link to Cleanup" [level=2] [ref=e847]:
                  - text: Cleanup
                  - link "Direct link to Cleanup" [ref=e848] [cursor=pointer]:
                    - /url: "#cleanup"
                    - text: "#"
                - paragraph [ref=e849]: Use the following steps to remove all the resources you created for the tutorial.
                - paragraph [ref=e850]:
                  - text: To remove the cluster, navigate to the left
                  - strong [ref=e851]: Main Menu
                  - text: and click on
                  - strong [ref=e852]: Clusters
                  - text: . Select the cluster you want to delete to access its details page.
                - paragraph [ref=e853]:
                  - text: Click on
                  - strong [ref=e854]: Settings
                  - text: to expand the menu, and select
                  - strong [ref=e855]: Delete Cluster
                  - text: .
                - paragraph [ref=e856]:
                  - img "Delete cluster" [ref=e857]
                - paragraph [ref=e858]:
                  - text: You will be prompted to type in the cluster name to confirm the delete action. Type in the cluster name
                  - code [ref=e859]: vmware-cluster
                  - text: to proceed with the delete step. The deletion process takes several minutes to complete.
                - generic [ref=e860]:
                  - generic [ref=e861]:
                    - img [ref=e863]
                    - text: info
                  - paragraph [ref=e866]:
                    - text: If a cluster remains in the delete phase for over 15 minutes, it becomes eligible for a force delete. To trigger a force delete, navigate to the clusterâ€™s details page, click on
                    - strong [ref=e867]: Settings
                    - text: ", then select"
                    - strong [ref=e868]: Force Delete Cluster
                    - text: . Palette automatically removes clusters stuck in the cluster deletion phase for over 24 hours.
                - paragraph [ref=e869]:
                  - text: Once the cluster is deleted, navigate to the left
                  - strong [ref=e870]: Main Menu
                  - text: and click on
                  - strong [ref=e871]: Profiles
                  - text: . Find the cluster profile you created and click on the
                  - strong [ref=e872]: three-dot Menu
                  - text: to display the
                  - strong [ref=e873]: Delete
                  - text: button. Select
                  - strong [ref=e874]: Delete
                  - text: and confirm the selection to remove the cluster profile.
                - paragraph [ref=e875]:
                  - text: Click on the
                  - strong [ref=e876]: drop-down Menu
                  - text: at the top of the page and switch to
                  - strong [ref=e877]: Tenant Admin
                  - text: scope.
                - paragraph [ref=e878]:
                  - text: Navigate to the left
                  - strong [ref=e879]: Main Menu
                  - text: and click on
                  - strong [ref=e880]: Projects
                  - text: .
                - paragraph [ref=e881]:
                  - text: Click on the
                  - strong [ref=e882]: three-dot Menu
                  - text: of the
                  - strong [ref=e883]: Project-ScaleSecureTutorial
                  - text: and select
                  - strong [ref=e884]: Delete
                  - text: . A pop-up box will ask you to confirm the action. Confirm the deletion.
                - paragraph [ref=e885]:
                  - text: Navigate to the left
                  - strong [ref=e886]: Main Menu
                  - text: and click on
                  - strong [ref=e887]: Users & Teams
                  - text: . Select the
                  - strong [ref=e888]: Teams
                  - text: tab.
                - paragraph [ref=e889]:
                  - text: Click on
                  - strong [ref=e890]: scale-secure-tutorial-team
                  - text: list entry. The
                  - strong [ref=e891]: Team Details
                  - text: pane appears. Click on
                  - strong [ref=e892]: Delete Team
                  - text: . A pop-up box will ask you to confirm the action. Confirm the deletion.
                - heading "Wrap-upDirect link to Wrap-up" [level=2] [ref=e893]:
                  - text: Wrap-up
                  - link "Direct link to Wrap-up" [ref=e894] [cursor=pointer]:
                    - /url: "#wrap-up"
                    - text: "#"
                - paragraph [ref=e895]: In this tutorial, you learned how to perform very important operations relating to the scalability and availability of your clusters. First, you created a project and team. Next, you imported a cluster profile and deployed a host VMware vSphere cluster. Then, you upgraded the Kubernetes version of your cluster and scanned your clusters using Palette's scanning capabilities. Finally, you scaled your cluster's nodes and used taints to select which Hello Universe pods execute on them.
                - paragraph [ref=e896]:
                  - text: We encourage you to check out the
                  - link "Additional Capabilities" [ref=e897] [cursor=pointer]:
                    - /url: /tutorials/getting-started/additional-capabilities/
                  - text: to explore other Palette functionalities.
                - heading "ðŸ§‘â€ðŸš€ Catch up with SpacetasticDirect link to ðŸ§‘â€ðŸš€ Catch up with Spacetastic" [level=2] [ref=e898]:
                  - text: ðŸ§‘â€ðŸš€ Catch up with Spacetastic
                  - link "Direct link to ðŸ§‘â€ðŸš€ Catch up with Spacetastic" [ref=e899] [cursor=pointer]:
                    - /url: "#-catch-up-with-spacetastic"
                    - text: "#"
                - paragraph [ref=e900]: After going through the steps in the tutorial, Kai is confident in Palette's upgrade and scanning capabilities.
                - blockquote [ref=e901]:
                  - paragraph [ref=e902]: "\"What have you found out, Kai?\" says Meera walking over to Kai's desk. \"Can I rely on Palette when a zero-day vulnerability comes in?\""
                  - paragraph [ref=e903]: "\"Yes, I know how stressful it is when those are reported,\" says Kai with a sympathetic nod. \"I found out that Palette has our security covered through their pack updates and scanning capabilities. Relying on this kind of tooling is invaluable to security-conscious engineers like us.\""
                  - paragraph [ref=e904]: "\"Excellent! These capabilities will be a great addition to our existing systems at Spacetastic,\" says Meera with a big grin."
                  - paragraph [ref=e905]: "\"I'm so glad that we found a platform that can support everyone!\" says Kai. \"There is so much more to explore though. I will keep reading through the Getting Started section and find out what additional capabilities Palette provides.\""
                  - paragraph [ref=e906]: "\"Good thinking, Kai,\" says Meera, nodding. \"We should maximize all of Palette's features now that we have implemented it in production. We've got big ideas and goals on our company roadmap, so let's find out how Palette can help us deliver them.\""
              - generic [ref=e907]:
                - generic [ref=e909]:
                  - text: "Tags:"
                  - list [ref=e910]:
                    - listitem [ref=e911]:
                      - link "getting-started" [ref=e912] [cursor=pointer]:
                        - /url: /tags/getting-started/
                    - listitem [ref=e913]:
                      - link "vmware" [ref=e914] [cursor=pointer]:
                        - /url: /tags/vmware/
                    - listitem [ref=e915]:
                      - link "tutorial" [ref=e916] [cursor=pointer]:
                        - /url: /tags/tutorial/
                - link "Edit this page" [ref=e919] [cursor=pointer]:
                  - /url: https://github.com/spectrocloud/librarium/blob/master/docs/docs-content/tutorials/getting-started/palette/vmware/scale-secure-cluster.md
                  - img [ref=e920]
                  - text: Edit this page
            - navigation "Docs pages" [ref=e924]:
              - link "Previous Â« Cluster Management with Terraform" [ref=e925] [cursor=pointer]:
                - /url: /tutorials/getting-started/palette/vmware/deploy-manage-k8s-cluster-tf/
                - generic [ref=e926]: Previous
                - generic [ref=e927]: Â« Cluster Management with Terraform
              - link "Next Additional Capabilities Â»" [ref=e928] [cursor=pointer]:
                - /url: /tutorials/getting-started/additional-capabilities/
                - generic [ref=e929]: Next
                - generic [ref=e930]: Additional Capabilities Â»
          - list [ref=e933]:
            - listitem [ref=e934]:
              - link "ðŸ§‘â€ðŸš€ Back at Spacetastic HQ" [ref=e935] [cursor=pointer]:
                - /url: "#-back-at-spacetastic-hq"
            - listitem [ref=e936]:
              - link "Prerequisites" [ref=e937] [cursor=pointer]:
                - /url: "#prerequisites"
            - listitem [ref=e938]:
              - link "Create Palette Projects" [ref=e939] [cursor=pointer]:
                - /url: "#create-palette-projects"
            - listitem [ref=e940]:
              - link "Import a Cluster Profile" [ref=e941] [cursor=pointer]:
                - /url: "#import-a-cluster-profile"
            - listitem [ref=e942]:
              - link "Deploy a Cluster" [ref=e943] [cursor=pointer]:
                - /url: "#deploy-a-cluster"
              - list [ref=e944]:
                - listitem [ref=e945]:
                  - link "Verify the Application" [ref=e946] [cursor=pointer]:
                    - /url: "#verify-the-application"
            - listitem [ref=e947]:
              - link "Upgrade Kubernetes Versions" [ref=e948] [cursor=pointer]:
                - /url: "#upgrade-kubernetes-versions"
              - list [ref=e949]:
                - listitem [ref=e950]:
                  - link "Verify the Application" [ref=e951] [cursor=pointer]:
                    - /url: "#verify-the-application-1"
            - listitem [ref=e952]:
              - link "Scan Clusters" [ref=e953] [cursor=pointer]:
                - /url: "#scan-clusters"
            - listitem [ref=e954]:
              - link "Scale a Cluster" [ref=e955] [cursor=pointer]:
                - /url: "#scale-a-cluster"
              - list [ref=e956]:
                - listitem [ref=e957]:
                  - link "Verify the Application" [ref=e958] [cursor=pointer]:
                    - /url: "#verify-the-application-2"
            - listitem [ref=e959]:
              - link "Cleanup" [ref=e960] [cursor=pointer]:
                - /url: "#cleanup"
            - listitem [ref=e961]:
              - link "Wrap-up" [ref=e962] [cursor=pointer]:
                - /url: "#wrap-up"
            - listitem [ref=e963]:
              - link "ðŸ§‘â€ðŸš€ Catch up with Spacetastic" [ref=e964] [cursor=pointer]:
                - /url: "#-catch-up-with-spacetastic"
  - button "Project Logo Ask AI" [ref=e965] [cursor=pointer]:
    - generic [ref=e968]:
      - img "Project Logo" [ref=e969]
      - paragraph [ref=e970]: Ask AI
```