# Page snapshot

```yaml
- generic [active] [ref=e1]:
  - generic [ref=e2]:
    - region "Skip to main content":
      - link "Skip to main content" [ref=e3] [cursor=pointer]:
        - /url: "#__docusaurus_skipToContent_fallback"
    - navigation "Main" [ref=e4]:
      - generic [ref=e5]:
        - generic [ref=e6]:
          - link "Spectro cloud logo" [ref=e7] [cursor=pointer]:
            - /url: /
            - img "Spectro cloud logo" [ref=e9]
          - link "Tutorials" [ref=e10] [cursor=pointer]:
            - /url: /tutorials/
          - link "Downloads" [ref=e11] [cursor=pointer]:
            - /url: /downloads/
          - link "API" [ref=e12] [cursor=pointer]:
            - /url: /api/introduction/
          - button "latest" [ref=e14] [cursor=pointer]
        - generic [ref=e15]:
          - link "Go to Spectro Cloud homepage (opens in a new tab)" [ref=e17] [cursor=pointer]:
            - /url: https://spectrocloud.com
            - text: spectrocloud.com ↗
          - link "GitHub repository" [ref=e18] [cursor=pointer]:
            - /url: https://github.com/spectrocloud/librarium
          - button "Ask AI" [ref=e20] [cursor=pointer]
          - button "Switch between dark and light mode (currently system mode)" [ref=e22] [cursor=pointer]:
            - img [ref=e23]
          - button "Search (Control+k)" [ref=e26] [cursor=pointer]:
            - generic [ref=e27]:
              - img [ref=e28]
              - generic [ref=e31]: Search
    - generic [ref=e35]:
      - complementary [ref=e36]:
        - generic [ref=e38]:
          - link "Spectro cloud logo" [ref=e39] [cursor=pointer]:
            - /url: /
            - img "Spectro cloud logo" [ref=e40]
          - navigation "Docs sidebar" [ref=e41]:
            - list [ref=e42]:
              - listitem [ref=e43]:
                - generic [ref=e44]:
                  - link "Release Notes" [expanded] [ref=e45] [cursor=pointer]:
                    - /url: /release-notes/
                    - img [ref=e47]
                    - text: Release Notes
                  - button "Toggle the collapsible sidebar category 'Release Notes'" [ref=e49] [cursor=pointer]
                - list [ref=e50]:
                  - listitem [ref=e51]:
                    - link "Announcements" [ref=e52] [cursor=pointer]:
                      - /url: /release-notes/announcements/
                  - listitem [ref=e53]:
                    - link "Known Issues" [ref=e54] [cursor=pointer]:
                      - /url: /release-notes/known-issues/
                  - listitem [ref=e55]:
                    - link "Find Breaking Changes" [ref=e56] [cursor=pointer]:
                      - /url: /release-notes/breaking-changes/
              - listitem [ref=e57]:
                - generic [ref=e58]:
                  - link "Security Bulletins" [ref=e59] [cursor=pointer]:
                    - /url: /security-bulletins/
                    - img [ref=e61]
                    - text: Security Bulletins
                  - button "Toggle the collapsible sidebar category 'Security Bulletins'" [ref=e63] [cursor=pointer]
              - listitem [ref=e64]:
                - generic [ref=e65]:
                  - link "What is Palette?" [ref=e66] [cursor=pointer]:
                    - /url: /
                    - img [ref=e68]
                    - text: What is Palette?
                  - button "Toggle the collapsible sidebar category 'What is Palette?'" [ref=e70] [cursor=pointer]
              - listitem [ref=e71]:
                - link "Getting Started" [ref=e72] [cursor=pointer]:
                  - /url: /getting-started/
                  - img [ref=e74]
                  - text: Getting Started
              - listitem [ref=e76]:
                - generic [ref=e77]:
                  - link "Architecture" [ref=e78] [cursor=pointer]:
                    - /url: /architecture/
                    - img [ref=e80]
                    - text: Architecture
                  - button "Toggle the collapsible sidebar category 'Architecture'" [ref=e82] [cursor=pointer]
              - listitem [ref=e83]:
                - generic [ref=e84]:
                  - link "Profiles" [ref=e85] [cursor=pointer]:
                    - /url: /profiles/
                    - img [ref=e87]
                    - text: Profiles
                  - button "Toggle the collapsible sidebar category 'Profiles'" [ref=e91] [cursor=pointer]
              - listitem [ref=e92]:
                - generic [ref=e93]:
                  - link "Cluster Templates" [ref=e94] [cursor=pointer]:
                    - /url: /cluster-templates/
                    - img [ref=e96]
                    - text: Cluster Templates
                  - button "Toggle the collapsible sidebar category 'Cluster Templates'" [ref=e98] [cursor=pointer]
              - listitem [ref=e99]:
                - generic [ref=e100]:
                  - link "Deployment Modes" [ref=e101] [cursor=pointer]:
                    - /url: /deployment-modes/
                    - img [ref=e103]
                    - text: Deployment Modes
                  - button "Toggle the collapsible sidebar category 'Deployment Modes'" [ref=e105] [cursor=pointer]
              - listitem [ref=e106]:
                - generic [ref=e107]:
                  - link "Clusters" [ref=e108] [cursor=pointer]:
                    - /url: /clusters/
                    - img [ref=e110]
                    - text: Clusters
                  - button "Toggle the collapsible sidebar category 'Clusters'" [ref=e116] [cursor=pointer]
              - listitem [ref=e117]:
                - generic [ref=e118]:
                  - link "Bring Your Own OS (BYOOS)" [ref=e119] [cursor=pointer]:
                    - /url: /byoos/
                    - img [ref=e121]
                    - text: Bring Your Own OS (BYOOS)
                  - button "Toggle the collapsible sidebar category 'Bring Your Own OS (BYOOS)'" [ref=e123] [cursor=pointer]
              - listitem [ref=e124]:
                - generic [ref=e125]:
                  - link "Palette Dev Engine" [ref=e126] [cursor=pointer]:
                    - /url: /devx/
                    - img [ref=e128]
                    - text: Palette Dev Engine
                  - button "Toggle the collapsible sidebar category 'Palette Dev Engine'" [ref=e130] [cursor=pointer]
              - listitem [ref=e131]:
                - generic [ref=e132]:
                  - link "Virtual Machine Orchestrator" [ref=e133] [cursor=pointer]:
                    - /url: /vm-management/
                    - img [ref=e135]
                    - text: Virtual Machine Orchestrator
                  - button "Toggle the collapsible sidebar category 'Virtual Machine Orchestrator'" [ref=e137] [cursor=pointer]
              - listitem [ref=e138]:
                - generic [ref=e139]:
                  - link "Workspaces" [ref=e140] [cursor=pointer]:
                    - /url: /workspace/
                    - img [ref=e142]
                    - text: Workspaces
                  - button "Toggle the collapsible sidebar category 'Workspaces'" [ref=e144] [cursor=pointer]
              - listitem [ref=e145]:
                - generic [ref=e146]:
                  - link "Packs List" [ref=e147] [cursor=pointer]:
                    - /url: /integrations/
                    - img [ref=e149]
                    - text: Packs List
                  - button "Toggle the collapsible sidebar category 'Packs List'" [ref=e151] [cursor=pointer]
              - listitem [ref=e152]:
                - generic [ref=e153]:
                  - link "User & Role Management" [ref=e154] [cursor=pointer]:
                    - /url: /user-management/
                    - img [ref=e156]
                    - text: User & Role Management
                  - button "Toggle the collapsible sidebar category 'User & Role Management'" [ref=e158] [cursor=pointer]
              - listitem [ref=e159]:
                - generic [ref=e160]:
                  - link "Registries and Packs" [ref=e161] [cursor=pointer]:
                    - /url: /registries-and-packs/
                    - img [ref=e163]
                    - text: Registries and Packs
                  - button "Toggle the collapsible sidebar category 'Registries and Packs'" [ref=e165] [cursor=pointer]
              - listitem [ref=e166]:
                - generic [ref=e167]:
                  - link "Security" [ref=e168] [cursor=pointer]:
                    - /url: /security/
                    - img [ref=e170]
                    - text: Security
                  - button "Toggle the collapsible sidebar category 'Security'" [ref=e172] [cursor=pointer]
              - listitem [ref=e173]:
                - generic [ref=e174]:
                  - link "Audit Logs" [ref=e175] [cursor=pointer]:
                    - /url: /audit-logs/
                    - img [ref=e177]
                    - text: Audit Logs
                  - button "Toggle the collapsible sidebar category 'Audit Logs'" [ref=e179] [cursor=pointer]
              - listitem [ref=e180]:
                - generic [ref=e181]:
                  - link "Self-Hosted Palette" [ref=e182] [cursor=pointer]:
                    - /url: /enterprise-version/
                    - img [ref=e184]
                    - text: Self-Hosted Palette
                  - button "Toggle the collapsible sidebar category 'Self-Hosted Palette'" [ref=e186] [cursor=pointer]
              - listitem [ref=e187]:
                - generic [ref=e188]:
                  - link "Palette VerteX" [ref=e189] [cursor=pointer]:
                    - /url: /vertex/
                    - img [ref=e191]
                    - text: Palette VerteX
                  - button "Toggle the collapsible sidebar category 'Palette VerteX'" [ref=e193] [cursor=pointer]
              - listitem [ref=e194]:
                - generic [ref=e195]:
                  - link "Tenant Administration" [ref=e196] [cursor=pointer]:
                    - /url: /tenant-settings/
                    - img [ref=e198]
                    - text: Tenant Administration
                  - button "Toggle the collapsible sidebar category 'Tenant Administration'" [ref=e200] [cursor=pointer]
              - listitem [ref=e201]:
                - generic [ref=e202]:
                  - link "Automation" [ref=e203] [cursor=pointer]:
                    - /url: /automation/
                    - img [ref=e205]
                    - text: Automation
                  - button "Toggle the collapsible sidebar category 'Automation'" [ref=e207] [cursor=pointer]
              - listitem [ref=e208]:
                - generic [ref=e209]:
                  - link "Troubleshooting" [ref=e210] [cursor=pointer]:
                    - /url: /troubleshooting/
                    - img [ref=e212]
                    - text: Troubleshooting
                  - button "Toggle the collapsible sidebar category 'Troubleshooting'" [ref=e214] [cursor=pointer]
              - listitem [ref=e215]:
                - link "Glossary" [ref=e216] [cursor=pointer]:
                  - /url: /glossary-all/
                  - img [ref=e218]
                  - text: Glossary
              - listitem [ref=e220]:
                - generic [ref=e221]:
                  - link "Compliance & Legal" [ref=e222] [cursor=pointer]:
                    - /url: /legal-licenses/
                    - img [ref=e224]
                    - text: Compliance & Legal
                  - button "Toggle the collapsible sidebar category 'Compliance & Legal'" [ref=e226] [cursor=pointer]
              - listitem [ref=e227]:
                - button "Privacy Settings" [ref=e228] [cursor=pointer]:
                  - img [ref=e229]
                  - text: Privacy Settings
      - main [ref=e231]:
        - generic [ref=e233]:
          - generic [ref=e235]:
            - article [ref=e236]:
              - navigation "Breadcrumbs" [ref=e237]:
                - list [ref=e238]:
                  - listitem [ref=e239]:
                    - link "Home page" [ref=e240] [cursor=pointer]:
                      - /url: /
                      - img [ref=e241]
                  - listitem [ref=e243]:
                    - link "Release Notes" [ref=e244] [cursor=pointer]:
                      - /url: /release-notes/
                  - listitem [ref=e245]:
                    - generic [ref=e246]: Known Issues
              - generic [ref=e247]:
                - heading "Known Issues" [level=1] [ref=e249]
                - paragraph [ref=e250]: Throughout the development lifecycle of Palette, known issues may arise that affect the user experience. Use this page to review and stay informed about the status of known issues in Palette. As issues are resolved, this page is updated.
                - heading "Active Known IssuesDirect link to Active Known Issues" [level=2] [ref=e251]:
                  - text: Active Known Issues
                  - link "Direct link to Active Known Issues" [ref=e252] [cursor=pointer]:
                    - /url: "#active-known-issues"
                    - text: "#"
                - paragraph [ref=e253]: The following table lists all known issues that are currently active and affecting users.
                - table [ref=e254]:
                  - rowgroup [ref=e255]:
                    - row "Description Workaround Publish Date Product Component" [ref=e256]:
                      - cell "Description" [ref=e257]
                      - cell "Workaround" [ref=e258]
                      - cell "Publish Date" [ref=e259]
                      - cell "Product Component" [ref=e260]
                  - rowgroup [ref=e261]:
                    - row "Palette Management Appliance and VerteX Management Appliance installations may become stuck due to ErrImagePull errors on the Calico pods. This affects all Palette versions later than 4.8.0. Restart the node on which the pod is scheduled and wait for the pod to recover. January 27, 2025 Clusters, Packs" [ref=e262]:
                      - cell "Palette Management Appliance and VerteX Management Appliance installations may become stuck due to ErrImagePull errors on the Calico pods. This affects all Palette versions later than 4.8.0." [ref=e263]:
                        - link "Palette Management Appliance" [ref=e264] [cursor=pointer]:
                          - /url: /enterprise-version/install-palette/palette-management-appliance/
                        - text: and
                        - link "VerteX Management Appliance" [ref=e265] [cursor=pointer]:
                          - /url: /vertex/install-palette-vertex/vertex-management-appliance/
                        - text: installations may become stuck due to
                        - code [ref=e266]: ErrImagePull
                        - text: errors on the
                        - link "Calico" [ref=e267] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=cni-calico
                        - text: pods. This affects all Palette versions later than 4.8.0.
                      - cell "Restart the node on which the pod is scheduled and wait for the pod to recover." [ref=e268]
                      - cell "January 27, 2025" [ref=e269]
                      - cell "Clusters, Packs" [ref=e270]
                    - 'row "AWS EKS clusters using Kubernetes (EKS) version 1.33 with Palette as an Identity Provider (IdP) fail to deploy. This is due to IdentityProviderConfigName not being automatically injected into the values.yaml file. In the managedControlPlane.oidcIdentityProvider section of your cluster profile''s Kubernetes layer, add the value identityProviderConfigName: \"eks-oidc\". December 30, 2025 Packs" [ref=e271]':
                      - cell "AWS EKS clusters using Kubernetes (EKS) version 1.33 with Palette as an Identity Provider (IdP) fail to deploy. This is due to IdentityProviderConfigName not being automatically injected into the values.yaml file." [ref=e272]:
                        - text: AWS EKS clusters using
                        - link "Kubernetes (EKS)" [ref=e273] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=kubernetes-eks
                        - text: version 1.33 with Palette as an Identity Provider (IdP) fail to deploy. This is due to
                        - code [ref=e274]: IdentityProviderConfigName
                        - text: not being automatically injected into the
                        - code [ref=e275]: values.yaml
                        - text: file.
                      - 'cell "In the managedControlPlane.oidcIdentityProvider section of your cluster profile''s Kubernetes layer, add the value identityProviderConfigName: \"eks-oidc\"." [ref=e276]':
                        - text: In the
                        - code [ref=e277]: managedControlPlane.oidcIdentityProvider
                        - text: section of your cluster profile's Kubernetes layer, add the value
                        - code [ref=e278]: "identityProviderConfigName: \"eks-oidc\""
                        - text: .
                      - cell "December 30, 2025" [ref=e279]
                      - cell "Packs" [ref=e280]
                    - row "Velero backups may fail with errors Timed out awaiting reconciliation of volumesnapshot hipster-app/velero-data-redis-cart-2-zctck. This occurs because the CloudStack CSI Helm chart is missing the required csi-snapshotter sidecar container in the controller deployment, preventing CSI snapshot operations from executing. As a result, all CSI-based snapshots and any Velero backups that rely on them time out and fail. No workaround available. December 17, 2025 Packs" [ref=e281]:
                      - cell "Velero backups may fail with errors Timed out awaiting reconciliation of volumesnapshot hipster-app/velero-data-redis-cart-2-zctck. This occurs because the CloudStack CSI Helm chart is missing the required csi-snapshotter sidecar container in the controller deployment, preventing CSI snapshot operations from executing. As a result, all CSI-based snapshots and any Velero backups that rely on them time out and fail." [ref=e282]:
                        - text: Velero backups may fail with errors
                        - code [ref=e283]: Timed out awaiting reconciliation of volumesnapshot hipster-app/velero-data-redis-cart-2-zctck
                        - text: . This occurs because the CloudStack CSI Helm chart is missing the required csi-snapshotter sidecar container in the controller deployment, preventing CSI snapshot operations from executing. As a result, all CSI-based snapshots and any Velero backups that rely on them time out and fail.
                      - cell "No workaround available." [ref=e284]
                      - cell "December 17, 2025" [ref=e285]
                      - cell "Packs" [ref=e286]
                    - row "Edge hosts using encrypted persistent partitions may fail to complete a reset. A limitation in Kairos version 3.5.3 prevents these partitions from being reinitialized during the reset workflow, resulting in the operation stopping prematurely. Refer to Troubleshooting - Edge for the workaround. December 17, 2025 Edge" [ref=e287]:
                      - cell "Edge hosts using encrypted persistent partitions may fail to complete a reset. A limitation in Kairos version 3.5.3 prevents these partitions from being reinitialized during the reset workflow, resulting in the operation stopping prematurely." [ref=e288]
                      - cell "Refer to Troubleshooting - Edge for the workaround." [ref=e289]:
                        - text: Refer to
                        - link "Troubleshooting - Edge" [ref=e290] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---edge-host-reset-fails-with-encrypted-persistent-partition
                        - text: for the workaround.
                      - cell "December 17, 2025" [ref=e291]
                      - cell "Edge" [ref=e292]
                    - row "After upgrading to Palette Management Appliance or VerteX Management Appliance 4.8.10, the new stylus version is not reflected in Local UI, which continues to display the previous value of 4.7.16. No workaround available. December 12, 2025 Edge" [ref=e293]:
                      - cell "After upgrading to Palette Management Appliance or VerteX Management Appliance 4.8.10, the new stylus version is not reflected in Local UI, which continues to display the previous value of 4.7.16." [ref=e294]:
                        - text: After upgrading to
                        - link "Palette Management Appliance" [ref=e295] [cursor=pointer]:
                          - /url: /enterprise-version/install-palette/palette-management-appliance/
                        - text: or
                        - link "VerteX Management Appliance" [ref=e296] [cursor=pointer]:
                          - /url: /vertex/install-palette-vertex/vertex-management-appliance/
                        - text: 4.8.10, the new
                        - code [ref=e297]: stylus
                        - text: version is not reflected in
                        - link "Local UI" [ref=e298] [cursor=pointer]:
                          - /url: /clusters/edge/local-ui/
                        - text: ", which continues to display the previous value of 4.7.16."
                      - cell "No workaround available." [ref=e299]
                      - cell "December 12, 2025" [ref=e300]
                      - cell "Edge" [ref=e301]
                    - row "Agent mode Edge cluster nodes configured with Agent version 4.8.1 may lose pre-existing systemd-networkd configurations (VLANs, bonds, and bridges) after a reboot. This occurs because the cleanup logic intended to remove stale VLAN configurations inadvertently deletes valid, pre-existing configurations not created through user data. Pre-configure the nodes with the required systemd-networkd settings via user data, so the Agent recognizes these as managed configurations and retains them on boot. December 5, 2025 Edge" [ref=e302]:
                      - cell "Agent mode Edge cluster nodes configured with Agent version 4.8.1 may lose pre-existing systemd-networkd configurations (VLANs, bonds, and bridges) after a reboot. This occurs because the cleanup logic intended to remove stale VLAN configurations inadvertently deletes valid, pre-existing configurations not created through user data." [ref=e303]:
                        - link "Agent mode" [ref=e304] [cursor=pointer]:
                          - /url: /deployment-modes/agent-mode/
                        - text: Edge cluster nodes configured with Agent version 4.8.1 may lose pre-existing
                        - code [ref=e305]: systemd-networkd
                        - text: configurations (VLANs, bonds, and bridges) after a reboot. This occurs because the cleanup logic intended to remove stale VLAN configurations inadvertently deletes valid, pre-existing configurations not created through user data.
                      - cell "Pre-configure the nodes with the required systemd-networkd settings via user data, so the Agent recognizes these as managed configurations and retains them on boot." [ref=e306]:
                        - text: Pre-configure the nodes with the required
                        - code [ref=e307]: systemd-networkd
                        - text: settings via
                        - link "user data" [ref=e308] [cursor=pointer]:
                          - /url: /clusters/edge/edge-configuration/installer-reference/
                        - text: ", so the Agent recognizes these as managed configurations and retains them on boot."
                      - cell "December 5, 2025" [ref=e309]
                      - cell "Edge" [ref=e310]
                    - row "The Palette CLI fails to deploy PCGs on environments where SSO is enabled. For environments configured with SSO, specify the organization specific endpoint using the --console-url option on the Palette CLI Login command. November 28, 2025 Automation" [ref=e311]:
                      - cell "The Palette CLI fails to deploy PCGs on environments where SSO is enabled." [ref=e312]:
                        - text: The
                        - link "Palette CLI" [ref=e313] [cursor=pointer]:
                          - /url: /automation/palette-cli/
                        - text: fails to deploy
                        - link "PCGs" [ref=e314] [cursor=pointer]:
                          - /url: /clusters/pcg/
                        - text: on environments where SSO is enabled.
                      - cell "For environments configured with SSO, specify the organization specific endpoint using the --console-url option on the Palette CLI Login command." [ref=e315]:
                        - text: For environments configured with SSO, specify the organization specific endpoint using the
                        - code [ref=e316]: "--console-url"
                        - text: option on the Palette CLI
                        - link "Login" [ref=e317] [cursor=pointer]:
                          - /url: /automation/palette-cli/commands/login/
                        - text: command.
                      - cell "November 28, 2025" [ref=e318]
                      - cell "Automation" [ref=e319]
                    - row "After upgrading to 4.8.8, the self-hosted Palette and VerteX management cluster appears to be Unhealthy in the Palette UI due to mongodb-key-manager-manifest errors. The cluster functionality is not affected and will return to Healthy after a few hours. No workaround. November 24, 2025 Self-hosted, VerteX" [ref=e320]:
                      - cell "After upgrading to 4.8.8, the self-hosted Palette and VerteX management cluster appears to be Unhealthy in the Palette UI due to mongodb-key-manager-manifest errors. The cluster functionality is not affected and will return to Healthy after a few hours." [ref=e321]:
                        - text: After upgrading to 4.8.8, the
                        - link "self-hosted Palette" [ref=e322] [cursor=pointer]:
                          - /url: /enterprise-version/
                        - text: and
                        - link "VerteX" [ref=e323] [cursor=pointer]:
                          - /url: /vertex/
                        - text: management cluster appears to be
                        - strong [ref=e324]: Unhealthy
                        - text: in the Palette UI due to
                        - code [ref=e325]: mongodb-key-manager-manifest
                        - text: errors. The cluster functionality is not affected and will return to
                        - strong [ref=e326]: Healthy
                        - text: after a few hours.
                      - cell "No workaround." [ref=e327]
                      - cell "November 24, 2025" [ref=e328]
                      - cell "Self-hosted, VerteX" [ref=e329]
                    - 'row "The Terminal User Interface (TUI) introduced in Palette version 4.8.6, using CanvOS version 4.8.1, does not start automatically on Edge hosts upgraded directly from Palette versions prior to 4.6.32, using CanvOS versions earlier than 4.6.21. Reboot the hosts, or issue the following command on each host: - systemctl restart palette-tui for appliance mode - systemctl restart spectro-palette-tui for agent mode November 22, 2025 Edge" [ref=e330]':
                      - cell "The Terminal User Interface (TUI) introduced in Palette version 4.8.6, using CanvOS version 4.8.1, does not start automatically on Edge hosts upgraded directly from Palette versions prior to 4.6.32, using CanvOS versions earlier than 4.6.21." [ref=e331]:
                        - text: The
                        - link "Terminal User Interface (TUI)" [ref=e332] [cursor=pointer]:
                          - /url: /clusters/edge/site-deployment/site-installation/initial-setup/
                        - text: introduced in Palette version 4.8.6, using CanvOS version 4.8.1, does not start automatically on Edge hosts upgraded directly from Palette versions prior to 4.6.32, using CanvOS versions earlier than 4.6.21.
                      - 'cell "Reboot the hosts, or issue the following command on each host: - systemctl restart palette-tui for appliance mode - systemctl restart spectro-palette-tui for agent mode" [ref=e333]':
                        - text: "Reboot the hosts, or issue the following command on each host:"
                        - text: "-"
                        - code [ref=e334]: systemctl restart palette-tui
                        - text: for
                        - strong [ref=e335]: appliance mode
                        - text: "-"
                        - code [ref=e336]: systemctl restart spectro-palette-tui
                        - text: for
                        - strong [ref=e337]: agent mode
                      - cell "November 22, 2025" [ref=e338]
                      - cell "Edge" [ref=e339]
                    - 'row "On Edge Native clusters, restores of security-hardened applications (such as Argo CD) may fail with the affected application pods stuck in the Init:CreateContainerConfigError state, showing Error: container has runAsNonRoot and image has non-numeric user (cnb), cannot verify user is non-root on the restore-wait init container. Refer to Troubleshooting - Edge for the workaround. November 14, 2025 Edge, Packs" [ref=e340]':
                      - 'cell "On Edge Native clusters, restores of security-hardened applications (such as Argo CD) may fail with the affected application pods stuck in the Init:CreateContainerConfigError state, showing Error: container has runAsNonRoot and image has non-numeric user (cnb), cannot verify user is non-root on the restore-wait init container." [ref=e341]':
                        - text: On Edge Native clusters, restores of security-hardened applications (such as Argo CD) may fail with the affected application pods stuck in the
                        - code [ref=e342]: Init:CreateContainerConfigError
                        - text: state, showing
                        - code [ref=e343]: "Error: container has runAsNonRoot and image has non-numeric user (cnb), cannot verify user is non-root"
                        - text: on the
                        - code [ref=e344]: restore-wait
                        - text: init container.
                      - cell "Refer to Troubleshooting - Edge for the workaround." [ref=e345]:
                        - text: Refer to
                        - link "Troubleshooting - Edge" [ref=e346] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---velero-restore-fails-with-runasnonroot-validation-error
                        - text: for the workaround.
                      - cell "November 14, 2025" [ref=e347]
                      - cell "Edge, Packs" [ref=e348]
                    - row "Users cannot remove proxy values for connected Edge hosts in Local UI due to a validation error. Proxy values can still be added and updated. No workaround available. October 19, 2025 Edge" [ref=e349]:
                      - cell "Users cannot remove proxy values for connected Edge hosts in Local UI due to a validation error. Proxy values can still be added and updated." [ref=e350]
                      - cell "No workaround available." [ref=e351]
                      - cell "October 19, 2025" [ref=e352]
                      - cell "Edge" [ref=e353]
                    - row "Due to strict schema adherence enforcement, Helm charts with parameters that do not exist in the chart schema fail to install on Palette 4.7.15 or later. Remove parameters that do not exist in the chart schema from the pack values. Alternatively, add the missing parameters to the chart schema or remove the chart schema file entirely. September 20, 2025 Packs" [ref=e354]:
                      - cell "Due to strict schema adherence enforcement, Helm charts with parameters that do not exist in the chart schema fail to install on Palette 4.7.15 or later." [ref=e355]:
                        - text: Due to strict schema adherence enforcement,
                        - link "Helm charts" [ref=e356] [cursor=pointer]:
                          - /url: /profiles/cluster-profiles/create-cluster-profiles/create-addon-profile/create-helm-addon/
                        - text: with parameters that do not exist in the chart schema fail to install on Palette 4.7.15 or later.
                      - cell "Remove parameters that do not exist in the chart schema from the pack values. Alternatively, add the missing parameters to the chart schema or remove the chart schema file entirely." [ref=e357]
                      - cell "September 20, 2025" [ref=e358]
                      - cell "Packs" [ref=e359]
                    - 'row "Edge clusters using the Palette Optimized Canonical pack versions 1.32.3 and 1.33.0 may fail to come up because CoreDNS pods do not reach the running state. On existing clusters, CoreDNS pods can fall into a CrashLoopBackOff state with the error exec /bin/pebble: no such file or directory. This is due to a Canonical Kubernetes known issue. The Palette Optimized Canonical pack references the CoreDNS images ghcr.io/canonical/coredns:1.11.3-ck0 in version 1.32.3 and ghcr.io/canonical/coredns:1.11.4-ck1 in version 1.33.0. Both of these images are broken and cause CoreDNS pods to fail. Use the Palette Optimized Canonical pack versions other than 1.32.3 and 1.33.0 which include the fixed CoreDNS image. September 20, 2025 Edge, Packs" [ref=e360]':
                      - 'cell "Edge clusters using the Palette Optimized Canonical pack versions 1.32.3 and 1.33.0 may fail to come up because CoreDNS pods do not reach the running state. On existing clusters, CoreDNS pods can fall into a CrashLoopBackOff state with the error exec /bin/pebble: no such file or directory. This is due to a Canonical Kubernetes known issue. The Palette Optimized Canonical pack references the CoreDNS images ghcr.io/canonical/coredns:1.11.3-ck0 in version 1.32.3 and ghcr.io/canonical/coredns:1.11.4-ck1 in version 1.33.0. Both of these images are broken and cause CoreDNS pods to fail." [ref=e361]':
                        - text: Edge clusters using the
                        - link "Palette Optimized Canonical pack" [ref=e362] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=edge-canonical
                        - text: versions 1.32.3 and 1.33.0 may fail to come up because CoreDNS pods do not reach the running state. On existing clusters, CoreDNS pods can fall into a
                        - code [ref=e363]: CrashLoopBackOff
                        - text: state with the error
                        - code [ref=e364]: "exec /bin/pebble: no such file or directory"
                        - text: . This is due to a
                        - link "Canonical Kubernetes known issue" [ref=e365] [cursor=pointer]:
                          - /url: https://github.com/canonical/k8s-snap/issues/1864
                        - text: . The Palette Optimized Canonical pack references the CoreDNS images
                        - code [ref=e366]: ghcr.io/canonical/coredns:1.11.3-ck0
                        - text: in version 1.32.3 and
                        - code [ref=e367]: ghcr.io/canonical/coredns:1.11.4-ck1
                        - text: in version 1.33.0. Both of these images are broken and cause CoreDNS pods to fail.
                      - cell "Use the Palette Optimized Canonical pack versions other than 1.32.3 and 1.33.0 which include the fixed CoreDNS image." [ref=e368]
                      - cell "September 20, 2025" [ref=e369]
                      - cell "Edge, Packs" [ref=e370]
                    - 'row "Agent mode Edge cluster creation may fail with logs showing the error failed calling webhook \"pod-registry.spectrocloud.com\": tls: failed to verify certificate: x509: certificate signed by unknown authority (\"Spectro Cloud\").... As a result, core components such as CNI, Harbor, and cluster controllers never start. All pods remain in Pending or Failed state. In the Local UI, packs display Invalid date in the Started On and Completed On fields. Refer to Troubleshooting - Edge for the workaround. September 1, 2025 Edge" [ref=e371]':
                      - 'cell "Agent mode Edge cluster creation may fail with logs showing the error failed calling webhook \"pod-registry.spectrocloud.com\": tls: failed to verify certificate: x509: certificate signed by unknown authority (\"Spectro Cloud\").... As a result, core components such as CNI, Harbor, and cluster controllers never start. All pods remain in Pending or Failed state. In the Local UI, packs display Invalid date in the Started On and Completed On fields." [ref=e372]':
                        - text: Agent mode Edge cluster creation may fail with logs showing the error
                        - code [ref=e373]: "failed calling webhook \"pod-registry.spectrocloud.com\": tls: failed to verify certificate: x509: certificate signed by unknown authority (\"Spectro Cloud\")..."
                        - text: . As a result, core components such as CNI, Harbor, and cluster controllers never start. All pods remain in
                        - strong [ref=e374]: Pending
                        - text: or
                        - strong [ref=e375]: Failed
                        - text: state. In the Local UI, packs display
                        - strong [ref=e376]: Invalid date
                        - text: in the
                        - strong [ref=e377]: Started On
                        - text: and
                        - strong [ref=e378]: Completed On
                        - text: fields.
                      - cell "Refer to Troubleshooting - Edge for the workaround." [ref=e379]:
                        - text: Refer to
                        - link "Troubleshooting - Edge" [ref=e380] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---x509-certificate-signed-by-unknown-authority-errors-during-agent-mode-cluster-creation
                        - text: for the workaround.
                      - cell "September 1, 2025" [ref=e381]
                      - cell "Edge" [ref=e382]
                    - row "Virtual Machine Orchestrator (VMO) 4.7.1 cannot be uninstalled due to a missing image. No workaround available. September 1, 2025 Virtual Machine Orchestrator" [ref=e383]:
                      - cell "Virtual Machine Orchestrator (VMO) 4.7.1 cannot be uninstalled due to a missing image." [ref=e384]:
                        - link "Virtual Machine Orchestrator (VMO)" [ref=e385] [cursor=pointer]:
                          - /url: /vm-management/
                        - text: 4.7.1 cannot be uninstalled due to a missing image.
                      - cell "No workaround available." [ref=e386]
                      - cell "September 1, 2025" [ref=e387]
                      - cell "Virtual Machine Orchestrator" [ref=e388]
                    - 'row "After an OS image upgrade in appliance mode, an Edge host may fail to boot into the expected active system image and instead boot into the passive partition as a fallback due to an upgrade failure. When this happens, the Edge host does not automatically rejoin the cluster. The kernel command line (/proc/cmdline) includes the upgrade_failure flag and confirms the system root is set to LABEL=COS_PASSIVE. Recover the Edge host manually using one of the following methods: - Reboot the host and select Palette eXtended Kubernetes – Edge at the GRand Unified Bootloader (GRUB) menu to boot the active image. - Establish an SSH connection to the host and run /usr/bin/grub2-editenv /oem/grubenv set next_entry=cos && reboot. This command updates GRUB to use the boot entry labeled cos (the active image) and reboots the host. September 1, 2025 Edge" [ref=e389]':
                      - cell "After an OS image upgrade in appliance mode, an Edge host may fail to boot into the expected active system image and instead boot into the passive partition as a fallback due to an upgrade failure. When this happens, the Edge host does not automatically rejoin the cluster. The kernel command line (/proc/cmdline) includes the upgrade_failure flag and confirms the system root is set to LABEL=COS_PASSIVE." [ref=e390]:
                        - text: After an OS image upgrade in appliance mode, an Edge host may fail to boot into the expected active system image and instead boot into the passive partition as a fallback due to an upgrade failure. When this happens, the Edge host does not automatically rejoin the cluster. The kernel command line (
                        - code [ref=e391]: /proc/cmdline
                        - text: ) includes the
                        - code [ref=e392]: upgrade_failure
                        - text: flag and confirms the system root is set to
                        - code [ref=e393]: LABEL=COS_PASSIVE
                        - text: .
                      - 'cell "Recover the Edge host manually using one of the following methods: - Reboot the host and select Palette eXtended Kubernetes – Edge at the GRand Unified Bootloader (GRUB) menu to boot the active image. - Establish an SSH connection to the host and run /usr/bin/grub2-editenv /oem/grubenv set next_entry=cos && reboot. This command updates GRUB to use the boot entry labeled cos (the active image) and reboots the host." [ref=e394]':
                        - text: "Recover the Edge host manually using one of the following methods:"
                        - text: "- Reboot the host and select"
                        - strong [ref=e395]: Palette eXtended Kubernetes – Edge
                        - text: at the GRand Unified Bootloader (GRUB) menu to boot the active image.
                        - text: "- Establish an SSH connection to the host and run"
                        - code [ref=e396]: /usr/bin/grub2-editenv /oem/grubenv set next_entry=cos && reboot
                        - text: . This command updates GRUB to use the boot entry labeled
                        - code [ref=e397]: cos
                        - text: (the active image) and reboots the host.
                      - cell "September 1, 2025" [ref=e398]
                      - cell "Edge" [ref=e399]
                    - row "On Azure IaaS clusters created using a Palette version prior to 4.6.32, scaling worker node pools does not attach newly created nodes to an outbound load balancer after upgrading to Palette version 4.6.32 or later and the cluster's Palette Agent version to 4.6.7 or later. This impacts outbound connectivity and may also disassociate existing NAT gateways from the worker node pool subnet, resulting in a loss of egress connectivity. - Multi-Tenant SaaS - No workaround available. - Self-Hosted Palette or VerteX - Before upgrading your self-hosted Palette or VerteX instance to Palette version 4.6.32 or later, pause agent upgrades on any Azure IaaS clusters where you plan to perform Day-2 scaling or repave operations. September 1, 2025 Clusters, Self-Hosted" [ref=e400]:
                      - cell "On Azure IaaS clusters created using a Palette version prior to 4.6.32, scaling worker node pools does not attach newly created nodes to an outbound load balancer after upgrading to Palette version 4.6.32 or later and the cluster's Palette Agent version to 4.6.7 or later. This impacts outbound connectivity and may also disassociate existing NAT gateways from the worker node pool subnet, resulting in a loss of egress connectivity." [ref=e401]
                      - cell "- Multi-Tenant SaaS - No workaround available. - Self-Hosted Palette or VerteX - Before upgrading your self-hosted Palette or VerteX instance to Palette version 4.6.32 or later, pause agent upgrades on any Azure IaaS clusters where you plan to perform Day-2 scaling or repave operations." [ref=e402]:
                        - text: "-"
                        - strong [ref=e403]: Multi-Tenant SaaS
                        - text: "- No workaround available."
                        - text: "-"
                        - strong [ref=e404]: Self-Hosted Palette or VerteX
                        - text: "- Before upgrading your"
                        - link "self-hosted Palette" [ref=e405] [cursor=pointer]:
                          - /url: /enterprise-version/
                        - text: or
                        - link "VerteX" [ref=e406] [cursor=pointer]:
                          - /url: /vertex/
                        - text: instance to Palette version 4.6.32 or later,
                        - link "pause agent upgrades" [ref=e407] [cursor=pointer]:
                          - /url: /clusters/cluster-management/platform-settings/pause-platform-upgrades/
                        - text: on any Azure IaaS clusters where you plan to perform Day-2 scaling or repave operations.
                      - cell "September 1, 2025" [ref=e408]
                      - cell "Clusters, Self-Hosted" [ref=e409]
                    - row "In self-hosted Palette and Vertex Management Appliance environments, uploading the same pack as both a FIPS and non-FIPS version to the same registry overwrites the original pack. For example, if you have a non-FIPS version of the byoi-2.1.0 pack in your Zot registry and you upload the FIPS version of byoi-2.1.0, the new version will overwrite the existing one. This results in a SHA mismatch between the pack stored in the registry and the pack referenced in the cluster profile, which can lead to cluster creation failures. Upload either a FIPS or non-FIPS version of a pack to your registry. Do not upload both to the same registry. September 1, 2025 Clusters, Self-Hosted" [ref=e410]:
                      - cell "In self-hosted Palette and Vertex Management Appliance environments, uploading the same pack as both a FIPS and non-FIPS version to the same registry overwrites the original pack. For example, if you have a non-FIPS version of the byoi-2.1.0 pack in your Zot registry and you upload the FIPS version of byoi-2.1.0, the new version will overwrite the existing one. This results in a SHA mismatch between the pack stored in the registry and the pack referenced in the cluster profile, which can lead to cluster creation failures." [ref=e411]:
                        - text: In self-hosted
                        - link "Palette" [ref=e412] [cursor=pointer]:
                          - /url: /enterprise-version/install-palette/palette-management-appliance/
                        - text: and
                        - link "Vertex Management Appliance" [ref=e413] [cursor=pointer]:
                          - /url: /vertex/install-palette-vertex/vertex-management-appliance/
                        - text: environments, uploading the same pack as both a FIPS and non-FIPS version to the same registry overwrites the original pack.
                        - text: For example, if you have a non-FIPS version of the
                        - code [ref=e414]: byoi-2.1.0
                        - text: pack in your Zot registry and you upload the FIPS version of
                        - code [ref=e415]: byoi-2.1.0
                        - text: ", the new version will overwrite the existing one. This results in a SHA mismatch between the pack stored in the registry and the pack referenced in the cluster profile, which can lead to cluster creation failures."
                      - cell "Upload either a FIPS or non-FIPS version of a pack to your registry. Do not upload both to the same registry." [ref=e416]
                      - cell "September 1, 2025" [ref=e417]
                      - cell "Clusters, Self-Hosted" [ref=e418]
                    - 'row "Cilium may fail to start on MAAS machines that are configured with a br0 interface and are part of a Canonical Kubernetes cluster, displaying errors like daemon creation failed: failed to detect devices: unable to determine direct routing device. Use --direct-routing-device to specify it. This happens because Canonical Kubernetes supports setting various Cilium annotations, but it lacks some fields required for the MAAS br0 network configuration due to a limitation in k8s-snap. Avoid using MAAS machines with a br0 interface when provisioning Canonical Kubernetes clusters. Instead, choose machines whose primary interface is directly connected to the MAAS-managed subnet or VLAN. August 17, 2025 Clusters, Packs" [ref=e419]':
                      - 'cell "Cilium may fail to start on MAAS machines that are configured with a br0 interface and are part of a Canonical Kubernetes cluster, displaying errors like daemon creation failed: failed to detect devices: unable to determine direct routing device. Use --direct-routing-device to specify it. This happens because Canonical Kubernetes supports setting various Cilium annotations, but it lacks some fields required for the MAAS br0 network configuration due to a limitation in k8s-snap." [ref=e420]':
                        - text: Cilium may fail to start on MAAS machines that are configured with a
                        - code [ref=e421]: br0
                        - text: interface and are part of a
                        - link "Canonical Kubernetes" [ref=e422] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=kubernetes-ck8s
                        - text: cluster, displaying errors like
                        - code [ref=e423]: "daemon creation failed: failed to detect devices: unable to determine direct routing device. Use --direct-routing-device to specify it"
                        - text: . This happens because Canonical Kubernetes supports setting various Cilium annotations, but it lacks some fields required for the MAAS
                        - code [ref=e424]: br0
                        - text: network configuration due to
                        - link "a limitation in k8s-snap" [ref=e425] [cursor=pointer]:
                          - /url: https://github.com/canonical/k8s-snap/issues/1740
                          - text: a limitation in
                          - code [ref=e426]: k8s-snap
                        - text: .
                      - cell "Avoid using MAAS machines with a br0 interface when provisioning Canonical Kubernetes clusters. Instead, choose machines whose primary interface is directly connected to the MAAS-managed subnet or VLAN." [ref=e427]:
                        - text: Avoid using MAAS machines with a
                        - code [ref=e428]: br0
                        - text: interface when provisioning Canonical Kubernetes clusters. Instead, choose machines whose primary interface is directly connected to the MAAS-managed subnet or VLAN.
                      - cell "August 17, 2025" [ref=e429]
                      - cell "Clusters, Packs" [ref=e430]
                    - row "Network overlay cluster nodes may display erroneous failed to add static FDB entry after cleanup...Stdout already set, output logs after upgrading the Palette agent to version 4.7.9. Cluster functionality is not affected. No workaround available. August 17, 2025 Edge" [ref=e431]:
                      - cell "Network overlay cluster nodes may display erroneous failed to add static FDB entry after cleanup...Stdout already set, output logs after upgrading the Palette agent to version 4.7.9. Cluster functionality is not affected." [ref=e432]:
                        - text: Network overlay cluster nodes may display erroneous
                        - code [ref=e433]: failed to add static FDB entry after cleanup...Stdout already set, output
                        - text: logs after
                        - link "upgrading the Palette agent" [ref=e434] [cursor=pointer]:
                          - /url: /clusters/edge/cluster-management/agent-upgrade-airgap/
                        - text: to version 4.7.9. Cluster functionality is not affected.
                      - cell "No workaround available." [ref=e435]
                      - cell "August 17, 2025" [ref=e436]
                      - cell "Edge" [ref=e437]
                    - 'row "Container runtime may fail to run with the message Failed to run CRI service error=failed to recover state: failed to get metadata for stored sandbox after a node is upgraded to 1.29.14. This is related to an upstream issue with containerd. Remove the container runtime folder with rm -rf /var/lib/containerd. Then restart containerd and kubelet using systemctl restart containerd && systemctl restart kublet. August 17, 2025 Edge" [ref=e438]':
                      - 'cell "Container runtime may fail to run with the message Failed to run CRI service error=failed to recover state: failed to get metadata for stored sandbox after a node is upgraded to 1.29.14. This is related to an upstream issue with containerd." [ref=e439]':
                        - text: Container runtime may fail to run with the message
                        - code [ref=e440]: "Failed to run CRI service error=failed to recover state: failed to get metadata for stored sandbox"
                        - text: after a node is upgraded to 1.29.14. This is related to an
                        - link "upstream issue with containerd" [ref=e441] [cursor=pointer]:
                          - /url: https://github.com/containerd/containerd/issues/10848
                        - text: .
                      - cell "Remove the container runtime folder with rm -rf /var/lib/containerd. Then restart containerd and kubelet using systemctl restart containerd && systemctl restart kublet." [ref=e442]:
                        - text: Remove the container runtime folder with
                        - code [ref=e443]: rm -rf /var/lib/containerd
                        - text: . Then restart containerd and kubelet using
                        - code [ref=e444]: systemctl restart containerd && systemctl restart kublet
                        - text: .
                      - cell "August 17, 2025" [ref=e445]
                      - cell "Edge" [ref=e446]
                    - 'row "Due to an upstream issue with a Go library and CLIs for working with container registries, unintended or non-graceful reboots during content push operations to registries can cause consistency issues. This leads to content sync in locally managed clusters throwing the content-length: 0 error. Refer to Troubleshooting - Edge for the workaround. August 17, 2025 Edge" [ref=e447]':
                      - 'cell "Due to an upstream issue with a Go library and CLIs for working with container registries, unintended or non-graceful reboots during content push operations to registries can cause consistency issues. This leads to content sync in locally managed clusters throwing the content-length: 0 error." [ref=e448]':
                        - text: Due to
                        - link "an upstream issue with a Go library and CLIs for working with container registries" [ref=e449] [cursor=pointer]:
                          - /url: https://github.com/google/go-containerregistry/issues/2124
                        - text: ", unintended or non-graceful reboots during content push operations to registries can cause consistency issues. This leads to content sync in locally managed clusters throwing the"
                        - code [ref=e450]: "content-length: 0"
                        - text: error.
                      - cell "Refer to Troubleshooting - Edge for the workaround." [ref=e451]:
                        - text: Refer to
                        - link "Troubleshooting - Edge" [ref=e452] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---content-length-0-errors-during-content-synchronization
                        - text: for the workaround.
                      - cell "August 17, 2025" [ref=e453]
                      - cell "Edge" [ref=e454]
                    - row "Controller mode MAAS deployments using the Canonical Kubernetes pack automatically install the Cilium CNI. This happens because of a known issue with the Canonical Kubernetes Cluster API (CAPI) bootstrap provider and cannot be disabled. However, Palette still requires users to explicitly configure a CNI in the cluster profile. Select the Cilium CNI (Canonical Kubernetes) pack when creating a cluster profile to fulfill the CNI requirement. Palette recognizes this selection and allows cluster creation to proceed, even though Cilium is installed by the bootstrap process. August 17, 2025 Clusters, Packs" [ref=e455]:
                      - cell "Controller mode MAAS deployments using the Canonical Kubernetes pack automatically install the Cilium CNI. This happens because of a known issue with the Canonical Kubernetes Cluster API (CAPI) bootstrap provider and cannot be disabled. However, Palette still requires users to explicitly configure a CNI in the cluster profile." [ref=e456]:
                        - text: Controller mode MAAS deployments using the
                        - link "Canonical Kubernetes pack" [ref=e457] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=kubernetes-ck8s
                        - text: automatically install the Cilium CNI. This happens because of a known issue with the Canonical Kubernetes Cluster API (CAPI) bootstrap provider and cannot be disabled. However, Palette still requires users to explicitly configure a CNI in the cluster profile.
                      - cell "Select the Cilium CNI (Canonical Kubernetes) pack when creating a cluster profile to fulfill the CNI requirement. Palette recognizes this selection and allows cluster creation to proceed, even though Cilium is installed by the bootstrap process." [ref=e458]:
                        - text: Select the
                        - strong [ref=e459]: Cilium CNI (Canonical Kubernetes)
                        - text: pack when creating a cluster profile to fulfill the CNI requirement. Palette recognizes this selection and allows cluster creation to proceed, even though Cilium is installed by the bootstrap process.
                      - cell "August 17, 2025" [ref=e460]
                      - cell "Clusters, Packs" [ref=e461]
                    - row "If you configure static IP on a host using the Terminal User Interface (TUI), the cluster that is formed by the host cannot enable network overlay. Do not enable network overlay on clusters using static IPs configured via TUI. If you must use both static IP and network overlay, configure the static IP with the user data network block. July 31, 2025 Edge" [ref=e462]:
                      - cell "If you configure static IP on a host using the Terminal User Interface (TUI), the cluster that is formed by the host cannot enable network overlay." [ref=e463]:
                        - text: If you configure static IP on a host using the
                        - link "Terminal User Interface (TUI)" [ref=e464] [cursor=pointer]:
                          - /url: /clusters/edge/site-deployment/site-installation/initial-setup/
                        - text: ", the cluster that is formed by the host cannot"
                        - link "enable network overlay" [ref=e465] [cursor=pointer]:
                          - /url: /clusters/edge/networking/vxlan-overlay/
                        - text: .
                      - cell "Do not enable network overlay on clusters using static IPs configured via TUI. If you must use both static IP and network overlay, configure the static IP with the user data network block." [ref=e466]:
                        - text: Do not enable network overlay on clusters using static IPs configured via TUI. If you must use both static IP and network overlay, configure the static IP with the
                        - link "user data network block" [ref=e467] [cursor=pointer]:
                          - /url: /clusters/edge/edge-configuration/installer-reference/#site-network-parameters
                        - text: .
                      - cell "July 31, 2025" [ref=e468]
                      - cell "Edge" [ref=e469]
                    - row "When deploying an Edge RKE2 cluster on Rocky Linux, a worker node may fail to join the cluster if TCP port 9345 is not open on the control plane node. This port is required for communication between the RKE2 agent and the control plane. Verify if the port is open by running firewall-cmd --list-all on the control plane node. If 9345/tcp is not listed in the output, open it with firewall-cmd --zone=public --add-port=9345/tcp --permanent and apply the change using firewall-cmd --reload. July 21, 2025 Edge" [ref=e470]:
                      - cell "When deploying an Edge RKE2 cluster on Rocky Linux, a worker node may fail to join the cluster if TCP port 9345 is not open on the control plane node. This port is required for communication between the RKE2 agent and the control plane." [ref=e471]
                      - cell "Verify if the port is open by running firewall-cmd --list-all on the control plane node. If 9345/tcp is not listed in the output, open it with firewall-cmd --zone=public --add-port=9345/tcp --permanent and apply the change using firewall-cmd --reload." [ref=e472]:
                        - text: Verify if the port is open by running
                        - code [ref=e473]: firewall-cmd --list-all
                        - text: on the control plane node. If 9345/tcp is not listed in the output, open it with
                        - code [ref=e474]: firewall-cmd --zone=public --add-port=9345/tcp --permanent
                        - text: and apply the change using
                        - code [ref=e475]: firewall-cmd --reload
                        - text: .
                      - cell "July 21, 2025" [ref=e476]
                      - cell "Edge" [ref=e477]
                    - row "When using the Palette/VerteX Management Appliance, clicking on the Zot service link in Local UI results in a new tab displaying Client sent an HTTP request to an HTTPS server. Change the prefix of the URL in your web browser to https:// instead of http://. July 21, 2025 Clusters, Packs" [ref=e478]:
                      - cell "When using the Palette/VerteX Management Appliance, clicking on the Zot service link in Local UI results in a new tab displaying Client sent an HTTP request to an HTTPS server." [ref=e479]:
                        - text: When using the Palette/VerteX Management Appliance, clicking on the Zot service link in Local UI results in a new tab displaying
                        - code [ref=e480]: Client sent an HTTP request to an HTTPS server
                        - text: .
                      - cell "Change the prefix of the URL in your web browser to https:// instead of http://." [ref=e481]:
                        - text: Change the prefix of the URL in your web browser to
                        - code [ref=e482]: https://
                        - text: instead of
                        - code [ref=e483]: http://
                        - text: .
                      - cell "July 21, 2025" [ref=e484]
                      - cell "Clusters, Packs" [ref=e485]
                    - 'row "When deploying a workload cluster with packs using namespaceLabels, the associated Pods get stuck if the cluster is deployed via self-hosted Palette or Palette VerteX, or if the palette-agent ConfigMap specifies data.feature.workloads: disable. Force-apply privileged labels to the affected namespace. Refer to the Packs - Troubleshooting guide for additional information. July 19, 2025 Clusters" [ref=e486]':
                      - 'cell "When deploying a workload cluster with packs using namespaceLabels, the associated Pods get stuck if the cluster is deployed via self-hosted Palette or Palette VerteX, or if the palette-agent ConfigMap specifies data.feature.workloads: disable." [ref=e487]':
                        - text: When deploying a workload cluster with packs using
                        - code [ref=e488]: namespaceLabels
                        - text: ", the associated Pods get stuck if the cluster is deployed via"
                        - link "self-hosted Palette" [ref=e489] [cursor=pointer]:
                          - /url: /enterprise-version/
                        - text: or
                        - link "Palette VerteX" [ref=e490] [cursor=pointer]:
                          - /url: /vertex/
                        - text: ", or if the"
                        - code [ref=e491]: palette-agent
                        - text: ConfigMap specifies
                        - code [ref=e492]: "data.feature.workloads: disable"
                        - text: .
                      - cell "Force-apply privileged labels to the affected namespace. Refer to the Packs - Troubleshooting guide for additional information." [ref=e493]:
                        - text: Force-apply
                        - code [ref=e494]: privileged
                        - text: labels to the affected namespace. Refer to the
                        - link "Packs - Troubleshooting" [ref=e495] [cursor=pointer]:
                          - /url: /troubleshooting/pack-issues/#scenario---pods-with-namespacelabels-are-stuck-on-deployment
                        - text: guide for additional information.
                      - cell "July 19, 2025" [ref=e496]
                      - cell "Clusters" [ref=e497]
                    - row "Day-2 node pool operations cannot be performed on AWS EKS clusters previously deployed with both Enable Nodepool Customization enabled and Amazon Linux 2023 (AL2023) node labels after upgrading to version 4.7.3. Create a new node pool with the desired Amazon Machine Image (AMI) and node pool customizations and migrate existing workloads to the new node pool. For an example of how to migrate workloads, refer to the AWS Scale, Upgrade, and Secure Clusters guide. July 19, 2025 Clusters" [ref=e498]:
                      - cell "Day-2 node pool operations cannot be performed on AWS EKS clusters previously deployed with both Enable Nodepool Customization enabled and Amazon Linux 2023 (AL2023) node labels after upgrading to version 4.7.3." [ref=e499]:
                        - text: Day-2
                        - link "node pool" [ref=e500] [cursor=pointer]:
                          - /url: /clusters/cluster-management/node-pool/
                        - text: operations cannot be performed on
                        - link "AWS EKS clusters" [ref=e501] [cursor=pointer]:
                          - /url: /clusters/public-cloud/aws/eks/
                        - text: previously deployed with both
                        - strong [ref=e502]: Enable Nodepool Customization
                        - text: enabled and Amazon Linux 2023 (AL2023)
                        - link "node labels" [ref=e503] [cursor=pointer]:
                          - /url: /clusters/cluster-management/node-labels/
                        - text: after upgrading to version 4.7.3.
                      - cell "Create a new node pool with the desired Amazon Machine Image (AMI) and node pool customizations and migrate existing workloads to the new node pool. For an example of how to migrate workloads, refer to the AWS Scale, Upgrade, and Secure Clusters guide." [ref=e504]:
                        - text: Create a new node pool with the desired
                        - link "Amazon Machine Image (AMI) and node pool customizations" [ref=e505] [cursor=pointer]:
                          - /url: /clusters/public-cloud/aws/eks/#cloud-configuration-settings
                        - text: and migrate existing workloads to the new node pool. For an example of how to migrate workloads, refer to the
                        - link "AWS Scale, Upgrade, and Secure Clusters" [ref=e506] [cursor=pointer]:
                          - /url: /tutorials/getting-started/palette/aws/scale-secure-cluster/#scale-a-cluster
                        - text: guide.
                      - cell "July 19, 2025" [ref=e507]
                      - cell "Clusters" [ref=e508]
                    - row "Cloning a virtual machine using KubeVirt 1.5 or later may hang if volume snapshots are not configured. Ensure that you configure a VolumeSnapshotClass in the charts.virtual-machine-orchestrator.snapshot-controller.volumeSnapshotClass resource in the Virtual Machine Orchestrator pack. July 19, 2025 Virtual Machine Orchestrator" [ref=e509]:
                      - cell "Cloning a virtual machine using KubeVirt 1.5 or later may hang if volume snapshots are not configured." [ref=e510]:
                        - link "Cloning a virtual machine" [ref=e511] [cursor=pointer]:
                          - /url: /vm-management/create-manage-vm/clone-vm/
                        - text: using KubeVirt 1.5 or later may hang if
                        - link "volume snapshots" [ref=e512] [cursor=pointer]:
                          - /url: /vm-management/create-manage-vm/take-snapshot-of-vm/
                        - text: are not configured.
                      - cell "Ensure that you configure a VolumeSnapshotClass in the charts.virtual-machine-orchestrator.snapshot-controller.volumeSnapshotClass resource in the Virtual Machine Orchestrator pack." [ref=e513]:
                        - text: Ensure that you configure a
                        - code [ref=e514]: VolumeSnapshotClass
                        - text: in the
                        - code [ref=e515]: charts.virtual-machine-orchestrator.snapshot-controller.volumeSnapshotClass
                        - text: resource in the
                        - link "Virtual Machine Orchestrator" [ref=e516] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=virtual-machine-orchestrator
                        - text: pack.
                      - cell "July 19, 2025" [ref=e517]
                      - cell "Virtual Machine Orchestrator" [ref=e518]
                    - row "Edge K3s clusters may fail kube-bench tests even when they are expected to pass. These failures do not indicate security issues, but rather stem from how the tests are executed. No workaround available. July 11, 2025 Edge" [ref=e519]:
                      - cell "Edge K3s clusters may fail kube-bench tests even when they are expected to pass. These failures do not indicate security issues, but rather stem from how the tests are executed." [ref=e520]:
                        - text: Edge K3s clusters may fail
                        - code [ref=e521]: kube-bench
                        - text: tests even when they are expected to pass. These failures do not indicate security issues, but rather stem from how the tests are executed.
                      - cell "No workaround available." [ref=e522]
                      - cell "July 11, 2025" [ref=e523]
                      - cell "Edge" [ref=e524]
                    - row "Palette eXtended Kubernetes - Edge (PXK-E) clusters running Kubernetes v1.32.x or later on RHEL or Rocky Linux 8.x may experience failure during Kubernetes initialization due to unsupported kernel version. Use RHEL or Rocky Linux 9.x as the base OS or update the kernel version to 4.19 or later in the 4.x series, or to any 5.x or 6.x version. Refer to Troubleshooting - Edge for debug steps. June 23, 2025 Edge" [ref=e525]:
                      - cell "Palette eXtended Kubernetes - Edge (PXK-E) clusters running Kubernetes v1.32.x or later on RHEL or Rocky Linux 8.x may experience failure during Kubernetes initialization due to unsupported kernel version." [ref=e526]:
                        - link "Palette eXtended Kubernetes - Edge (PXK-E)" [ref=e527] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=edge-k8s
                        - text: clusters running Kubernetes v1.32.x or later on RHEL or Rocky Linux 8.x may experience failure during Kubernetes initialization due to unsupported kernel version.
                      - cell "Use RHEL or Rocky Linux 9.x as the base OS or update the kernel version to 4.19 or later in the 4.x series, or to any 5.x or 6.x version. Refer to Troubleshooting - Edge for debug steps." [ref=e528]:
                        - text: Use RHEL or Rocky Linux 9.x as the base OS or update the kernel version to 4.19 or later in the 4.x series, or to any 5.x or 6.x version. Refer to
                        - link "Troubleshooting - Edge" [ref=e529] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario--pxk-e-clusters-on-rhel-and-rocky-8-fail-kubernetes-initialization
                        - text: for debug steps.
                      - cell "June 23, 2025" [ref=e530]
                      - cell "Edge" [ref=e531]
                    - row "Calico fails to start when IPv6 is enabled on hosts running specific kernel versions due to missing or incompatible kernel modules required for ip6tables MARK support. Affected kernel versions include 5.15.0-127 and 5.15.0-128 (generic), 6.8.0-57 and 6.8.0-58 (generic), and 6.8.0-1022 (cloud). Use a different CNI, disable IPv6, or use an unaffected kernel version. Refer to the troubleshooting guide for debug steps. June 23, 2025 Packs" [ref=e532]:
                      - cell "Calico fails to start when IPv6 is enabled on hosts running specific kernel versions due to missing or incompatible kernel modules required for ip6tables MARK support. Affected kernel versions include 5.15.0-127 and 5.15.0-128 (generic), 6.8.0-57 and 6.8.0-58 (generic), and 6.8.0-1022 (cloud)." [ref=e533]:
                        - link "Calico" [ref=e534] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=cni-calico
                        - text: fails to start when IPv6 is enabled on hosts running specific kernel versions due to missing or incompatible kernel modules required for
                        - code [ref=e535]: ip6tables
                        - code [ref=e536]: MARK
                        - text: support. Affected kernel versions include 5.15.0-127 and 5.15.0-128 (generic), 6.8.0-57 and 6.8.0-58 (generic), and 6.8.0-1022 (cloud).
                      - cell "Use a different CNI, disable IPv6, or use an unaffected kernel version. Refer to the troubleshooting guide for debug steps." [ref=e537]:
                        - text: Use a different CNI, disable IPv6, or use an unaffected kernel version. Refer to the
                        - link "troubleshooting" [ref=e538] [cursor=pointer]:
                          - /url: /troubleshooting/pack-issues/#scenario---calico-fails-to-start-when-ipv6-is-enabled
                        - text: guide for debug steps.
                      - cell "June 23, 2025" [ref=e539]
                      - cell "Packs" [ref=e540]
                    - row "Palette eXtended Kubernetes - Edge (PXK-E) control plane nodes in VerteX clusters may experience failure of the kube-vip component after reboot. Refer to Troubleshooting - Edge for debug steps. June 23, 2025 Edge" [ref=e541]:
                      - cell "Palette eXtended Kubernetes - Edge (PXK-E) control plane nodes in VerteX clusters may experience failure of the kube-vip component after reboot." [ref=e542]:
                        - link "Palette eXtended Kubernetes - Edge (PXK-E)" [ref=e543] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=edge-k8s
                        - text: control plane nodes in VerteX clusters may experience failure of the
                        - code [ref=e544]: kube-vip
                        - text: component after reboot.
                      - cell "Refer to Troubleshooting - Edge for debug steps." [ref=e545]:
                        - text: Refer to
                        - link "Troubleshooting - Edge" [ref=e546] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---pxk-e-clusters-in-vertex-deployments-experience-failure-upon-reboot
                        - text: for debug steps.
                      - cell "June 23, 2025" [ref=e547]
                      - cell "Edge" [ref=e548]
                    - row "The Pause Agent Upgrades configuration is not applied to Edge hosts that are not part of a cluster. Edge hosts that are part of a cluster are not affected. No workaround. June 23, 2025 Edge" [ref=e549]:
                      - cell "The Pause Agent Upgrades configuration is not applied to Edge hosts that are not part of a cluster. Edge hosts that are part of a cluster are not affected." [ref=e550]:
                        - text: The
                        - link "Pause Agent Upgrades" [ref=e551] [cursor=pointer]:
                          - /url: /clusters/cluster-management/platform-settings/pause-platform-upgrades/
                        - text: configuration is not applied to Edge hosts that are not part of a cluster. Edge hosts that are part of a cluster are not affected.
                      - cell "No workaround." [ref=e552]
                      - cell "June 23, 2025" [ref=e553]
                      - cell "Edge" [ref=e554]
                    - row "Due to CAPZ upgrades in version 4.6.32, Azure IaaS and AKS clusters cannot be deployed on both Azure Public Cloud and Azure US Government. Clusters will get stuck during the provisioning stage. Users who want to deploy a cluster on both Azure environments must use a PCG when adding an Azure US Government cloud account. June 11, 2025 Clusters" [ref=e555]:
                      - cell "Due to CAPZ upgrades in version 4.6.32, Azure IaaS and AKS clusters cannot be deployed on both Azure Public Cloud and Azure US Government. Clusters will get stuck during the provisioning stage." [ref=e556]:
                        - text: Due to CAPZ upgrades in version 4.6.32,
                        - link "Azure IaaS" [ref=e557] [cursor=pointer]:
                          - /url: /clusters/public-cloud/azure/
                        - text: and
                        - link "AKS" [ref=e558] [cursor=pointer]:
                          - /url: /clusters/public-cloud/azure/aks/
                        - text: clusters cannot be deployed on both
                        - link "Azure Public Cloud" [ref=e559] [cursor=pointer]:
                          - /url: /clusters/public-cloud/azure/
                        - text: and
                        - link "Azure US Government" [ref=e560] [cursor=pointer]:
                          - /url: https://azure.microsoft.com/en-us/explore/global-infrastructure/government
                        - text: . Clusters will get stuck during the provisioning stage.
                      - cell "Users who want to deploy a cluster on both Azure environments must use a PCG when adding an Azure US Government cloud account." [ref=e561]:
                        - text: Users who want to deploy a cluster on both Azure environments must use a
                        - link "PCG" [ref=e562] [cursor=pointer]:
                          - /url: /clusters/pcg/
                        - text: when adding an
                        - link "Azure US Government cloud account" [ref=e563] [cursor=pointer]:
                          - /url: /clusters/public-cloud/azure/azure-cloud/
                        - text: .
                      - cell "June 11, 2025" [ref=e564]
                      - cell "Clusters" [ref=e565]
                    - row "Palette eXtended Kubernetes (PXK) and Palette eXtended Kubernetes - Edge (PXK-E) versions 1.30.10, 1.31.6, and 1.32.2 or older do not support TLS 1.3 or applications that require TLS 1.3 encrypted communications. Use PXK and PXK-E versions 1.30.11, 1.31.7, and 1.32.3 or later instead. June 5, 2025 Edge" [ref=e566]:
                      - cell "Palette eXtended Kubernetes (PXK) and Palette eXtended Kubernetes - Edge (PXK-E) versions 1.30.10, 1.31.6, and 1.32.2 or older do not support TLS 1.3 or applications that require TLS 1.3 encrypted communications." [ref=e567]
                      - cell "Use PXK and PXK-E versions 1.30.11, 1.31.7, and 1.32.3 or later instead." [ref=e568]
                      - cell "June 5, 2025" [ref=e569]
                      - cell "Edge" [ref=e570]
                    - row "Clusters with Pause Agent Upgrades enabled may be stuck in the Deleting state. Cluster resources will not be deleted without manual intervention. Disable the Pause Agent Upgrades setting and trigger the cluster deletion. May 31, 2025 Clusters" [ref=e571]:
                      - cell "Clusters with Pause Agent Upgrades enabled may be stuck in the Deleting state. Cluster resources will not be deleted without manual intervention." [ref=e572]:
                        - text: Clusters with
                        - link "Pause Agent Upgrades" [ref=e573] [cursor=pointer]:
                          - /url: /clusters/cluster-management/platform-settings/pause-platform-upgrades/
                        - text: enabled may be stuck in the
                        - strong [ref=e574]: Deleting
                        - text: state. Cluster resources will not be deleted without manual intervention.
                      - cell "Disable the Pause Agent Upgrades setting and trigger the cluster deletion." [ref=e575]:
                        - text: Disable the
                        - strong [ref=e576]: Pause Agent Upgrades
                        - text: setting and trigger the cluster deletion.
                      - cell "May 31, 2025" [ref=e577]
                      - cell "Clusters" [ref=e578]
                    - row "When upgrading airgapped self-hosted Palette and VerteX clusters to 4.6.32, the IPAM controller may report an Exhausted IP Pools error despite having available IP addresses, preventing the cluster from upgrading. This is due to a race condition in CAPV version 1.12.0, which may lead to an orphaned IP claim. Delete the orphaned IP claim and re-run the upgrade. Refer to the troubleshooting guide for debug steps. May 31, 2025 Clusters" [ref=e579]:
                      - cell "When upgrading airgapped self-hosted Palette and VerteX clusters to 4.6.32, the IPAM controller may report an Exhausted IP Pools error despite having available IP addresses, preventing the cluster from upgrading. This is due to a race condition in CAPV version 1.12.0, which may lead to an orphaned IP claim." [ref=e580]:
                        - text: When upgrading airgapped self-hosted Palette and VerteX clusters to 4.6.32, the IPAM controller may report an
                        - code [ref=e581]: Exhausted IP Pools
                        - text: error despite having available IP addresses, preventing the cluster from upgrading. This is due to a race condition in CAPV version 1.12.0, which may lead to an orphaned IP claim.
                      - cell "Delete the orphaned IP claim and re-run the upgrade. Refer to the troubleshooting guide for debug steps." [ref=e582]:
                        - text: Delete the orphaned IP claim and re-run the upgrade. Refer to the
                        - link "troubleshooting" [ref=e583] [cursor=pointer]:
                          - /url: /troubleshooting/enterprise-install/#scenario---ip-pool-exhausted-during-airgapped-upgrade
                        - text: guide for debug steps.
                      - cell "May 31, 2025" [ref=e584]
                      - cell "Clusters" [ref=e585]
                    - row "Edge clusters using K3s version 1.32.1 or 1.32.2 may fail to provision due to an upstream issue. Refer to the K3s issue page for more information. No workaround available. May 31, 2025 Edge" [ref=e586]:
                      - cell "Edge clusters using K3s version 1.32.1 or 1.32.2 may fail to provision due to an upstream issue. Refer to the K3s issue page for more information." [ref=e587]:
                        - text: Edge clusters using K3s version 1.32.1 or 1.32.2 may fail to provision due to an upstream issue. Refer to the
                        - link "K3s issue page" [ref=e588] [cursor=pointer]:
                          - /url: https://github.com/k3s-io/k3s/issues/11973
                        - text: for more information.
                      - cell "No workaround available." [ref=e589]
                      - cell "May 31, 2025" [ref=e590]
                      - cell "Edge" [ref=e591]
                    - row "For clusters deployed with Palette eXtended Kubernetes - Edge (PXK-E) and agent mode using the FIPS installation package, adding a custom stylus.path to the user-data file causes cluster creation to fail as it cannot find kubelet. No workaround available. May 31, 2025 Edge" [ref=e592]:
                      - cell "For clusters deployed with Palette eXtended Kubernetes - Edge (PXK-E) and agent mode using the FIPS installation package, adding a custom stylus.path to the user-data file causes cluster creation to fail as it cannot find kubelet." [ref=e593]:
                        - text: For clusters deployed with
                        - link "Palette eXtended Kubernetes - Edge (PXK-E)" [ref=e594] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=edge-k8s
                        - text: and
                        - link "agent mode" [ref=e595] [cursor=pointer]:
                          - /url: /deployment-modes/agent-mode/
                        - text: using the FIPS installation package, adding a custom
                        - code [ref=e596]: stylus.path
                        - text: to the
                        - code [ref=e597]: user-data
                        - text: file causes cluster creation to fail as it cannot find
                        - link "kubelet" [ref=e598] [cursor=pointer]:
                          - /url: https://kubernetes.io/docs/concepts/architecture/#kubelet
                        - text: .
                      - cell "No workaround available." [ref=e599]
                      - cell "May 31, 2025" [ref=e600]
                      - cell "Edge" [ref=e601]
                    - row "During a Kubernetes upgrade, the Cilium pod may get stuck in the Init:CrashLoopBackoff state due to nsenter permission issues. Refer to Troubleshooting - Edge for debug steps. May 31, 2025 Edge" [ref=e602]:
                      - cell "During a Kubernetes upgrade, the Cilium pod may get stuck in the Init:CrashLoopBackoff state due to nsenter permission issues." [ref=e603]:
                        - text: During a Kubernetes upgrade, the Cilium pod may get stuck in the
                        - code [ref=e604]: Init:CrashLoopBackoff
                        - text: state due to nsenter permission issues.
                      - cell "Refer to Troubleshooting - Edge for debug steps." [ref=e605]:
                        - text: Refer to
                        - link "Troubleshooting - Edge" [ref=e606] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---cilium-pod-stuck-during-kubernetes-upgrade-due-to-nsenter-permission-issue
                        - text: for debug steps.
                      - cell "May 31, 2025" [ref=e607]
                      - cell "Edge" [ref=e608]
                    - row "Pods with emptyDir volumes that are backed up using Velero 1.9, restored using Velero 1.15, and backed up and restored again with Velero 1.15 are stuck in the init state when performing a second restore. This is caused by a known upstream issue with Velero. Delete stuck pods or restart affected deployments. May 31, 2025 Clusters" [ref=e609]:
                      - cell "Pods with emptyDir volumes that are backed up using Velero 1.9, restored using Velero 1.15, and backed up and restored again with Velero 1.15 are stuck in the init state when performing a second restore. This is caused by a known upstream issue with Velero." [ref=e610]:
                        - text: Pods with
                        - link "emptyDir" [ref=e611] [cursor=pointer]:
                          - /url: https://kubernetes.io/docs/concepts/storage/volumes/#emptydir
                        - text: volumes that are
                        - link "backed up" [ref=e612] [cursor=pointer]:
                          - /url: /clusters/cluster-management/backup-restore/create-cluster-backup/
                        - text: using Velero 1.9,
                        - link "restored" [ref=e613] [cursor=pointer]:
                          - /url: /clusters/cluster-management/backup-restore/restore-cluster-backup/
                        - text: using Velero 1.15, and backed up and restored again with Velero 1.15 are stuck in the
                        - code [ref=e614]: init
                        - text: state when performing a second restore. This is caused by a known
                        - link "upstream issue" [ref=e615] [cursor=pointer]:
                          - /url: https://github.com/vmware-tanzu/velero/pull/8880
                        - text: with Velero.
                      - cell "Delete stuck pods or restart affected deployments." [ref=e616]
                      - cell "May 31, 2025" [ref=e617]
                      - cell "Clusters" [ref=e618]
                    - row "Appliance Studio does not validate the value of each field in .arg or user-data files. No workaround available. May 31, 2025 Edge" [ref=e619]:
                      - cell "Appliance Studio does not validate the value of each field in .arg or user-data files." [ref=e620]:
                        - link "Appliance Studio" [ref=e621] [cursor=pointer]:
                          - /url: /deployment-modes/appliance-mode/appliance-studio/
                        - text: does not validate the value of each field in
                        - code [ref=e622]: .arg
                        - text: or
                        - code [ref=e623]: user-data
                        - text: files.
                      - cell "No workaround available." [ref=e624]
                      - cell "May 31, 2025" [ref=e625]
                      - cell "Edge" [ref=e626]
                    - row "Palette virtual clusters provisioned with EKS clusters as host clusters in the cluster group and using the Calico CNI are stuck in the Provisioning state due to Cert Manager not being reachable. This stems from an upstream limitation between Cert Manager on EKS and custom CNIs. No workaround available. May 21, 2025 Edge" [ref=e627]:
                      - cell "Palette virtual clusters provisioned with EKS clusters as host clusters in the cluster group and using the Calico CNI are stuck in the Provisioning state due to Cert Manager not being reachable. This stems from an upstream limitation between Cert Manager on EKS and custom CNIs." [ref=e628]:
                        - text: Palette virtual clusters provisioned with EKS clusters as host clusters in the cluster group and using the Calico CNI are stuck in the
                        - strong [ref=e629]: Provisioning
                        - text: state due to Cert Manager not being reachable. This stems from
                        - link "an upstream limitation" [ref=e630] [cursor=pointer]:
                          - /url: https://cert-manager.io/docs/installation/compatibility/#aws-eks
                        - text: between Cert Manager on EKS and custom CNIs.
                      - cell "No workaround available." [ref=e631]
                      - cell "May 21, 2025" [ref=e632]
                      - cell "Edge" [ref=e633]
                    - row "Remote shell sessions executing in the Chrome and Microsoft Edge browsers time out after approximately five minutes of inactivity. Start remote shell sessions in the Firefox browser instead. Firefox supports a 12 hour inactivity timeout. May 5, 2025 Edge" [ref=e634]:
                      - cell "Remote shell sessions executing in the Chrome and Microsoft Edge browsers time out after approximately five minutes of inactivity." [ref=e635]:
                        - link "Remote shell" [ref=e636] [cursor=pointer]:
                          - /url: /clusters/edge/cluster-management/remote-shell/
                        - text: sessions executing in the
                        - link "Chrome" [ref=e637] [cursor=pointer]:
                          - /url: https://www.google.com/intl/en_uk/chrome/
                        - text: and
                        - link "Microsoft Edge" [ref=e638] [cursor=pointer]:
                          - /url: https://www.microsoft.com/en-gb/edge/download?form=MA13FJ
                        - text: browsers time out after approximately five minutes of inactivity.
                      - cell "Start remote shell sessions in the Firefox browser instead. Firefox supports a 12 hour inactivity timeout." [ref=e639]:
                        - text: Start
                        - link "remote shell" [ref=e640] [cursor=pointer]:
                          - /url: /clusters/edge/cluster-management/remote-shell/
                        - text: sessions in the
                        - link "Firefox" [ref=e641] [cursor=pointer]:
                          - /url: https://www.mozilla.org/en-GB/firefox/new/
                        - text: browser instead. Firefox supports a 12 hour inactivity timeout.
                      - cell "May 5, 2025" [ref=e642]
                      - cell "Edge" [ref=e643]
                    - row "When upgrading an airgapped Edge cluster to version 4.6.24, some pods may get stuck in the ImagePullBackOff state. Re-upload the content bundle. May 5, 2025 Edge" [ref=e644]:
                      - cell "When upgrading an airgapped Edge cluster to version 4.6.24, some pods may get stuck in the ImagePullBackOff state." [ref=e645]:
                        - text: When upgrading an airgapped Edge cluster to version 4.6.24, some pods may get stuck in the
                        - code [ref=e646]: ImagePullBackOff
                        - text: state.
                      - cell "Re-upload the content bundle." [ref=e647]
                      - cell "May 5, 2025" [ref=e648]
                      - cell "Edge" [ref=e649]
                    - row "When you enable remote shell on an Edge host, the remote shell configuration may become stuck in the Configuring state. Disable remote shell in the UI, and wait for one minute before enabling it again. April 19, 2025 Edge" [ref=e650]:
                      - cell "When you enable remote shell on an Edge host, the remote shell configuration may become stuck in the Configuring state." [ref=e651]:
                        - text: When you
                        - link "enable remote shell" [ref=e652] [cursor=pointer]:
                          - /url: /clusters/edge/cluster-management/remote-shell/
                        - text: on an Edge host, the remote shell configuration may become stuck in the
                        - strong [ref=e653]: Configuring
                        - text: state.
                      - cell "Disable remote shell in the UI, and wait for one minute before enabling it again." [ref=e654]
                      - cell "April 19, 2025" [ref=e655]
                      - cell "Edge" [ref=e656]
                    - row "Disconnected Edge clusters using PXK-E version 1.29.14 or 1.30.10 will sometimes go into the unknown state after a reboot. Use the command kubectl delete pod kube-vip-<node-name> --namespace kube-system to delete the Kubernetes VIP pod and let it be re-created automatically. Replace node-name with the name of the host node. March 15, 2025 Edge" [ref=e657]:
                      - cell "Disconnected Edge clusters using PXK-E version 1.29.14 or 1.30.10 will sometimes go into the unknown state after a reboot." [ref=e658]
                      - cell "Use the command kubectl delete pod kube-vip-<node-name> --namespace kube-system to delete the Kubernetes VIP pod and let it be re-created automatically. Replace node-name with the name of the host node." [ref=e659]:
                        - text: Use the command
                        - code [ref=e660]: kubectl delete pod kube-vip-<node-name> --namespace kube-system
                        - text: to delete the Kubernetes VIP pod and let it be re-created automatically. Replace
                        - code [ref=e661]: node-name
                        - text: with the name of the host node.
                      - cell "March 15, 2025" [ref=e662]
                      - cell "Edge" [ref=e663]
                    - row "MAAS and VMware vSphere clusters fail to provision on existing self-hosted Palette and VerteX environments deployed with Palette 4.2.13 or later. These installations have an incorrectly configured default image endpoint, which causes image resolution to fail. New self-hosted installations are not affected. Refer to Troubleshooting for a workaround for non-airgap environments. For airgap environments, ensure that the images are downloaded to your environment. Refer to the Additional OVAs page for further details. February 16, 2025 Self-Hosted, Clusters" [ref=e664]:
                      - cell "MAAS and VMware vSphere clusters fail to provision on existing self-hosted Palette and VerteX environments deployed with Palette 4.2.13 or later. These installations have an incorrectly configured default image endpoint, which causes image resolution to fail. New self-hosted installations are not affected." [ref=e665]:
                        - link "MAAS" [ref=e666] [cursor=pointer]:
                          - /url: /clusters/data-center/maas/
                        - text: and
                        - link "VMware vSphere" [ref=e667] [cursor=pointer]:
                          - /url: /clusters/data-center/vmware/
                        - text: clusters fail to provision on existing self-hosted Palette and VerteX environments deployed with Palette 4.2.13 or later. These installations have an incorrectly configured default image endpoint, which causes image resolution to fail. New self-hosted installations are not affected.
                      - cell "Refer to Troubleshooting for a workaround for non-airgap environments. For airgap environments, ensure that the images are downloaded to your environment. Refer to the Additional OVAs page for further details." [ref=e668]:
                        - text: Refer to
                        - link "Troubleshooting" [ref=e669] [cursor=pointer]:
                          - /url: /troubleshooting/enterprise-install/#scenario---maas-and-vmware-vsphere-clusters-fail-image-resolution-in-non-airgap-environments
                        - text: for a workaround for non-airgap environments. For airgap environments, ensure that the images are downloaded to your environment. Refer to the
                        - link "Additional OVAs" [ref=e670] [cursor=pointer]:
                          - /url: /downloads/self-hosted-palette/additional-ovas/
                        - text: page for further details.
                      - cell "February 16, 2025" [ref=e671]
                      - cell "Self-Hosted, Clusters" [ref=e672]
                    - row "Performing a MicroK8s InPlaceUpgrade from version 1.28 to 1.29 on active MAAS and AWS clusters with Cilium prevents new pods from being deployed on control plane nodes due to an upstream issue with Canonical. This issue also occurs when performing a MicroK8s SmartUpgrade from version 1.28 to 1.29 on active MAAS and AWS clusters with one control plane node and Cilium. Manually restart the Cilium pods on each control plane node using the command microk8s kubectl rollout restart daemonset cilium --namespace kube-system. February 16, 2025 Clusters, Packs" [ref=e673]:
                      - cell "Performing a MicroK8s InPlaceUpgrade from version 1.28 to 1.29 on active MAAS and AWS clusters with Cilium prevents new pods from being deployed on control plane nodes due to an upstream issue with Canonical. This issue also occurs when performing a MicroK8s SmartUpgrade from version 1.28 to 1.29 on active MAAS and AWS clusters with one control plane node and Cilium." [ref=e674]:
                        - text: Performing a
                        - link "MicroK8s" [ref=e675] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=kubernetes-microk8s
                        - code [ref=e676]: InPlaceUpgrade
                        - text: from version 1.28 to 1.29 on active MAAS and AWS clusters with Cilium prevents new pods from being deployed on control plane nodes due to an
                        - link "upstream issue" [ref=e677] [cursor=pointer]:
                          - /url: https://github.com/canonical/cluster-api-control-plane-provider-microk8s/issues/74
                        - text: with Canonical. This issue also occurs when performing a MicroK8s
                        - code [ref=e678]: SmartUpgrade
                        - text: from version 1.28 to 1.29 on active MAAS and AWS clusters with one control plane node and Cilium.
                      - cell "Manually restart the Cilium pods on each control plane node using the command microk8s kubectl rollout restart daemonset cilium --namespace kube-system." [ref=e679]:
                        - text: Manually restart the Cilium pods on
                        - emphasis [ref=e680]: each
                        - text: control plane node using the command
                        - code [ref=e681]: microk8s kubectl rollout restart daemonset cilium --namespace kube-system
                        - text: .
                      - cell "February 16, 2025" [ref=e682]
                      - cell "Clusters, Packs" [ref=e683]
                    - row "For clusters deployed with Virtual Machine Orchestrator (VMO), namespaces on the Virtual Machine tab cannot be viewed by users with any spectro-vm cluster role. Add the spectro-namespace-list cluster role to users who need to view virtual machines and virtual machine namespaces. Refer to the Add Roles and Role Bindings guide for instructions on how to add roles for VMO clusters. February 5, 2025 Virtual Machine Orchestrator" [ref=e684]:
                      - cell "For clusters deployed with Virtual Machine Orchestrator (VMO), namespaces on the Virtual Machine tab cannot be viewed by users with any spectro-vm cluster role." [ref=e685]:
                        - text: For clusters deployed with
                        - link "Virtual Machine Orchestrator (VMO)" [ref=e686] [cursor=pointer]:
                          - /url: /vm-management/
                        - text: ", namespaces on the"
                        - strong [ref=e687]: Virtual Machine
                        - text: tab cannot be viewed by users with any
                        - code [ref=e688]: spectro-vm
                        - text: cluster role.
                      - cell "Add the spectro-namespace-list cluster role to users who need to view virtual machines and virtual machine namespaces. Refer to the Add Roles and Role Bindings guide for instructions on how to add roles for VMO clusters." [ref=e689]:
                        - text: Add the
                        - code [ref=e690]: spectro-namespace-list
                        - text: cluster role to users who need to view virtual machines and virtual machine namespaces. Refer to the
                        - link "Add Roles and Role Bindings" [ref=e691] [cursor=pointer]:
                          - /url: /vm-management/rbac/add-roles-and-role-bindings/
                        - text: guide for instructions on how to add roles for VMO clusters.
                      - cell "February 5, 2025" [ref=e692]
                      - cell "Virtual Machine Orchestrator" [ref=e693]
                    - row "For clusters deployed with Palette eXtended Kubernetes - Edge (PXK-E) and agent mode, the contents of the /opt/cni/bin folder are not set correctly, causing cluster deployment issues because the cluster network cannot come up. Refer to Troubleshooting for a workaround. January 30, 2025 Palette agent" [ref=e694]:
                      - cell "For clusters deployed with Palette eXtended Kubernetes - Edge (PXK-E) and agent mode, the contents of the /opt/cni/bin folder are not set correctly, causing cluster deployment issues because the cluster network cannot come up." [ref=e695]:
                        - text: For clusters deployed with
                        - link "Palette eXtended Kubernetes - Edge (PXK-E)" [ref=e696] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=edge-k8s
                        - text: and
                        - link "agent mode" [ref=e697] [cursor=pointer]:
                          - /url: /deployment-modes/agent-mode/
                        - text: ", the contents of the"
                        - code [ref=e698]: /opt/cni/bin
                        - text: folder are not set correctly, causing cluster deployment issues because the cluster network cannot come up.
                      - cell "Refer to Troubleshooting for a workaround." [ref=e699]:
                        - text: Refer to
                        - link "Troubleshooting" [ref=e700] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---agent-mode-deployments-cni-folder-permission-issues
                        - text: for a workaround.
                      - cell "January 30, 2025" [ref=e701]
                      - cell "Palette agent" [ref=e702]
                    - row "Palette workload clusters deployed with Calico version 3.28.2, 3.29.0, or 3.29.1 are experiencing memory leaks due to an upstream issue with Calico, which is caused by failing to close netlink handles. Create a new profile version using Calico version 3.28.0 or 3.28.1 and update your cluster. January 27, 2025 Clusters, Packs" [ref=e703]:
                      - cell "Palette workload clusters deployed with Calico version 3.28.2, 3.29.0, or 3.29.1 are experiencing memory leaks due to an upstream issue with Calico, which is caused by failing to close netlink handles." [ref=e704]:
                        - text: Palette
                        - link "workload clusters" [ref=e705] [cursor=pointer]:
                          - /url: /glossary-all/#workload-cluster
                        - text: deployed with Calico version 3.28.2, 3.29.0, or 3.29.1 are experiencing memory leaks due to an
                        - link "upstream issue" [ref=e706] [cursor=pointer]:
                          - /url: https://github.com/projectcalico/calico/pull/9612
                        - text: with Calico, which is caused by failing to close netlink handles.
                      - cell "Create a new profile version using Calico version 3.28.0 or 3.28.1 and update your cluster." [ref=e707]:
                        - link "Create a new profile version" [ref=e708] [cursor=pointer]:
                          - /url: /profiles/cluster-profiles/modify-cluster-profiles/version-cluster-profile/
                        - text: using Calico version 3.28.0 or 3.28.1 and
                        - link "update your cluster" [ref=e709] [cursor=pointer]:
                          - /url: /clusters/cluster-management/cluster-updates/#update-a-cluster
                        - text: .
                      - cell "January 27, 2025" [ref=e710]
                      - cell "Clusters, Packs" [ref=e711]
                    - row "For clusters deployed with Palette eXtended Kubernetes - Edge (PXK-E) and agent mode on Palette agent version 4.5.14, adding a custom stylus.path to the user-data file causes cluster creation to fail as it cannot find kubeadm. Review the Edge Troubleshooting section for workarounds. Refer to Identify the Target Agent Version for guidance in retrieving your Palette agent version number. January 19, 2025 Edge" [ref=e712]:
                      - cell "For clusters deployed with Palette eXtended Kubernetes - Edge (PXK-E) and agent mode on Palette agent version 4.5.14, adding a custom stylus.path to the user-data file causes cluster creation to fail as it cannot find kubeadm." [ref=e713]:
                        - text: For clusters deployed with
                        - link "Palette eXtended Kubernetes - Edge (PXK-E)" [ref=e714] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=edge-k8s
                        - text: and
                        - link "agent mode" [ref=e715] [cursor=pointer]:
                          - /url: /deployment-modes/agent-mode/
                        - text: on Palette agent version 4.5.14, adding a custom
                        - code [ref=e716]: stylus.path
                        - text: to the
                        - strong [ref=e717]: user-data
                        - text: file causes cluster creation to fail as it cannot find
                        - link "kubeadm" [ref=e718] [cursor=pointer]:
                          - /url: https://kubernetes.io/docs/reference/setup-tools/kubeadm/
                        - text: .
                      - cell "Review the Edge Troubleshooting section for workarounds. Refer to Identify the Target Agent Version for guidance in retrieving your Palette agent version number." [ref=e719]:
                        - text: Review the
                        - link "Edge Troubleshooting" [ref=e720] [cursor=pointer]:
                          - /url: /troubleshooting/edge/
                        - text: section for workarounds. Refer to
                        - link "Identify the Target Agent Version" [ref=e721] [cursor=pointer]:
                          - /url: /clusters/edge/cluster-management/agent-upgrade-airgap/#identify-the-target-agent-version
                        - text: for guidance in retrieving your Palette agent version number.
                      - cell "January 19, 2025" [ref=e722]
                      - cell "Edge" [ref=e723]
                    - row "For clusters deployed with Palette eXtended Kubernetes - Edge (PXK-E) and agent mode, upgrades to higher Kubernetes versions are not supported with Palette agent version 4.5.12 or earlier. No workaround available. Upgrades to higher Kubernetes versions are only supported from Palette agent version 4.5.14 and above for clusters deployed with PXK-E and agent mode. Refer to Identify the Target Agent Version for guidance in retrieving your Palette agent version number. January 19, 2025 Edge" [ref=e724]:
                      - cell "For clusters deployed with Palette eXtended Kubernetes - Edge (PXK-E) and agent mode, upgrades to higher Kubernetes versions are not supported with Palette agent version 4.5.12 or earlier." [ref=e725]:
                        - text: For clusters deployed with
                        - link "Palette eXtended Kubernetes - Edge (PXK-E)" [ref=e726] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=edge-k8s
                        - text: and
                        - link "agent mode" [ref=e727] [cursor=pointer]:
                          - /url: /deployment-modes/agent-mode/
                        - text: ", upgrades to higher Kubernetes versions are not supported with Palette agent version 4.5.12 or earlier."
                      - cell "No workaround available. Upgrades to higher Kubernetes versions are only supported from Palette agent version 4.5.14 and above for clusters deployed with PXK-E and agent mode. Refer to Identify the Target Agent Version for guidance in retrieving your Palette agent version number." [ref=e728]:
                        - text: No workaround available. Upgrades to higher Kubernetes versions are only supported from Palette agent version 4.5.14 and above for clusters deployed with PXK-E and agent mode. Refer to
                        - link "Identify the Target Agent Version" [ref=e729] [cursor=pointer]:
                          - /url: /clusters/edge/cluster-management/agent-upgrade-airgap/#identify-the-target-agent-version
                        - text: for guidance in retrieving your Palette agent version number.
                      - cell "January 19, 2025" [ref=e730]
                      - cell "Edge" [ref=e731]
                    - row "Transferring the management of a local Edge cluster to central management by Palette or VerteX is not supported for multi-node clusters. No workaround is available. January 19, 2025 Edge" [ref=e732]:
                      - cell "Transferring the management of a local Edge cluster to central management by Palette or VerteX is not supported for multi-node clusters." [ref=e733]
                      - cell "No workaround is available." [ref=e734]
                      - cell "January 19, 2025" [ref=e735]
                      - cell "Edge" [ref=e736]
                    - row "Edits on the Hybrid Profile of an EKS Hybrid node pool take effect as soon as you click the Save button on the Configure Profile tab, not when you click Confirm on the Edit node pool screen. No workaround available. January 19, 2025 Clusters" [ref=e737]:
                      - cell "Edits on the Hybrid Profile of an EKS Hybrid node pool take effect as soon as you click the Save button on the Configure Profile tab, not when you click Confirm on the Edit node pool screen." [ref=e738]:
                        - text: Edits on the
                        - link "Hybrid Profile" [ref=e739] [cursor=pointer]:
                          - /url: /clusters/public-cloud/aws/eks-hybrid-nodes/create-hybrid-node-pools/#create-node-pool
                        - text: of an
                        - link "EKS Hybrid node pool" [ref=e740] [cursor=pointer]:
                          - /url: /clusters/public-cloud/aws/eks-hybrid-nodes/
                        - text: take effect as soon as you click the
                        - strong [ref=e741]: Save
                        - text: button on the
                        - strong [ref=e742]: Configure Profile
                        - text: tab, not when you click
                        - strong [ref=e743]: Confirm
                        - text: on the
                        - strong [ref=e744]: Edit node pool
                        - text: screen.
                      - cell "No workaround available." [ref=e745]
                      - cell "January 19, 2025" [ref=e746]
                      - cell "Clusters" [ref=e747]
                    - row "EKS Hybrid node statuses are not displayed accurately when an update is in progress. This has no effect on the update operation itself. No workaround available. January 19, 2025 Clusters" [ref=e748]:
                      - cell "EKS Hybrid node statuses are not displayed accurately when an update is in progress. This has no effect on the update operation itself." [ref=e749]:
                        - link "EKS Hybrid node" [ref=e750] [cursor=pointer]:
                          - /url: /clusters/public-cloud/aws/eks-hybrid-nodes/
                        - text: statuses are not displayed accurately when an update is in progress. This has no effect on the update operation itself.
                      - cell "No workaround available." [ref=e751]
                      - cell "January 19, 2025" [ref=e752]
                      - cell "Clusters" [ref=e753]
                    - row "Deleting an EKS Hybrid node from the Node Details page will result in an error in the Palette UI and the operation will have no effect. Additionally, deletion cannot be performed if the node pool is in the middle of an update operation. You can remove a node by changing the node pool instead. Refer to the Change a Node Pool page. Ensure that the node pool update only includes deletion and that the node to be deleted is in a Running state. January 19, 2025 Clusters" [ref=e754]:
                      - cell "Deleting an EKS Hybrid node from the Node Details page will result in an error in the Palette UI and the operation will have no effect. Additionally, deletion cannot be performed if the node pool is in the middle of an update operation." [ref=e755]:
                        - text: Deleting an
                        - link "EKS Hybrid node" [ref=e756] [cursor=pointer]:
                          - /url: /clusters/public-cloud/aws/eks-hybrid-nodes/
                        - text: from the Node Details page will result in an error in the Palette UI and the operation will have no effect. Additionally, deletion cannot be performed if the node pool is in the middle of an update operation.
                      - cell "You can remove a node by changing the node pool instead. Refer to the Change a Node Pool page. Ensure that the node pool update only includes deletion and that the node to be deleted is in a Running state." [ref=e757]:
                        - text: You can remove a node by changing the node pool instead. Refer to the
                        - link "Change a Node Pool" [ref=e758] [cursor=pointer]:
                          - /url: /clusters/cluster-management/node-pool/#change-a-node-pool
                        - text: page. Ensure that the node pool update only includes deletion and that the node to be deleted is in a Running state.
                      - cell "January 19, 2025" [ref=e759]
                      - cell "Clusters" [ref=e760]
                    - row "Maintenance mode cannot be activated on EKS Hybrid nodes. Attempting to activate maintenance mode will result in an error in the Palette UI and the operation will have no effect. No workaround available. January 19, 2025 Clusters" [ref=e761]:
                      - cell "Maintenance mode cannot be activated on EKS Hybrid nodes. Attempting to activate maintenance mode will result in an error in the Palette UI and the operation will have no effect." [ref=e762]:
                        - link "Maintenance mode" [ref=e763] [cursor=pointer]:
                          - /url: /clusters/cluster-management/maintenance-mode/#activate-maintenance-mode
                        - text: cannot be activated on
                        - link "EKS Hybrid nodes" [ref=e764] [cursor=pointer]:
                          - /url: /clusters/public-cloud/aws/eks-hybrid-nodes/
                        - text: . Attempting to activate maintenance mode will result in an error in the Palette UI and the operation will have no effect.
                      - cell "No workaround available." [ref=e765]
                      - cell "January 19, 2025" [ref=e766]
                      - cell "Clusters" [ref=e767]
                    - row "When using the VM Migration Assistant to migrate VMs to your VMO cluster, migration plans can enter an Unknown state if more VMs are selected for migration than the Max concurrent virtual machine migrations setting allows. Review the Virtual Machine Orchestrator (VMO) Troubleshooting section for workarounds. January 19, 2025 Virtual Machine Orchestrator" [ref=e768]:
                      - cell "When using the VM Migration Assistant to migrate VMs to your VMO cluster, migration plans can enter an Unknown state if more VMs are selected for migration than the Max concurrent virtual machine migrations setting allows." [ref=e769]:
                        - text: When using the
                        - link "VM Migration Assistant" [ref=e770] [cursor=pointer]:
                          - /url: /vm-management/vm-migration-assistant/
                        - text: to migrate VMs to your VMO cluster, migration plans can enter an
                        - strong [ref=e771]: Unknown
                        - text: state if more VMs are selected for migration than the
                        - strong [ref=e772]: Max concurrent virtual machine migrations
                        - text: setting allows.
                      - cell "Review the Virtual Machine Orchestrator (VMO) Troubleshooting section for workarounds." [ref=e773]:
                        - text: Review the
                        - link "Virtual Machine Orchestrator (VMO) Troubleshooting" [ref=e774] [cursor=pointer]:
                          - /url: /troubleshooting/vmo-issues/#scenario---virtual-machine-vm-migration-plans-in-unknown-state
                        - text: section for workarounds.
                      - cell "January 19, 2025" [ref=e775]
                      - cell "Virtual Machine Orchestrator" [ref=e776]
                    - row "Palette upgrades on K3s virtual clusters may be blocked if the cluster does not have enough resources to accommodate additional pods. Ensure that your cluster has 1 CPU, 1 GiB of memory, and 1 GiB storage of free resources before commencing an upgrade. You may increase the virtual cluster's resource quotas or disable them. Refer to the Adjust Virtual Clusters Limits guide for workaround steps. January 19, 2025 Virtual Clusters" [ref=e777]:
                      - cell "Palette upgrades on K3s virtual clusters may be blocked if the cluster does not have enough resources to accommodate additional pods. Ensure that your cluster has 1 CPU, 1 GiB of memory, and 1 GiB storage of free resources before commencing an upgrade. You may increase the virtual cluster's resource quotas or disable them." [ref=e778]
                      - cell "Refer to the Adjust Virtual Clusters Limits guide for workaround steps." [ref=e779]:
                        - text: Refer to the
                        - link "Adjust Virtual Clusters Limits" [ref=e780] [cursor=pointer]:
                          - /url: /troubleshooting/palette-dev-engine/#scenario---adjust-virtual-clusters-limits-before-palette-upgrades
                        - text: guide for workaround steps.
                      - cell "January 19, 2025" [ref=e781]
                      - cell "Virtual Clusters" [ref=e782]
                    - row "If you have manually configured the metrics server in your Edge airgap cluster using a manifest, upgrading to 4.5.15 may cause an additional metrics server pod to be created in your cluster. Remove the manifest layer that adds the manifest server from your cluster profile and apply the update on your cluster. December 15, 2024 Edge" [ref=e783]:
                      - cell "If you have manually configured the metrics server in your Edge airgap cluster using a manifest, upgrading to 4.5.15 may cause an additional metrics server pod to be created in your cluster." [ref=e784]
                      - cell "Remove the manifest layer that adds the manifest server from your cluster profile and apply the update on your cluster." [ref=e785]
                      - cell "December 15, 2024" [ref=e786]
                      - cell "Edge" [ref=e787]
                    - row "When deploying an Edge cluster using content bundles built from cluster profiles with PXK-E as the Kubernetes layer, some images in the Kubernetes layer fail to load into containerd. This issue occurs due to image signature problems, resulting in deployment failure. Remove the packs.content.images from the Kubernetes layer in the pack configuration before building the content bundle. These components are already included in the provider image and do not need to be included in the content bundle. December 13, 2024 Edge" [ref=e788]:
                      - cell "When deploying an Edge cluster using content bundles built from cluster profiles with PXK-E as the Kubernetes layer, some images in the Kubernetes layer fail to load into containerd. This issue occurs due to image signature problems, resulting in deployment failure." [ref=e789]
                      - cell "Remove the packs.content.images from the Kubernetes layer in the pack configuration before building the content bundle. These components are already included in the provider image and do not need to be included in the content bundle." [ref=e790]:
                        - text: Remove the
                        - code [ref=e791]: packs.content.images
                        - text: from the Kubernetes layer in the pack configuration before building the content bundle. These components are already included in the provider image and do not need to be included in the content bundle.
                      - cell "December 13, 2024" [ref=e792]
                      - cell "Edge" [ref=e793]
                    - row "Hosts provisioned in agent mode do not display host information in the console after using the Palette Terminal User Interface to complete host setup. Local UI is still available and will display host information. Refer to Access Local UI to learn how to access Local UI. December 12, 2024 Edge" [ref=e794]:
                      - cell "Hosts provisioned in agent mode do not display host information in the console after using the Palette Terminal User Interface to complete host setup." [ref=e795]:
                        - text: Hosts provisioned in
                        - link "agent mode" [ref=e796] [cursor=pointer]:
                          - /url: /deployment-modes/agent-mode/
                        - text: do not display host information in the console after using the Palette Terminal User Interface to complete host setup.
                      - cell "Local UI is still available and will display host information. Refer to Access Local UI to learn how to access Local UI." [ref=e797]:
                        - text: Local UI is still available and will display host information. Refer to
                        - link "Access Local UI" [ref=e798] [cursor=pointer]:
                          - /url: /clusters/edge/local-ui/host-management/access-console/
                        - text: to learn how to access Local UI.
                      - cell "December 12, 2024" [ref=e799]
                      - cell "Edge" [ref=e800]
                    - row "In a multi-node Edge cluster, the reset action on a cluster node does not update the node status on the leader node's linking screen. Scale down the cluster and free up the follower node before resetting the node. December 12, 2024 Edge" [ref=e801]:
                      - cell "In a multi-node Edge cluster, the reset action on a cluster node does not update the node status on the leader node's linking screen." [ref=e802]
                      - cell "Scale down the cluster and free up the follower node before resetting the node." [ref=e803]:
                        - link "Scale down" [ref=e804] [cursor=pointer]:
                          - /url: /clusters/edge/local-ui/cluster-management/scale-cluster/#scale-down-a-cluster
                        - text: the cluster and free up the follower node before resetting the node.
                      - cell "December 12, 2024" [ref=e805]
                      - cell "Edge" [ref=e806]
                    - row "For Edge airgap clusters, manifests attached to packs are not applied during cluster deployment. Add the manifest as a layer directly instead of attaching it to a pack. For more information, refer to Add a Manifest. November 15, 2024 Edge" [ref=e807]:
                      - cell "For Edge airgap clusters, manifests attached to packs are not applied during cluster deployment." [ref=e808]
                      - cell "Add the manifest as a layer directly instead of attaching it to a pack. For more information, refer to Add a Manifest." [ref=e809]:
                        - text: Add the manifest as a layer directly instead of attaching it to a pack. For more information, refer to
                        - link "Add a Manifest" [ref=e810] [cursor=pointer]:
                          - /url: /profiles/cluster-profiles/create-cluster-profiles/create-addon-profile/create-manifest-addon/
                        - text: .
                      - cell "November 15, 2024" [ref=e811]
                      - cell "Edge" [ref=e812]
                    - row "In some cases, the differential editor incorrectly reports YAML differences for customizations not created by you. The issue is more common when items in a list or array are removed. Clicking the Keep button when non-user-generated customization is the focus causes the button to become unresponsive after the first usage. Skip differential highlights not created by you. Click the arrow button to skip and proceed. November 11, 2024 Cluster Profiles" [ref=e813]:
                      - cell "In some cases, the differential editor incorrectly reports YAML differences for customizations not created by you. The issue is more common when items in a list or array are removed. Clicking the Keep button when non-user-generated customization is the focus causes the button to become unresponsive after the first usage." [ref=e814]:
                        - text: In some cases, the differential editor incorrectly reports YAML differences for customizations not created by you. The issue is more common when items in a list or array are removed. Clicking the
                        - strong [ref=e815]: Keep
                        - text: button when non-user-generated customization is the focus causes the button to become unresponsive after the first usage.
                      - cell "Skip differential highlights not created by you. Click the arrow button to skip and proceed." [ref=e816]
                      - cell "November 11, 2024" [ref=e817]
                      - cell "Cluster Profiles" [ref=e818]
                    - row "Palette fails to provision virtual clusters on airgapped and proxy Edge cluster groups. This error is caused by Palette incorrectly defaulting to fetch charts from an external repository, which is unreachable from these environments. No workaround. November 9, 2024 Virtual Clusters" [ref=e819]:
                      - cell "Palette fails to provision virtual clusters on airgapped and proxy Edge cluster groups. This error is caused by Palette incorrectly defaulting to fetch charts from an external repository, which is unreachable from these environments." [ref=e820]
                      - cell "No workaround." [ref=e821]
                      - cell "November 9, 2024" [ref=e822]
                      - cell "Virtual Clusters" [ref=e823]
                    - row "The resource limits on Palette Virtual Clusters are too low and may cause the Palette agent to experience resource exhaustion. As a result, Palette pods required for Palette operations may experience Out-of-Memory (OOM) errors. Refer to the Apply Host Cluster Resource Limits to Virtual Cluster guide for workaround steps. November 4, 2024 Virtual Clusters" [ref=e824]:
                      - cell "The resource limits on Palette Virtual Clusters are too low and may cause the Palette agent to experience resource exhaustion. As a result, Palette pods required for Palette operations may experience Out-of-Memory (OOM) errors." [ref=e825]
                      - cell "Refer to the Apply Host Cluster Resource Limits to Virtual Cluster guide for workaround steps." [ref=e826]:
                        - text: Refer to the
                        - link "Apply Host Cluster Resource Limits to Virtual Cluster" [ref=e827] [cursor=pointer]:
                          - /url: /troubleshooting/palette-dev-engine/#scenario---apply-host-cluster-resource-limits-to-virtual-cluster
                        - text: guide for workaround steps.
                      - cell "November 4, 2024" [ref=e828]
                      - cell "Virtual Clusters" [ref=e829]
                    - row "Palette incorrectly modifies the indentation of the Prometheus Operator pack after it is configured as a cluster profile layer. The modified indentation does not cause errors, but you may observe changes to the pack values.yaml. No workaround available. October 30, 2024 Cluster Profiles, Pack" [ref=e830]:
                      - cell "Palette incorrectly modifies the indentation of the Prometheus Operator pack after it is configured as a cluster profile layer. The modified indentation does not cause errors, but you may observe changes to the pack values.yaml." [ref=e831]:
                        - text: Palette incorrectly modifies the indentation of the
                        - link "Prometheus Operator" [ref=e832] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=prometheus-operator
                        - text: pack after it is configured as a cluster profile layer. The modified indentation does not cause errors, but you may observe changes to the pack
                        - strong [ref=e833]: values.yaml
                        - text: .
                      - cell "No workaround available." [ref=e834]
                      - cell "October 30, 2024" [ref=e835]
                      - cell "Cluster Profiles, Pack" [ref=e836]
                    - row "Palette does not correctly configure multiple search domains when provided during the self-hosted installation. The configuration file resolve.conf ends up containing incorrect values. Connect remotely to each node in the Palette self-hosted instance and edit the resolution.conf configuration file. October 17, 2024 Self-Hosted, PCG" [ref=e837]:
                      - cell "Palette does not correctly configure multiple search domains when provided during the self-hosted installation. The configuration file resolve.conf ends up containing incorrect values." [ref=e838]:
                        - text: Palette does not correctly configure multiple search domains when provided during the self-hosted installation. The configuration file
                        - strong [ref=e839]: resolve.conf
                        - text: ends up containing incorrect values.
                      - cell "Connect remotely to each node in the Palette self-hosted instance and edit the resolution.conf configuration file." [ref=e840]:
                        - text: Connect remotely to each node in the Palette self-hosted instance and edit the
                        - strong [ref=e841]: resolution.conf
                        - text: configuration file.
                      - cell "October 17, 2024" [ref=e842]
                      - cell "Self-Hosted, PCG" [ref=e843]
                    - row "Upgrading the RKE2 version from 1.29 to 1.30 fails due to an upstream issue with RKE2 and Cilium. Refer to the Troubleshooting section for the workaround. October 12, 2024 Edge" [ref=e844]:
                      - cell "Upgrading the RKE2 version from 1.29 to 1.30 fails due to an upstream issue with RKE2 and Cilium." [ref=e845]:
                        - text: Upgrading the RKE2 version from 1.29 to 1.30 fails due to
                        - link "an upstream issue" [ref=e846] [cursor=pointer]:
                          - /url: https://github.com/rancher/rancher/issues/46726
                        - text: with RKE2 and Cilium.
                      - cell "Refer to the Troubleshooting section for the workaround." [ref=e847]:
                        - text: Refer to the
                        - link "Troubleshooting section" [ref=e848] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---clusters-with-cilium-and-rke2-experiences-kubernetes-upgrade-failure
                        - text: for the workaround.
                      - cell "October 12, 2024" [ref=e849]
                      - cell "Edge" [ref=e850]
                    - row "Kubernetes clusters deployed on MAAS with Microk8s are experiencing deployment issues when using the upgrade strategy RollingUpgrade. This issue is affecting new cluster deployments and node provisioning. Use the InPlaceUpgrade strategy to upgrade nodes deployed in MAAS. October 12, 2024 Clusters, Pack" [ref=e851]:
                      - cell "Kubernetes clusters deployed on MAAS with Microk8s are experiencing deployment issues when using the upgrade strategy RollingUpgrade. This issue is affecting new cluster deployments and node provisioning." [ref=e852]:
                        - text: Kubernetes clusters deployed on MAAS with Microk8s are experiencing deployment issues when using the upgrade strategy
                        - code [ref=e853]: RollingUpgrade
                        - text: . This issue is affecting new cluster deployments and node provisioning.
                      - cell "Use the InPlaceUpgrade strategy to upgrade nodes deployed in MAAS." [ref=e854]:
                        - text: Use the
                        - code [ref=e855]: InPlaceUpgrade
                        - text: strategy to upgrade nodes deployed in MAAS.
                      - cell "October 12, 2024" [ref=e856]
                      - cell "Clusters, Pack" [ref=e857]
                    - row "Clusters using Mircrok8s and conducting backup and restore operations using Velero with restic are encountering restic pods going into the crashloopbackoff state. This issue stems from an upstream problem in the Velero project. You can learn more about it in the GitHub issue 4035 page. Refer to the MicroK8s Additional Details section for troubleshooting workaround steps. October 12, 2024 Clusters" [ref=e858]:
                      - cell "Clusters using Mircrok8s and conducting backup and restore operations using Velero with restic are encountering restic pods going into the crashloopbackoff state. This issue stems from an upstream problem in the Velero project. You can learn more about it in the GitHub issue 4035 page." [ref=e859]:
                        - text: Clusters using Mircrok8s and conducting backup and restore operations using Velero with
                        - link "restic" [ref=e860] [cursor=pointer]:
                          - /url: https://github.com/restic/restic
                        - text: are encountering restic pods going into the
                        - code [ref=e861]: crashloopbackoff
                        - text: state. This issue stems from an upstream problem in the Velero project. You can learn more about it in the GitHub issue
                        - link "4035" [ref=e862] [cursor=pointer]:
                          - /url: https://github.com/vmware-tanzu/velero/issues/4035
                        - text: page.
                      - cell "Refer to the MicroK8s Additional Details section for troubleshooting workaround steps." [ref=e863]:
                        - text: Refer to the
                        - link "MicroK8s" [ref=e864] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=kubernetes-microk8s
                        - text: Additional Details section for troubleshooting workaround steps.
                      - cell "October 12, 2024" [ref=e865]
                      - cell "Clusters" [ref=e866]
                    - row "Clusters deployed with Microk8s cannot accept kubectl commands if the Spectro Proxy pack is added to the cluster's cluster profile. The reason behind this issue is Microk8s' lack of support for certSANs. This causes the Kubernetes API server to reject Spectro Proxy certificates. Check out GitHub issue 114 in the MircoK8s cluster-api-bootstrap-provider-microk8s repository to learn more. Use the admin kubeconfig file to access the cluster API, as it does not use the Spectro Proxy server. This option may be limited to environments where you can access the cluster directly from a network perspective. October 1, 2024 Clusters, Pack" [ref=e867]:
                      - cell "Clusters deployed with Microk8s cannot accept kubectl commands if the Spectro Proxy pack is added to the cluster's cluster profile. The reason behind this issue is Microk8s' lack of support for certSANs. This causes the Kubernetes API server to reject Spectro Proxy certificates. Check out GitHub issue 114 in the MircoK8s cluster-api-bootstrap-provider-microk8s repository to learn more." [ref=e868]:
                        - text: Clusters deployed with Microk8s cannot accept kubectl commands if the
                        - link "Spectro Proxy" [ref=e869] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=spectro-proxy
                        - text: pack is added to the cluster's cluster profile. The reason behind this issue is Microk8s' lack of support for
                        - code [ref=e870]: certSANs
                        - text: . This causes the Kubernetes API server to reject Spectro Proxy certificates. Check out GitHub issue
                        - link "114" [ref=e871] [cursor=pointer]:
                          - /url: https://github.com/canonical/cluster-api-bootstrap-provider-microk8s/issues/114
                        - text: in the MircoK8s cluster-api-bootstrap-provider-microk8s repository to learn more.
                      - cell "Use the admin kubeconfig file to access the cluster API, as it does not use the Spectro Proxy server. This option may be limited to environments where you can access the cluster directly from a network perspective." [ref=e872]:
                        - text: Use the
                        - link "admin kubeconfig file" [ref=e873] [cursor=pointer]:
                          - /url: /clusters/cluster-management/kubeconfig/#kubeconfig-files
                        - text: to access the cluster API, as it does not use the Spectro Proxy server. This option may be limited to environments where you can access the cluster directly from a network perspective.
                      - cell "October 1, 2024" [ref=e874]
                      - cell "Clusters, Pack" [ref=e875]
                    - row "Clusters deployed with Microk8s cannot accept kubectl commands if the Spectro Proxy pack is added to the cluster's cluster profile. The reason behind these issues is Microk8s' lack of support for certSANs . This causes the Kubernetes API server to reject Spectro Proxy certificates. Use the CLI flag --insecure-skip-tls-verify with kubectl commands or use the admin kubeconfig file to access the cluster API, as it does not use the Spectro Proxy server. This option may be limited to environments where you can access the cluster directly from a network perspective. October 1, 2024 Clusters, Pack" [ref=e876]:
                      - cell "Clusters deployed with Microk8s cannot accept kubectl commands if the Spectro Proxy pack is added to the cluster's cluster profile. The reason behind these issues is Microk8s' lack of support for certSANs . This causes the Kubernetes API server to reject Spectro Proxy certificates." [ref=e877]:
                        - text: Clusters deployed with Microk8s cannot accept kubectl commands if the
                        - link "Spectro Proxy" [ref=e878] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=spectro-proxy
                        - text: pack is added to the cluster's cluster profile. The reason behind these issues is Microk8s' lack of support for
                        - code [ref=e879]: certSANs
                        - text: . This causes the Kubernetes API server to reject Spectro Proxy certificates.
                      - cell "Use the CLI flag --insecure-skip-tls-verify with kubectl commands or use the admin kubeconfig file to access the cluster API, as it does not use the Spectro Proxy server. This option may be limited to environments where you can access the cluster directly from a network perspective." [ref=e880]:
                        - text: Use the CLI flag
                        - link "--insecure-skip-tls-verify" [ref=e881] [cursor=pointer]:
                          - /url: https://kubernetes.io/docs/reference/kubectl/kubectl/
                          - code [ref=e882]: "--insecure-skip-tls-verify"
                        - text: with kubectl commands or use the
                        - link "admin kubeconfig file" [ref=e883] [cursor=pointer]:
                          - /url: /clusters/cluster-management/kubeconfig/#kubeconfig-files
                        - text: to access the cluster API, as it does not use the Spectro Proxy server. This option may be limited to environments where you can access the cluster directly from a network perspective.
                      - cell "October 1, 2024" [ref=e884]
                      - cell "Clusters, Pack" [ref=e885]
                    - row "Deploying new Nutanix clusters fails for self-hosted Palette or VerteX users on version 4.4.18 or newer. No workaround is available. September 26, 2024 Clusters" [ref=e886]:
                      - cell "Deploying new Nutanix clusters fails for self-hosted Palette or VerteX users on version 4.4.18 or newer." [ref=e887]:
                        - text: Deploying new
                        - link "Nutanix clusters" [ref=e888] [cursor=pointer]:
                          - /url: /clusters/data-center/nutanix/
                        - text: fails for self-hosted Palette or VerteX users on version 4.4.18 or newer.
                      - cell "No workaround is available." [ref=e889]
                      - cell "September 26, 2024" [ref=e890]
                      - cell "Clusters" [ref=e891]
                    - row "OCI Helm registries added to Palette or VerteX before support for OCI Helm registries hosted in AWS ECR was available in Palette have an invalid API payload that is causing cluster imports to fail if the OCI Helm Registry is referenced in the cluster profile. Log in to Palette as a tenant administrator and navigate to the left Main Menu . Select Registries and click on the OCI Registries tab. For each OCI registry of the Helm type, click on the three-dot Menu at the end of the row. Select Edit. To fix the invalid API payload, click on Confirm. Palette will automatically add the correct provider type behind the scenes to address the issue. September 25, 2024 Helm Registries" [ref=e892]:
                      - cell "OCI Helm registries added to Palette or VerteX before support for OCI Helm registries hosted in AWS ECR was available in Palette have an invalid API payload that is causing cluster imports to fail if the OCI Helm Registry is referenced in the cluster profile." [ref=e893]
                      - cell "Log in to Palette as a tenant administrator and navigate to the left Main Menu . Select Registries and click on the OCI Registries tab. For each OCI registry of the Helm type, click on the three-dot Menu at the end of the row. Select Edit. To fix the invalid API payload, click on Confirm. Palette will automatically add the correct provider type behind the scenes to address the issue." [ref=e894]:
                        - text: Log in to Palette as a tenant administrator and navigate to the left
                        - strong [ref=e895]: Main Menu
                        - text: . Select
                        - strong [ref=e896]: Registries
                        - text: and click on the
                        - strong [ref=e897]: OCI Registries
                        - text: tab. For each OCI registry of the Helm type, click on the
                        - strong [ref=e898]: three-dot Menu
                        - text: at the end of the row. Select
                        - strong [ref=e899]: Edit
                        - text: . To fix the invalid API payload, click on
                        - strong [ref=e900]: Confirm
                        - text: . Palette will automatically add the correct provider type behind the scenes to address the issue.
                      - cell "September 25, 2024" [ref=e901]
                      - cell "Helm Registries" [ref=e902]
                    - row "Airgap self-hosted Palette or VerteX instances cannot use the Container service in App Profiles. The required dependency, DevSpace, is unavailable from the Palette pack registry and is downloaded from the Internet at runtime. Use the manifest service in an App Profile to specify a custom container image. September 25, 2024 App Mode" [ref=e903]:
                      - cell "Airgap self-hosted Palette or VerteX instances cannot use the Container service in App Profiles. The required dependency, DevSpace, is unavailable from the Palette pack registry and is downloaded from the Internet at runtime." [ref=e904]:
                        - text: Airgap self-hosted Palette or VerteX instances cannot use the Container service in App Profiles. The required dependency,
                        - link "DevSpace" [ref=e905] [cursor=pointer]:
                          - /url: https://github.com/devspace-sh/devspace
                        - text: ", is unavailable from the Palette pack registry and is downloaded from the Internet at runtime."
                      - cell "Use the manifest service in an App Profile to specify a custom container image." [ref=e906]:
                        - text: Use the manifest service in an
                        - link "App Profile" [ref=e907] [cursor=pointer]:
                          - /url: /profiles/app-profiles/
                        - text: to specify a custom container image.
                      - cell "September 25, 2024" [ref=e908]
                      - cell "App Mode" [ref=e909]
                    - row "Using the Flannel Container Network Interface (CSI) pack together with a Red Hat Enterprise Linux (RHEL)-based provider image may lead to a pod becoming stuck during deployment. This is caused by an upstream issue with Flannel that was discovered in a K3s GitHub issue. Refer to the K3s issue page for more information. No workaround is available September 14, 2024 Edge" [ref=e910]:
                      - cell "Using the Flannel Container Network Interface (CSI) pack together with a Red Hat Enterprise Linux (RHEL)-based provider image may lead to a pod becoming stuck during deployment. This is caused by an upstream issue with Flannel that was discovered in a K3s GitHub issue. Refer to the K3s issue page for more information." [ref=e911]:
                        - text: Using the Flannel Container Network Interface (CSI) pack together with a Red Hat Enterprise Linux (RHEL)-based provider image may lead to a pod becoming stuck during deployment. This is caused by an upstream issue with Flannel that was discovered in a K3s GitHub issue. Refer to
                        - link "the K3s issue page" [ref=e912] [cursor=pointer]:
                          - /url: https://github.com/k3s-io/k3s/issues/5013
                        - text: for more information.
                      - cell "No workaround is available" [ref=e913]
                      - cell "September 14, 2024" [ref=e914]
                      - cell "Edge" [ref=e915]
                    - row "Palette OVA import operations fail if the VMO cluster is using a storageClass with the volume bind method WaitForFirstConsumer. Refer to the OVA Imports Fail Due To Storage Class Attribute troubleshooting guide for workaround steps. September 13, 2024 Palette CLI, VMO" [ref=e916]:
                      - cell "Palette OVA import operations fail if the VMO cluster is using a storageClass with the volume bind method WaitForFirstConsumer." [ref=e917]:
                        - text: Palette OVA import operations fail if the VMO cluster is using a storageClass with the volume bind method
                        - code [ref=e918]: WaitForFirstConsumer
                        - text: .
                      - cell "Refer to the OVA Imports Fail Due To Storage Class Attribute troubleshooting guide for workaround steps." [ref=e919]:
                        - text: Refer to the
                        - link "OVA Imports Fail Due To Storage Class Attribute" [ref=e920] [cursor=pointer]:
                          - /url: /troubleshooting/vmo-issues/#scenario---ova-imports-fail-due-to-storage-class-attribute
                        - text: troubleshooting guide for workaround steps.
                      - cell "September 13, 2024" [ref=e921]
                      - cell "Palette CLI, VMO" [ref=e922]
                    - row "Persistent Volume Claims (PVCs) metadata do not use a unique identifier for self-hosted Palette clusters. This causes incorrect Cloud Native Storage (CNS) mappings in vSphere, potentially leading to issues during node operations and cluster upgrades. Refer to the Troubleshooting section for guidance. September 13, 2024 Self-hosted" [ref=e923]:
                      - cell "Persistent Volume Claims (PVCs) metadata do not use a unique identifier for self-hosted Palette clusters. This causes incorrect Cloud Native Storage (CNS) mappings in vSphere, potentially leading to issues during node operations and cluster upgrades." [ref=e924]
                      - cell "Refer to the Troubleshooting section for guidance." [ref=e925]:
                        - text: Refer to the
                        - link "Troubleshooting section" [ref=e926] [cursor=pointer]:
                          - /url: /troubleshooting/enterprise-install/#scenario---non-unique-vsphere-cns-mapping
                        - text: for guidance.
                      - cell "September 13, 2024" [ref=e927]
                      - cell "Self-hosted" [ref=e928]
                    - row "Third-party binaries downloaded and used by the Palette CLI may become stale and incompatible with the CLI. Refer to the Incompatible Stale Palette CLI Binaries troubleshooting guide for workaround guidance. September 11, 2024 CLI" [ref=e929]:
                      - cell "Third-party binaries downloaded and used by the Palette CLI may become stale and incompatible with the CLI." [ref=e930]
                      - cell "Refer to the Incompatible Stale Palette CLI Binaries troubleshooting guide for workaround guidance." [ref=e931]:
                        - text: Refer to the
                        - link "Incompatible Stale Palette CLI Binaries" [ref=e932] [cursor=pointer]:
                          - /url: /troubleshooting/automation/#scenario---incompatible-stale-palette-cli-binaries
                        - text: troubleshooting guide for workaround guidance.
                      - cell "September 11, 2024" [ref=e933]
                      - cell "CLI" [ref=e934]
                    - row "An issue with Edge hosts using Trusted Boot and encrypted drives occurs when TRIM is not enabled. As a result, Solid-State Drive and Nonvolatile Memory Express drives experience degraded performance and potentially cause cluster failures. This issue stems from Kairos not passing through the --allow-discards flag to the systemd-cryptsetup attach command. Check out the Degraded Performance on Disk Drives troubleshooting guide for guidance on workaround. September 4, 2024 Edge" [ref=e935]:
                      - cell "An issue with Edge hosts using Trusted Boot and encrypted drives occurs when TRIM is not enabled. As a result, Solid-State Drive and Nonvolatile Memory Express drives experience degraded performance and potentially cause cluster failures. This issue stems from Kairos not passing through the --allow-discards flag to the systemd-cryptsetup attach command." [ref=e936]:
                        - text: An issue with Edge hosts using
                        - link "Trusted Boot" [ref=e937] [cursor=pointer]:
                          - /url: /clusters/edge/trusted-boot/
                        - text: and encrypted drives occurs when TRIM is not enabled. As a result, Solid-State Drive and Nonvolatile Memory Express drives experience degraded performance and potentially cause cluster failures. This
                        - link "issue" [ref=e938] [cursor=pointer]:
                          - /url: https://github.com/kairos-io/kairos/issues/2693
                        - text: stems from
                        - link "Kairos" [ref=e939] [cursor=pointer]:
                          - /url: https://kairos.io/
                        - text: not passing through the
                        - code [ref=e940]: "--allow-discards"
                        - text: flag to the
                        - code [ref=e941]: systemd-cryptsetup attach
                        - text: command.
                      - cell "Check out the Degraded Performance on Disk Drives troubleshooting guide for guidance on workaround." [ref=e942]:
                        - text: Check out the
                        - link "Degraded Performance on Disk Drives" [ref=e943] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---degraded-performance-on-disk-drives
                        - text: troubleshooting guide for guidance on workaround.
                      - cell "September 4, 2024" [ref=e944]
                      - cell "Edge" [ref=e945]
                    - row "The AWS CSI pack has a Pod Disruption Budget (PDB) that allows for a maximum of one unavailable pod. This behavior causes an issue for single-node clusters as well as clusters with a single control plane node and a single worker node where the control plane lacks worker capability. Operating System (OS) patch updates may attempt to evict the CSI controller without success, resulting in the node remaining in the un-schedulable state. If OS patching is enabled, allow the control plane nodes to have worker capability. For single-node clusters, turn off the OS patching feature. September 4, 2024 Cluster, Packs" [ref=e946]:
                      - cell "The AWS CSI pack has a Pod Disruption Budget (PDB) that allows for a maximum of one unavailable pod. This behavior causes an issue for single-node clusters as well as clusters with a single control plane node and a single worker node where the control plane lacks worker capability. Operating System (OS) patch updates may attempt to evict the CSI controller without success, resulting in the node remaining in the un-schedulable state." [ref=e947]:
                        - text: The AWS CSI pack has a
                        - link "Pod Disruption Budget" [ref=e948] [cursor=pointer]:
                          - /url: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
                        - text: (PDB) that allows for a maximum of one unavailable pod. This behavior causes an issue for single-node clusters as well as clusters with a single control plane node and a single worker node where the control plane lacks worker capability.
                        - link "Operating System (OS) patch" [ref=e949] [cursor=pointer]:
                          - /url: /clusters/cluster-management/os-patching/
                        - text: updates may attempt to evict the CSI controller without success, resulting in the node remaining in the un-schedulable state.
                      - cell "If OS patching is enabled, allow the control plane nodes to have worker capability. For single-node clusters, turn off the OS patching feature." [ref=e950]
                      - cell "September 4, 2024" [ref=e951]
                      - cell "Cluster, Packs" [ref=e952]
                    - row "On AWS IaaS Microk8s clusters, OS patching can get stuck and fail. Refer to the Troubleshooting section for debug steps. August 17, 2024 Palette" [ref=e953]:
                      - cell "On AWS IaaS Microk8s clusters, OS patching can get stuck and fail." [ref=e954]
                      - cell "Refer to the Troubleshooting section for debug steps." [ref=e955]:
                        - text: Refer to the
                        - link "Troubleshooting" [ref=e956] [cursor=pointer]:
                          - /url: /troubleshooting/nodes/#os-patch-fails-on-aws-with-microk8s-127
                        - text: section for debug steps.
                      - cell "August 17, 2024" [ref=e957]
                      - cell "Palette" [ref=e958]
                    - 'row "When upgrading a self-hosted Palette instance from 4.3 to 4.4 the MongoDB pod may be stuck with the following error: ReadConcernMajorityNotAvailableYet: Read concern majority reads are currently not possible. Delete the PVC, PV and the pod manually. All resources will be recreated with the correct configuration. August 17, 2024 Self-Hosted Palette" [ref=e959]':
                      - 'cell "When upgrading a self-hosted Palette instance from 4.3 to 4.4 the MongoDB pod may be stuck with the following error: ReadConcernMajorityNotAvailableYet: Read concern majority reads are currently not possible." [ref=e960]':
                        - text: "When upgrading a self-hosted Palette instance from 4.3 to 4.4 the MongoDB pod may be stuck with the following error:"
                        - code [ref=e961]: "ReadConcernMajorityNotAvailableYet: Read concern majority reads are currently not possible."
                      - cell "Delete the PVC, PV and the pod manually. All resources will be recreated with the correct configuration." [ref=e962]
                      - cell "August 17, 2024" [ref=e963]
                      - cell "Self-Hosted Palette" [ref=e964]
                    - row "For existing clusters that have added a new machine and all new clusters, pods may be stuck in the draining process and require manual intervention to drain the pod. Manually delete the pod if it is stuck in the draining process. August 17, 2024 Palette" [ref=e965]:
                      - cell "For existing clusters that have added a new machine and all new clusters, pods may be stuck in the draining process and require manual intervention to drain the pod." [ref=e966]
                      - cell "Manually delete the pod if it is stuck in the draining process." [ref=e967]
                      - cell "August 17, 2024" [ref=e968]
                      - cell "Palette" [ref=e969]
                    - row "Clusters with the Virtual Machine Orchestrator (VMO) pack may experience VMs getting stuck in a continuous migration loop, as indicated by a Migrating or Migration VM status. Review the Virtual Machine Orchestrator (VMO) Troubleshooting section for workarounds. August 1, 2024 Virtual Machine Orchestrator" [ref=e970]:
                      - cell "Clusters with the Virtual Machine Orchestrator (VMO) pack may experience VMs getting stuck in a continuous migration loop, as indicated by a Migrating or Migration VM status." [ref=e971]:
                        - text: Clusters with the Virtual Machine Orchestrator (VMO) pack may experience VMs getting stuck in a continuous migration loop, as indicated by a
                        - code [ref=e972]: Migrating
                        - text: or
                        - code [ref=e973]: Migration
                        - text: VM status.
                      - cell "Review the Virtual Machine Orchestrator (VMO) Troubleshooting section for workarounds." [ref=e974]:
                        - text: Review the
                        - link "Virtual Machine Orchestrator (VMO) Troubleshooting" [ref=e975] [cursor=pointer]:
                          - /url: /troubleshooting/vmo-issues/
                        - text: section for workarounds.
                      - cell "August 1, 2024" [ref=e976]
                      - cell "Virtual Machine Orchestrator" [ref=e977]
                    - row "Palette CLI users who authenticated with the login command and specified a Palette console endpoint that does not contain the tenant name are encountering issues with expired JWT tokens. Re-authenticate using your tenant URL, for example, https://my-org.console.spectrocloud.com. If the issue persists after re-authenticating, remove the ~/.palette/palette.yaml file that is auto-generated by the Palette CLI. Re-authenticate with the login command if other commands require it. July 25, 2024 CLI" [ref=e978]:
                      - cell "Palette CLI users who authenticated with the login command and specified a Palette console endpoint that does not contain the tenant name are encountering issues with expired JWT tokens." [ref=e979]:
                        - text: Palette CLI users who authenticated with the
                        - code [ref=e980]: login
                        - text: command and specified a Palette console endpoint that does not contain the tenant name are encountering issues with expired JWT tokens.
                      - cell "Re-authenticate using your tenant URL, for example, https://my-org.console.spectrocloud.com. If the issue persists after re-authenticating, remove the ~/.palette/palette.yaml file that is auto-generated by the Palette CLI. Re-authenticate with the login command if other commands require it." [ref=e981]:
                        - text: Re-authenticate using your tenant URL, for example,
                        - code [ref=e982]: https://my-org.console.spectrocloud.com.
                        - text: If the issue persists after re-authenticating, remove the
                        - code [ref=e983]: ~/.palette/palette.yaml
                        - text: file that is auto-generated by the Palette CLI. Re-authenticate with the
                        - code [ref=e984]: login
                        - text: command if other commands require it.
                      - cell "July 25, 2024" [ref=e985]
                      - cell "CLI" [ref=e986]
                    - row "Adding new cloud providers, such as Nutanix, is currently unavailable. Private Cloud Gateway (PCG) deployments in new Nutanix environments fail to complete the installation. As a result, adding a new Nutanix environment to launch new host clusters is unavailable. This does not impact existing Nutanix deployments with a PCG deployed. No workarounds are available. July 20, 2024 Clusters, Self-Hosted, PCG" [ref=e987]:
                      - cell "Adding new cloud providers, such as Nutanix, is currently unavailable. Private Cloud Gateway (PCG) deployments in new Nutanix environments fail to complete the installation. As a result, adding a new Nutanix environment to launch new host clusters is unavailable. This does not impact existing Nutanix deployments with a PCG deployed." [ref=e988]
                      - cell "No workarounds are available." [ref=e989]
                      - cell "July 20, 2024" [ref=e990]
                      - cell "Clusters, Self-Hosted, PCG" [ref=e991]
                    - row "Single-node Private Cloud Gateway (PCG) clusters are experiencing an issue upgrading to 4.4.11. The vSphere CSI controller pod fails to start because there are no matching affinity rules. Check out the vSphere Controller Pod Fails to Start in Single Node PCG Cluster guide for workaround steps. July 20, 2024 PCG" [ref=e992]:
                      - cell "Single-node Private Cloud Gateway (PCG) clusters are experiencing an issue upgrading to 4.4.11. The vSphere CSI controller pod fails to start because there are no matching affinity rules." [ref=e993]
                      - cell "Check out the vSphere Controller Pod Fails to Start in Single Node PCG Cluster guide for workaround steps." [ref=e994]:
                        - text: Check out the
                        - link "vSphere Controller Pod Fails to Start in Single Node PCG Cluster" [ref=e995] [cursor=pointer]:
                          - /url: /troubleshooting/pcg/#scenario---vsphere-controller-pod-fails-to-start-in-single-node-pcg-cluster
                        - text: guide for workaround steps.
                      - cell "July 20, 2024" [ref=e996]
                      - cell "PCG" [ref=e997]
                    - row "When provisioning an Edge cluster, it's possible that some Operating System (OS) user credentials will be lost once the cluster is active. This is because the cloud-init stages from different sources merge during the deployment process, and sometimes, the same stages without distinct names overwrite each other. Give each of your cloud-init stages in the OS pack and in the Edge installer user-data file a unique name. For more information about cloud-init stages and examples of cloud-init stages with names, refer to Cloud-init Stages. July 17, 2024 Edge" [ref=e998]:
                      - cell "When provisioning an Edge cluster, it's possible that some Operating System (OS) user credentials will be lost once the cluster is active. This is because the cloud-init stages from different sources merge during the deployment process, and sometimes, the same stages without distinct names overwrite each other." [ref=e999]
                      - cell "Give each of your cloud-init stages in the OS pack and in the Edge installer user-data file a unique name. For more information about cloud-init stages and examples of cloud-init stages with names, refer to Cloud-init Stages." [ref=e1000]:
                        - text: Give each of your cloud-init stages in the OS pack and in the Edge installer
                        - strong [ref=e1001]: user-data
                        - text: file a unique name. For more information about cloud-init stages and examples of cloud-init stages with names, refer to
                        - link "Cloud-init Stages" [ref=e1002] [cursor=pointer]:
                          - /url: /clusters/edge/edge-configuration/cloud-init/
                        - text: .
                      - cell "July 17, 2024" [ref=e1003]
                      - cell "Edge" [ref=e1004]
                    - row "When you use a content bundle to provision a new cluster without using the local Harbor registry, it's possible for the images to be pulled from external networks instead of from the content bundle, consuming network bandwidth. If your Edge host has no connection to external networks or if it cannot locate the image on a remote registry, some pods may enter the ImagePullBackOff state at first, but eventually the pods will be created using images from the content bundle. For connected clusters, you can make sure that the remote images are not reachable by the Edge host, which will stop the Palette agent from downloading the image and consuming bandwidth, and eventually the cluster will be created using images from the content bundle. For airgap clusters, the ImagePullBackOff error will eventually resolve on its own and there is no action to take. July 11, 2024 Edge" [ref=e1005]:
                      - cell "When you use a content bundle to provision a new cluster without using the local Harbor registry, it's possible for the images to be pulled from external networks instead of from the content bundle, consuming network bandwidth. If your Edge host has no connection to external networks or if it cannot locate the image on a remote registry, some pods may enter the ImagePullBackOff state at first, but eventually the pods will be created using images from the content bundle." [ref=e1006]:
                        - text: When you use a content bundle to provision a new cluster without using the local Harbor registry, it's possible for the images to be pulled from external networks instead of from the content bundle, consuming network bandwidth. If your Edge host has no connection to external networks or if it cannot locate the image on a remote registry, some pods may enter the
                        - code [ref=e1007]: ImagePullBackOff
                        - text: state at first, but eventually the pods will be created using images from the content bundle.
                      - cell "For connected clusters, you can make sure that the remote images are not reachable by the Edge host, which will stop the Palette agent from downloading the image and consuming bandwidth, and eventually the cluster will be created using images from the content bundle. For airgap clusters, the ImagePullBackOff error will eventually resolve on its own and there is no action to take." [ref=e1008]:
                        - text: For connected clusters, you can make sure that the remote images are not reachable by the Edge host, which will stop the Palette agent from downloading the image and consuming bandwidth, and eventually the cluster will be created using images from the content bundle. For airgap clusters, the
                        - code [ref=e1009]: ImagePullBackOff
                        - text: error will eventually resolve on its own and there is no action to take.
                      - cell "July 11, 2024" [ref=e1010]
                      - cell "Edge" [ref=e1011]
                    - row "When you add a new VMware vSphere Edge host to an Edge cluster, the IP address may fail to be assigned to the Edge host after a reboot. Review the Edge Troubleshooting section for workarounds. July 9, 2024 Edge" [ref=e1012]:
                      - cell "When you add a new VMware vSphere Edge host to an Edge cluster, the IP address may fail to be assigned to the Edge host after a reboot." [ref=e1013]
                      - cell "Review the Edge Troubleshooting section for workarounds." [ref=e1014]:
                        - text: Review the
                        - link "Edge Troubleshooting" [ref=e1015] [cursor=pointer]:
                          - /url: /troubleshooting/edge/
                        - text: section for workarounds.
                      - cell "July 9, 2024" [ref=e1016]
                      - cell "Edge" [ref=e1017]
                    - row "When you install Palette Edge using an Edge Installer ISO with a RHEL 8 operating system on a Virtual Machine (VM) with insufficient video memory, the QR code in the registration screen does not display correctly. Increase the video memory of your VM to 8 MB or higher. The steps to do this vary depending on the platform you use to deploy your VM. In vSphere, you can right click on the VM, click Edit Settings and adjust the video card memory in the Video card tab. July 9, 2024 Edge" [ref=e1018]:
                      - cell "When you install Palette Edge using an Edge Installer ISO with a RHEL 8 operating system on a Virtual Machine (VM) with insufficient video memory, the QR code in the registration screen does not display correctly." [ref=e1019]
                      - cell "Increase the video memory of your VM to 8 MB or higher. The steps to do this vary depending on the platform you use to deploy your VM. In vSphere, you can right click on the VM, click Edit Settings and adjust the video card memory in the Video card tab." [ref=e1020]:
                        - text: Increase the video memory of your VM to 8 MB or higher. The steps to do this vary depending on the platform you use to deploy your VM. In vSphere, you can right click on the VM, click
                        - strong [ref=e1021]: Edit Settings
                        - text: and adjust the video card memory in the
                        - strong [ref=e1022]: Video card
                        - text: tab.
                      - cell "July 9, 2024" [ref=e1023]
                      - cell "Edge" [ref=e1024]
                    - row "Custom Certificate Authority (CA) is not supported for accessing AKS clusters. Using a custom CA prevents the spectro-proxy pack from working correctly with AKS clusters. No workaround is available. July 9, 2024 Packs, Clusters" [ref=e1025]:
                      - cell "Custom Certificate Authority (CA) is not supported for accessing AKS clusters. Using a custom CA prevents the spectro-proxy pack from working correctly with AKS clusters." [ref=e1026]:
                        - text: Custom Certificate Authority (CA) is not supported for accessing AKS clusters. Using a custom CA prevents the
                        - code [ref=e1027]: spectro-proxy
                        - text: pack from working correctly with AKS clusters.
                      - cell "No workaround is available." [ref=e1028]
                      - cell "July 9, 2024" [ref=e1029]
                      - cell "Packs, Clusters" [ref=e1030]
                    - row "Manifests attached to an Infrastructure Pack, such as OS, Kubernetes, Network, or Storage, are not applied to the Edge cluster. This issue does not impact the infrastructure pack's YAML definition, which is applied to the cluster. Specify custom configurations through an add-on pack or a custom manifest pack applied after the infrastructure packs. Jul 9, 2024 Edge, Packs" [ref=e1031]:
                      - cell "Manifests attached to an Infrastructure Pack, such as OS, Kubernetes, Network, or Storage, are not applied to the Edge cluster. This issue does not impact the infrastructure pack's YAML definition, which is applied to the cluster." [ref=e1032]
                      - cell "Specify custom configurations through an add-on pack or a custom manifest pack applied after the infrastructure packs." [ref=e1033]
                      - cell "Jul 9, 2024" [ref=e1034]
                      - cell "Edge, Packs" [ref=e1035]
                    - 'row "Clusters using Cilium and deployed to VMware environments with the VXLAN tunnel protocol may encounter an I/O timeout error. This issue is caused by the VXMNET3 adapter, which is dropping network traffic and resulting in VXLAN traffic being dropped. You can learn more about this issue in the Cilium''s GitHub issue #21801. Review the Cilium Troubleshooting section for workarounds. June 27, 2024 Packs, Clusters, Edge" [ref=e1036]':
                      - 'cell "Clusters using Cilium and deployed to VMware environments with the VXLAN tunnel protocol may encounter an I/O timeout error. This issue is caused by the VXMNET3 adapter, which is dropping network traffic and resulting in VXLAN traffic being dropped. You can learn more about this issue in the Cilium''s GitHub issue #21801." [ref=e1037]':
                        - text: Clusters using Cilium and deployed to VMware environments with the VXLAN tunnel protocol may encounter an I/O timeout error. This issue is caused by the VXMNET3 adapter, which is dropping network traffic and resulting in VXLAN traffic being dropped. You can learn more about this issue in the
                        - 'link "Cilium''s GitHub issue #21801" [ref=e1038] [cursor=pointer]':
                          - /url: https://github.com/cilium/cilium/issues/21801
                        - text: .
                      - cell "Review the Cilium Troubleshooting section for workarounds." [ref=e1039]:
                        - text: Review the
                        - link "Cilium Troubleshooting" [ref=e1040] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=cni-cilium-oss
                        - text: section for workarounds.
                      - cell "June 27, 2024" [ref=e1041]
                      - cell "Packs, Clusters, Edge" [ref=e1042]
                    - row "Sonobuoy scans fail to generate reports on airgapped Palette Edge clusters. No workaround is available. June 24, 2024 Edge" [ref=e1043]:
                      - cell "Sonobuoy scans fail to generate reports on airgapped Palette Edge clusters." [ref=e1044]:
                        - link "Sonobuoy" [ref=e1045] [cursor=pointer]:
                          - /url: /clusters/cluster-management/compliance-scan/#conformance-testing
                        - text: scans fail to generate reports on airgapped Palette Edge clusters.
                      - cell "No workaround is available." [ref=e1046]
                      - cell "June 24, 2024" [ref=e1047]
                      - cell "Edge" [ref=e1048]
                    - row "Clusters configured with OpenID Connect (OIDC) at the Kubernetes layer encounter issues when authenticating with the non-admin Kubeconfig file. Kubeconfig files using OIDC to authenticate will not work if the SSL certificate is set at the OIDC provider level. Use the admin Kubeconfig file to authenticate with the cluster, as it does not use OIDC to authenticate. June 21, 2024 Clusters" [ref=e1049]:
                      - cell "Clusters configured with OpenID Connect (OIDC) at the Kubernetes layer encounter issues when authenticating with the non-admin Kubeconfig file. Kubeconfig files using OIDC to authenticate will not work if the SSL certificate is set at the OIDC provider level." [ref=e1050]:
                        - text: Clusters configured with OpenID Connect (OIDC) at the Kubernetes layer encounter issues when authenticating with the
                        - link "non-admin Kubeconfig file" [ref=e1051] [cursor=pointer]:
                          - /url: /clusters/cluster-management/kubeconfig/#cluster-admin
                        - text: . Kubeconfig files using OIDC to authenticate will not work if the SSL certificate is set at the OIDC provider level.
                      - cell "Use the admin Kubeconfig file to authenticate with the cluster, as it does not use OIDC to authenticate." [ref=e1052]
                      - cell "June 21, 2024" [ref=e1053]
                      - cell "Clusters" [ref=e1054]
                    - row "During the platform upgrade from Palette 4.3 to 4.4, Virtual Clusters may encounter a scenario where the pod palette-controller-manager is not upgraded to the newer version of Palette. The virtual cluster will continue to be operational, and this does not impact its functionality. Refer to the Controller Manager Pod Not Upgraded troubleshooting guide. June 15, 2024 Virtual Clusters" [ref=e1055]:
                      - cell "During the platform upgrade from Palette 4.3 to 4.4, Virtual Clusters may encounter a scenario where the pod palette-controller-manager is not upgraded to the newer version of Palette. The virtual cluster will continue to be operational, and this does not impact its functionality." [ref=e1056]:
                        - text: During the platform upgrade from Palette 4.3 to 4.4, Virtual Clusters may encounter a scenario where the pod
                        - code [ref=e1057]: palette-controller-manager
                        - text: is not upgraded to the newer version of Palette. The virtual cluster will continue to be operational, and this does not impact its functionality.
                      - cell "Refer to the Controller Manager Pod Not Upgraded troubleshooting guide." [ref=e1058]:
                        - text: Refer to the
                        - link "Controller Manager Pod Not Upgraded" [ref=e1059] [cursor=pointer]:
                          - /url: /troubleshooting/palette-dev-engine/#scenario---controller-manager-pod-not-upgraded
                        - text: troubleshooting guide.
                      - cell "June 15, 2024" [ref=e1060]
                      - cell "Virtual Clusters" [ref=e1061]
                    - row "Edge hosts with FIPS-compliant Red Hat Enterprise Linux (RHEL) and Ubuntu Operating Systems (OS) may encounter the error where the systemd-resolved.service service enters the failed state. This prevents the nameserver from being configured, which will result in cluster deployment failure. Refer to TroubleShooting for a workaround. June 15, 2024 Edge" [ref=e1062]:
                      - cell "Edge hosts with FIPS-compliant Red Hat Enterprise Linux (RHEL) and Ubuntu Operating Systems (OS) may encounter the error where the systemd-resolved.service service enters the failed state. This prevents the nameserver from being configured, which will result in cluster deployment failure." [ref=e1063]:
                        - text: Edge hosts with FIPS-compliant Red Hat Enterprise Linux (RHEL) and Ubuntu Operating Systems (OS) may encounter the error where the
                        - code [ref=e1064]: systemd-resolved.service
                        - text: service enters the
                        - strong [ref=e1065]: failed
                        - text: state. This prevents the nameserver from being configured, which will result in cluster deployment failure.
                      - cell "Refer to TroubleShooting for a workaround." [ref=e1066]:
                        - text: Refer to
                        - link "TroubleShooting" [ref=e1067] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---systemd-resolvedservice-enters-failed-state
                        - text: for a workaround.
                      - cell "June 15, 2024" [ref=e1068]
                      - cell "Edge" [ref=e1069]
                    - row "The GKE cluster's Kubernetes pods are failing to start because the Kubernetes patch version is unavailable. This is encountered during pod restarts or node scaling operations. Deploy a new cluster and use a GKE cluster profile that does not contain a Kubernetes pack layer with a patch version. Migrate the workloads from the existing cluster to the new cluster. This is a breaking change introduced in Palette 4.4.0 June 15, 2024 Packs, Clusters" [ref=e1070]:
                      - cell "The GKE cluster's Kubernetes pods are failing to start because the Kubernetes patch version is unavailable. This is encountered during pod restarts or node scaling operations." [ref=e1071]
                      - cell "Deploy a new cluster and use a GKE cluster profile that does not contain a Kubernetes pack layer with a patch version. Migrate the workloads from the existing cluster to the new cluster. This is a breaking change introduced in Palette 4.4.0" [ref=e1072]
                      - cell "June 15, 2024" [ref=e1073]
                      - cell "Packs, Clusters" [ref=e1074]
                    - row "Microk8s does not support multi-node control plane clusters. The upgrade strategy, InPlaceUpgrade, is the only option available for use. No workaround is available. June 15, 2024 Packs" [ref=e1075]:
                      - cell "Microk8s does not support multi-node control plane clusters. The upgrade strategy, InPlaceUpgrade, is the only option available for use." [ref=e1076]:
                        - link "Microk8s" [ref=e1077] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=kubernetes-microk8s
                        - text: does not support multi-node control plane clusters. The upgrade strategy,
                        - code [ref=e1078]: InPlaceUpgrade
                        - text: ", is the only option available for use."
                      - cell "No workaround is available." [ref=e1079]
                      - cell "June 15, 2024" [ref=e1080]
                      - cell "Packs" [ref=e1081]
                    - row "Clusters using Microk8s as the Kubernetes distribution, the control plane node fails to upgrade when using the InPlaceUpgrade strategy for sequential upgrades, such as upgrading from version 1.25.x to version 1.26.x and then to version 1.27.x. Refer to the Control Plane Node Fails to Upgrade in Sequential MicroK8s Upgrades troubleshooting guide for resolution steps. June 15, 2024 Packs" [ref=e1082]:
                      - cell "Clusters using Microk8s as the Kubernetes distribution, the control plane node fails to upgrade when using the InPlaceUpgrade strategy for sequential upgrades, such as upgrading from version 1.25.x to version 1.26.x and then to version 1.27.x." [ref=e1083]:
                        - text: Clusters using
                        - link "Microk8s" [ref=e1084] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=kubernetes-microk8s
                        - text: as the Kubernetes distribution, the control plane node fails to upgrade when using the
                        - code [ref=e1085]: InPlaceUpgrade
                        - text: strategy for sequential upgrades, such as upgrading from version 1.25.x to version 1.26.x and then to version 1.27.x.
                      - cell "Refer to the Control Plane Node Fails to Upgrade in Sequential MicroK8s Upgrades troubleshooting guide for resolution steps." [ref=e1086]:
                        - text: Refer to the
                        - link "Control Plane Node Fails to Upgrade in Sequential MicroK8s Upgrades" [ref=e1087] [cursor=pointer]:
                          - /url: /troubleshooting/pack-issues/
                        - text: troubleshooting guide for resolution steps.
                      - cell "June 15, 2024" [ref=e1088]
                      - cell "Packs" [ref=e1089]
                    - row "Azure IaaS clusters are having issues with deployed load balancers and ingress deployments when using Kubernetes versions 1.29.0 and 1.29.4. Incoming connections time out as a result due to a lack of network path inside the cluster. AKS clusters are not impacted. Use a Kubernetes version lower than 1.29.0 June 12, 2024 Clusters" [ref=e1090]:
                      - cell "Azure IaaS clusters are having issues with deployed load balancers and ingress deployments when using Kubernetes versions 1.29.0 and 1.29.4. Incoming connections time out as a result due to a lack of network path inside the cluster. AKS clusters are not impacted." [ref=e1091]
                      - cell "Use a Kubernetes version lower than 1.29.0" [ref=e1092]
                      - cell "June 12, 2024" [ref=e1093]
                      - cell "Clusters" [ref=e1094]
                    - row "OIDC integration with Virtual Clusters is not functional. All other operations related to Virtual Clusters are operational. No workaround is available. Jun 11, 2024 Virtual Clusters" [ref=e1095]:
                      - cell "OIDC integration with Virtual Clusters is not functional. All other operations related to Virtual Clusters are operational." [ref=e1096]
                      - cell "No workaround is available." [ref=e1097]
                      - cell "Jun 11, 2024" [ref=e1098]
                      - cell "Virtual Clusters" [ref=e1099]
                    - row [ref=e1100]:
                      - cell "Deploying self-hosted Palette or VerteX to a vSphere environment fails if vCenter has standalone hosts directly under a data center. Persistent Volume (PV) provisioning fails due to an upstream issue with the vSphere Container Storage Interface (CSI) for all versions before v3.2.0. Palette and VerteX use the vSphere CSI version 3.1.2 internally. The issue may also occur in workload clusters deployed on vSphere using the same vSphere CSI for storage volume provisioning." [ref=e1101]
                      - 'cell "If you encounter the following error message when deploying self-hosted Palette or VerteX: ''ProvisioningFailed failed to provision volume with StorageClass \"spectro-storage-class\". Error: failed to fetch hosts from entity ComputeResource:domain-xyz then use the following workaround. Remove standalone hosts directly under the data center from vCenter and allow the volume provisioning to complete. After the volume is provisioned, you can add the standalone hosts back. You can also use a service account that does not have access to the standalone hosts as the user that deployed Palette." [ref=e1102]':
                        - text: "If you encounter the following error message when deploying self-hosted Palette or VerteX:"
                        - code [ref=e1103]: "'ProvisioningFailed failed to provision volume with StorageClass \"spectro-storage-class\". Error: failed to fetch hosts from entity ComputeResource:domain-xyz"
                        - text: then use the following workaround. Remove standalone hosts directly under the data center from vCenter and allow the volume provisioning to complete. After the volume is provisioned, you can add the standalone hosts back. You can also use a service account that does not have access to the standalone hosts as the user that deployed Palette.
                      - cell "May 21, 2024" [ref=e1104]
                      - cell "Self-Hosted" [ref=e1105]
                    - row "Conducting cluster node scaling operations on a cluster undergoing a backup can lead to issues and potential unresponsiveness. To avoid this, ensure no backup operations are in progress before scaling nodes or performing other cluster operations that change the cluster state April 14, 2024 Clusters" [ref=e1106]:
                      - cell "Conducting cluster node scaling operations on a cluster undergoing a backup can lead to issues and potential unresponsiveness." [ref=e1107]
                      - cell "To avoid this, ensure no backup operations are in progress before scaling nodes or performing other cluster operations that change the cluster state" [ref=e1108]
                      - cell "April 14, 2024" [ref=e1109]
                      - cell "Clusters" [ref=e1110]
                    - row "Palette automatically creates an AWS security group for worker nodes using the format <cluster-name>-node. If a security group with the same name already exists in the VPC, the cluster creation process fails. To avoid this, ensure that no security group with the same name exists in the VPC before creating a cluster. April 14, 2024 Clusters" [ref=e1111]:
                      - cell "Palette automatically creates an AWS security group for worker nodes using the format <cluster-name>-node. If a security group with the same name already exists in the VPC, the cluster creation process fails." [ref=e1112]:
                        - text: Palette automatically creates an AWS security group for worker nodes using the format
                        - code [ref=e1113]: <cluster-name>-node
                        - text: . If a security group with the same name already exists in the VPC, the cluster creation process fails.
                      - cell "To avoid this, ensure that no security group with the same name exists in the VPC before creating a cluster." [ref=e1114]
                      - cell "April 14, 2024" [ref=e1115]
                      - cell "Clusters" [ref=e1116]
                    - row "K3s version 1.27.7 has been marked as Deprecated. This version has a known issue that causes clusters to crash. Upgrade to a newer version of K3s to avoid the issue, such as versions 1.26.12, 1.28.5, and 1.27.11. You can learn more about the issue in the K3s GitHub issue page. April 14, 2024 Packs, Clusters" [ref=e1117]:
                      - cell "K3s version 1.27.7 has been marked as Deprecated. This version has a known issue that causes clusters to crash." [ref=e1118]:
                        - text: K3s version 1.27.7 has been marked as
                        - emphasis [ref=e1119]: Deprecated
                        - text: . This version has a known issue that causes clusters to crash.
                      - cell "Upgrade to a newer version of K3s to avoid the issue, such as versions 1.26.12, 1.28.5, and 1.27.11. You can learn more about the issue in the K3s GitHub issue page." [ref=e1120]:
                        - text: Upgrade to a newer version of K3s to avoid the issue, such as versions 1.26.12, 1.28.5, and 1.27.11. You can learn more about the issue in the
                        - link "K3s GitHub issue" [ref=e1121] [cursor=pointer]:
                          - /url: https://github.com/k3s-io/k3s/issues/9047
                        - text: page.
                      - cell "April 14, 2024" [ref=e1122]
                      - cell "Packs, Clusters" [ref=e1123]
                    - row "When deploying a multi-node AWS EKS cluster with the Container Network Interface (CNI) Calico , the cluster deployment fails. A workaround is to use the AWS VPC CNI in the interim while the issue is resolved. April 14, 2024 Packs, Clusters" [ref=e1124]:
                      - cell "When deploying a multi-node AWS EKS cluster with the Container Network Interface (CNI) Calico , the cluster deployment fails." [ref=e1125]:
                        - text: When deploying a multi-node AWS EKS cluster with the Container Network Interface (CNI)
                        - link "Calico" [ref=e1126] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=cni-calico
                        - text: ", the cluster deployment fails."
                      - cell "A workaround is to use the AWS VPC CNI in the interim while the issue is resolved." [ref=e1127]
                      - cell "April 14, 2024" [ref=e1128]
                      - cell "Packs, Clusters" [ref=e1129]
                    - row "If a Kubernetes cluster deployed onto VMware is deleted, and later re-created with the same name, the cluster creation process fails. The issue is caused by existing resources remaining inside the PCG, or the System PCG, that are not cleaned up during the cluster deletion process. Refer to the VMware Resources Remain After Cluster Deletion troubleshooting guide for resolution steps. April 14, 2024 Clusters" [ref=e1130]:
                      - cell "If a Kubernetes cluster deployed onto VMware is deleted, and later re-created with the same name, the cluster creation process fails. The issue is caused by existing resources remaining inside the PCG, or the System PCG, that are not cleaned up during the cluster deletion process." [ref=e1131]
                      - cell "Refer to the VMware Resources Remain After Cluster Deletion troubleshooting guide for resolution steps." [ref=e1132]:
                        - text: Refer to the
                        - link "VMware Resources Remain After Cluster Deletion" [ref=e1133] [cursor=pointer]:
                          - /url: /troubleshooting/pcg/#scenario---vmware-resources-remain-after-cluster-deletion
                        - text: troubleshooting guide for resolution steps.
                      - cell "April 14, 2024" [ref=e1134]
                      - cell "Clusters" [ref=e1135]
                    - row "Day-2 operations related to infrastructure changes, such as modifying the node size and count, when using MicroK8s are not taking effect. No workaround is available. April 14, 2024 Packs, Clusters" [ref=e1136]:
                      - cell "Day-2 operations related to infrastructure changes, such as modifying the node size and count, when using MicroK8s are not taking effect." [ref=e1137]
                      - cell "No workaround is available." [ref=e1138]
                      - cell "April 14, 2024" [ref=e1139]
                      - cell "Packs, Clusters" [ref=e1140]
                    - row "If a cluster that uses the Rook-Ceph pack experiences network issues, it's possible for the file mount to become and remain unavailable even after the network is restored. This a known issue disclosed in the Rook GitHub repository. To resolve this issue, refer to Rook-Ceph pack documentation. April 14, 2024 Packs, Edge" [ref=e1141]:
                      - cell "If a cluster that uses the Rook-Ceph pack experiences network issues, it's possible for the file mount to become and remain unavailable even after the network is restored." [ref=e1142]
                      - cell "This a known issue disclosed in the Rook GitHub repository. To resolve this issue, refer to Rook-Ceph pack documentation." [ref=e1143]:
                        - text: This a known issue disclosed in the
                        - link "Rook GitHub repository" [ref=e1144] [cursor=pointer]:
                          - /url: https://github.com/rook/rook/issues/13818
                        - text: . To resolve this issue, refer to
                        - link "Rook-Ceph" [ref=e1145] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=csi-rook-ceph
                        - text: pack documentation.
                      - cell "April 14, 2024" [ref=e1146]
                      - cell "Packs, Edge" [ref=e1147]
                    - row "Edge clusters on Edge hosts with ARM64 processors may experience instability issues that cause cluster failures. ARM64 support is limited to a specific set of Edge devices. Currently, Nvidia Jetson devices are supported. April 14, 2024 Edge" [ref=e1148]:
                      - cell "Edge clusters on Edge hosts with ARM64 processors may experience instability issues that cause cluster failures." [ref=e1149]
                      - cell "ARM64 support is limited to a specific set of Edge devices. Currently, Nvidia Jetson devices are supported." [ref=e1150]
                      - cell "April 14, 2024" [ref=e1151]
                      - cell "Edge" [ref=e1152]
                    - row "During the cluster provisioning process of new edge clusters, the Palette webhook pods may not always deploy successfully, causing the cluster to be stuck in the provisioning phase. This issue does not impact deployed clusters. Review the Palette Webhook Pods Fail to Start troubleshooting guide for resolution steps. April 14, 2024 Edge" [ref=e1153]:
                      - cell "During the cluster provisioning process of new edge clusters, the Palette webhook pods may not always deploy successfully, causing the cluster to be stuck in the provisioning phase. This issue does not impact deployed clusters." [ref=e1154]
                      - cell "Review the Palette Webhook Pods Fail to Start troubleshooting guide for resolution steps." [ref=e1155]:
                        - text: Review the
                        - link "Palette Webhook Pods Fail to Start" [ref=e1156] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---palette-webhook-pods-fail-to-start
                        - text: troubleshooting guide for resolution steps.
                      - cell "April 14, 2024" [ref=e1157]
                      - cell "Edge" [ref=e1158]
                - heading "Resolved Known IssuesDirect link to Resolved Known Issues" [level=2] [ref=e1159]:
                  - text: Resolved Known Issues
                  - link "Direct link to Resolved Known Issues" [ref=e1160] [cursor=pointer]:
                    - /url: "#resolved-known-issues"
                    - text: "#"
                - paragraph [ref=e1161]: The following table lists all known issues that have been resolved and are no longer affecting users. Refer to the table for information on the fix version and the date the issue was resolved.
                - table [ref=e1162]:
                  - rowgroup [ref=e1163]:
                    - row "Description Publish Date Product Component Fix Version" [ref=e1164]:
                      - cell "Description" [ref=e1165]
                      - cell "Publish Date" [ref=e1166]
                      - cell "Product Component" [ref=e1167]
                      - cell "Fix Version" [ref=e1168]
                  - rowgroup [ref=e1169]:
                    - 'row "On Edge clusters whose hosts run Ubuntu 24.04 with a Unified Kernel Image (UKI), CoreDNS pods may enter the CrashLoopBackOff state with logs showing [FATAL] plugin/loop: Loop (127.0.0.1:<ephemeral-port> -> :53) detected for zone \".\". This happens because /etc/resolv.conf is symlinked to /run/systemd/resolve/stub-resolv.conf, which lacks real DNS server entries. As a result, CoreDNS forwards DNS queries to itself, creating a recursive loop. Refer to Troubleshooting - Edge for the workaround. October 7, 2025 Edge 4.8.21" [ref=e1170]':
                      - 'cell "On Edge clusters whose hosts run Ubuntu 24.04 with a Unified Kernel Image (UKI), CoreDNS pods may enter the CrashLoopBackOff state with logs showing [FATAL] plugin/loop: Loop (127.0.0.1:<ephemeral-port> -> :53) detected for zone \".\". This happens because /etc/resolv.conf is symlinked to /run/systemd/resolve/stub-resolv.conf, which lacks real DNS server entries. As a result, CoreDNS forwards DNS queries to itself, creating a recursive loop. Refer to Troubleshooting - Edge for the workaround." [ref=e1171]':
                        - text: On Edge clusters whose hosts run Ubuntu 24.04 with a Unified Kernel Image (UKI), CoreDNS pods may enter the
                        - code [ref=e1172]: CrashLoopBackOff
                        - text: state with logs showing
                        - code [ref=e1173]: "[FATAL] plugin/loop: Loop (127.0.0.1:<ephemeral-port> -> :53) detected for zone \".\""
                        - text: . This happens because
                        - code [ref=e1174]: /etc/resolv.conf
                        - text: is symlinked to
                        - code [ref=e1175]: /run/systemd/resolve/stub-resolv.conf
                        - text: ", which lacks real DNS server entries. As a result, CoreDNS forwards DNS queries to itself, creating a recursive loop. Refer to"
                        - link "Troubleshooting - Edge" [ref=e1176] [cursor=pointer]:
                          - /url: /troubleshooting/edge/#scenario---coredns-pods-stuck-in-crashloopbackoff-due-to-dns-loop
                        - text: for the workaround.
                      - cell "October 7, 2025" [ref=e1177]
                      - cell "Edge" [ref=e1178]
                      - cell "4.8.21" [ref=e1179]
                    - row "Agent mode Edge cluster nodes configured with Agent version 4.8.1 may lose pre-existing systemd-networkd configurations (VLANs, bonds, and bridges) after a reboot. This occurs because the cleanup logic intended to remove stale VLAN configurations inadvertently deletes valid, pre-existing configurations not created through user data. December 5, 2025 Edge 4.8.12" [ref=e1180]:
                      - cell "Agent mode Edge cluster nodes configured with Agent version 4.8.1 may lose pre-existing systemd-networkd configurations (VLANs, bonds, and bridges) after a reboot. This occurs because the cleanup logic intended to remove stale VLAN configurations inadvertently deletes valid, pre-existing configurations not created through user data." [ref=e1181]:
                        - link "Agent mode" [ref=e1182] [cursor=pointer]:
                          - /url: /deployment-modes/agent-mode/
                        - text: Edge cluster nodes configured with Agent version 4.8.1 may lose pre-existing
                        - code [ref=e1183]: systemd-networkd
                        - text: configurations (VLANs, bonds, and bridges) after a reboot. This occurs because the cleanup logic intended to remove stale VLAN configurations inadvertently deletes valid, pre-existing configurations not created through user data.
                      - cell "December 5, 2025" [ref=e1184]
                      - cell "Edge" [ref=e1185]
                      - cell "4.8.12" [ref=e1186]
                    - row "Users cannot create Ubuntu 20.04 FIPS ISO images using CanvOS version 4.7.16. November 14, 2025 Edge 4.7.30" [ref=e1187]:
                      - cell "Users cannot create Ubuntu 20.04 FIPS ISO images using CanvOS version 4.7.16." [ref=e1188]
                      - cell "November 14, 2025" [ref=e1189]
                      - cell "Edge" [ref=e1190]
                      - cell "4.7.30" [ref=e1191]
                    - row "Cluster deployments may fail after upgrading the Cluster API Provider AWS (CAPA) if the Instance Metadata Service Version 2 (IMDSv2) is configured with Metadata version set to V2 only (token required) in the Amazon Elastic Compute Cloud (EC2) settings. September 1, 2025 Clusters 4.7.21" [ref=e1192]:
                      - cell "Cluster deployments may fail after upgrading the Cluster API Provider AWS (CAPA) if the Instance Metadata Service Version 2 (IMDSv2) is configured with Metadata version set to V2 only (token required) in the Amazon Elastic Compute Cloud (EC2) settings." [ref=e1193]:
                        - text: Cluster deployments may fail after upgrading the Cluster API Provider AWS (CAPA) if the Instance Metadata Service Version 2 (IMDSv2) is configured with
                        - strong [ref=e1194]: Metadata version
                        - text: set to
                        - strong [ref=e1195]: V2 only (token required)
                        - text: in the Amazon Elastic Compute Cloud (EC2) settings.
                      - cell "September 1, 2025" [ref=e1196]
                      - cell "Clusters" [ref=e1197]
                      - cell "4.7.21" [ref=e1198]
                    - row "Cluster profile variables are incorrectly applied to the charts.virtual-machine-orchestrator.appConfig.clusterInfo.consoleBaseAddress field used to provide direct access to Virtual Machine Dashboard. September 20, 2025 Virtual Machine Orchestrator 4.7.20" [ref=e1199]:
                      - cell "Cluster profile variables are incorrectly applied to the charts.virtual-machine-orchestrator.appConfig.clusterInfo.consoleBaseAddress field used to provide direct access to Virtual Machine Dashboard." [ref=e1200]:
                        - link "Cluster profile variables" [ref=e1201] [cursor=pointer]:
                          - /url: /profiles/cluster-profiles/create-cluster-profiles/define-profile-variables/create-cluster-profile-variables/
                        - text: are incorrectly applied to the
                        - code [ref=e1202]: charts.virtual-machine-orchestrator.appConfig.clusterInfo.consoleBaseAddress
                        - text: field used to provide
                        - link "direct access to Virtual Machine Dashboard" [ref=e1203] [cursor=pointer]:
                          - /url: /vm-management/configure-console-base-address/
                        - text: .
                      - cell "September 20, 2025" [ref=e1204]
                      - cell "Virtual Machine Orchestrator" [ref=e1205]
                      - cell "4.7.20" [ref=e1206]
                    - row "Network overlay cluster nodes may display erroneous failed to add static FDB entry after cleanup...Stdout already set, output logs after upgrading the Palette agent to version 4.7.9. Cluster functionality is not affected. September 20, 2025 Edge 4.7.20" [ref=e1207]:
                      - cell "Network overlay cluster nodes may display erroneous failed to add static FDB entry after cleanup...Stdout already set, output logs after upgrading the Palette agent to version 4.7.9. Cluster functionality is not affected." [ref=e1208]:
                        - text: Network overlay cluster nodes may display erroneous
                        - code [ref=e1209]: failed to add static FDB entry after cleanup...Stdout already set, output
                        - text: logs after
                        - link "upgrading the Palette agent" [ref=e1210] [cursor=pointer]:
                          - /url: /clusters/edge/cluster-management/agent-upgrade-airgap/
                        - text: to version 4.7.9. Cluster functionality is not affected.
                      - cell "September 20, 2025" [ref=e1211]
                      - cell "Edge" [ref=e1212]
                      - cell "4.7.20" [ref=e1213]
                    - row "On Azure IaaS clusters created using a Palette version prior to 4.6.32, scaling worker node pools does not attach newly created nodes to an outbound load balancer after upgrading to Palette version 4.6.32 or later and the cluster's Palette Agent version to 4.6.7 or later. This impacts outbound connectivity and may also disassociate existing NAT gateways from the worker node pool subnet, resulting in a loss of egress connectivity. September 1, 2025 Clusters, Self-Hosted 4.7.20" [ref=e1214]:
                      - cell "On Azure IaaS clusters created using a Palette version prior to 4.6.32, scaling worker node pools does not attach newly created nodes to an outbound load balancer after upgrading to Palette version 4.6.32 or later and the cluster's Palette Agent version to 4.6.7 or later. This impacts outbound connectivity and may also disassociate existing NAT gateways from the worker node pool subnet, resulting in a loss of egress connectivity." [ref=e1215]
                      - cell "September 1, 2025" [ref=e1216]
                      - cell "Clusters, Self-Hosted" [ref=e1217]
                      - cell "4.7.20" [ref=e1218]
                    - row "HTTP-Proxies configured in Local UI prior to cluster creation are not applied correctly. August 17, 2025 Edge 4.7.15" [ref=e1219]:
                      - cell "HTTP-Proxies configured in Local UI prior to cluster creation are not applied correctly." [ref=e1220]:
                        - link "HTTP-Proxies configured in Local UI" [ref=e1221] [cursor=pointer]:
                          - /url: /clusters/edge/local-ui/host-management/configure-proxy/
                        - text: prior to cluster creation are not applied correctly.
                      - cell "August 17, 2025" [ref=e1222]
                      - cell "Edge" [ref=e1223]
                      - cell "4.7.15" [ref=e1224]
                    - row "After deleting an Edge cluster via the Palette UI, users cannot establish an SSH connection with their user-data credentials to the Edge host on which the cluster was previously deployed. This affects Edge hosts using Palette agent versions 4.6.21 - 4.7.9. To restore SSH access, connect to the Edge host via remote shell with a temporary username and password and add your user's password to the user-data file. August 8, 2025 Edge, Clusters 4.7.15" [ref=e1225]:
                      - cell "After deleting an Edge cluster via the Palette UI, users cannot establish an SSH connection with their user-data credentials to the Edge host on which the cluster was previously deployed. This affects Edge hosts using Palette agent versions 4.6.21 - 4.7.9. To restore SSH access, connect to the Edge host via remote shell with a temporary username and password and add your user's password to the user-data file." [ref=e1226]:
                        - text: After deleting an Edge cluster via the Palette UI, users cannot establish an SSH connection with their
                        - code [ref=e1227]: user-data
                        - text: credentials to the Edge host on which the cluster was previously deployed. This affects Edge hosts using Palette agent versions 4.6.21 - 4.7.9. To restore SSH access, connect to the Edge host via
                        - link "remote shell" [ref=e1228] [cursor=pointer]:
                          - /url: /clusters/edge/cluster-management/remote-shell/
                        - text: with a temporary username and password and add your user's password to the
                        - code [ref=e1229]: user-data
                        - text: file.
                      - cell "August 8, 2025" [ref=e1230]
                      - cell "Edge, Clusters" [ref=e1231]
                      - cell "4.7.15" [ref=e1232]
                    - row "Agent mode clusters may initiate cluster repaves intermittently. July 21, 2025 Edge 4.7.13" [ref=e1233]:
                      - cell "Agent mode clusters may initiate cluster repaves intermittently." [ref=e1234]:
                        - link "Agent mode" [ref=e1235] [cursor=pointer]:
                          - /url: /deployment-modes/agent-mode/
                        - text: clusters may initiate
                        - link "cluster repaves" [ref=e1236] [cursor=pointer]:
                          - /url: /clusters/cluster-management/node-pool/#repave-behavior-and-configuration
                        - text: intermittently.
                      - cell "July 21, 2025" [ref=e1237]
                      - cell "Edge" [ref=e1238]
                      - cell "4.7.13" [ref=e1239]
                    - row "For locally deployed clusters, adding a custom stylus.path to the user-data file causes cluster creation to fail, as it prevents the Palette agent from locating the user-data file. July 21, 2025 Edge 4.7.13" [ref=e1240]:
                      - cell "For locally deployed clusters, adding a custom stylus.path to the user-data file causes cluster creation to fail, as it prevents the Palette agent from locating the user-data file." [ref=e1241]:
                        - text: For locally deployed clusters, adding a custom
                        - code [ref=e1242]: stylus.path
                        - text: to the
                        - code [ref=e1243]: user-data
                        - text: file causes cluster creation to fail, as it prevents the Palette agent from locating the
                        - code [ref=e1244]: user-data
                        - text: file.
                      - cell "July 21, 2025" [ref=e1245]
                      - cell "Edge" [ref=e1246]
                      - cell "4.7.13" [ref=e1247]
                    - row "Masked cluster profile variable values are displayed as plain text in Edge Management API responses. July 24, 2025 Clusters 4.6.44" [ref=e1248]:
                      - cell "Masked cluster profile variable values are displayed as plain text in Edge Management API responses." [ref=e1249]:
                        - text: Masked cluster profile variable values are displayed as plain text in
                        - link "Edge Management API" [ref=e1250] [cursor=pointer]:
                          - /url: /api/introduction/#edge-management-api
                        - text: responses.
                      - cell "July 24, 2025" [ref=e1251]
                      - cell "Clusters" [ref=e1252]
                      - cell "4.6.44" [ref=e1253]
                    - row "When rebooting nodes in a Canonical Edge cluster deployed in a proxied environment, the nodes may fail to come back online. May 31, 2025 Edge 4.7.3" [ref=e1254]:
                      - cell "When rebooting nodes in a Canonical Edge cluster deployed in a proxied environment, the nodes may fail to come back online." [ref=e1255]
                      - cell "May 31, 2025" [ref=e1256]
                      - cell "Edge" [ref=e1257]
                      - cell "4.7.3" [ref=e1258]
                    - row "Due to CAPZ upgrades in version 4.6.32, Azure US Government clusters (both IaaS and AKS) cannot be provisioned, and Day-2 operations cannot be performed on existing Azure US Government clusters. May 31, 2025 Clusters 4.6.36" [ref=e1259]:
                      - cell "Due to CAPZ upgrades in version 4.6.32, Azure US Government clusters (both IaaS and AKS) cannot be provisioned, and Day-2 operations cannot be performed on existing Azure US Government clusters." [ref=e1260]:
                        - text: Due to CAPZ upgrades in version 4.6.32, Azure US Government clusters (both IaaS and AKS) cannot be provisioned, and
                        - link "Day-2 operations" [ref=e1261] [cursor=pointer]:
                          - /url: /clusters/cluster-management/
                        - text: cannot be performed on existing Azure US Government clusters.
                      - cell "May 31, 2025" [ref=e1262]
                      - cell "Clusters" [ref=e1263]
                      - cell "4.6.36" [ref=e1264]
                    - row "Newly created AWS EKS clusters that have configured node pool customizations fail to provision. May 31, 2025 Clusters 4.6.33" [ref=e1265]:
                      - cell "Newly created AWS EKS clusters that have configured node pool customizations fail to provision." [ref=e1266]:
                        - text: Newly created
                        - link "AWS EKS clusters" [ref=e1267] [cursor=pointer]:
                          - /url: /clusters/public-cloud/aws/eks/
                        - text: that have configured
                        - link "node pool customizations" [ref=e1268] [cursor=pointer]:
                          - /url: /clusters/public-cloud/aws/eks/#cloud-configuration-settings
                        - text: fail to provision.
                      - cell "May 31, 2025" [ref=e1269]
                      - cell "Clusters" [ref=e1270]
                      - cell "4.6.33" [ref=e1271]
                    - row "Configuring node pool customizations on existing AWS EKS clusters causes errors. May 31, 2025 Clusters 4.6.33" [ref=e1272]:
                      - cell "Configuring node pool customizations on existing AWS EKS clusters causes errors." [ref=e1273]:
                        - text: Configuring
                        - link "node pool customizations" [ref=e1274] [cursor=pointer]:
                          - /url: /clusters/public-cloud/aws/eks/#cloud-configuration-settings
                        - text: on existing
                        - link "AWS EKS clusters" [ref=e1275] [cursor=pointer]:
                          - /url: /clusters/public-cloud/aws/eks/
                        - text: causes errors.
                      - cell "May 31, 2025" [ref=e1276]
                      - cell "Clusters" [ref=e1277]
                      - cell "4.6.33" [ref=e1278]
                    - row "Palette eXtended Kubernetes (PXK) versions 1.31.x and 1.32.x do not support GCP. Clusters with this configuration fail to deploy. May 31, 2025 Clusters 4.6.32" [ref=e1279]:
                      - cell "Palette eXtended Kubernetes (PXK) versions 1.31.x and 1.32.x do not support GCP. Clusters with this configuration fail to deploy." [ref=e1280]:
                        - link "Palette eXtended Kubernetes (PXK)" [ref=e1281] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=kubernetes
                        - text: versions 1.31.x and 1.32.x do not support GCP. Clusters with this configuration fail to deploy.
                      - cell "May 31, 2025" [ref=e1282]
                      - cell "Clusters" [ref=e1283]
                      - cell "4.6.32" [ref=e1284]
                    - row "Palette eXtended Kubernetes (PXK) versions 1.31.x and 1.32.x do not support AWS. Clusters with this configuration fail to deploy. May 31, 2025 Clusters 4.6.32" [ref=e1285]:
                      - cell "Palette eXtended Kubernetes (PXK) versions 1.31.x and 1.32.x do not support AWS. Clusters with this configuration fail to deploy." [ref=e1286]:
                        - link "Palette eXtended Kubernetes (PXK)" [ref=e1287] [cursor=pointer]:
                          - /url: /integrations/packs/?pack=kubernetes
                        - text: versions 1.31.x and 1.32.x do not support AWS. Clusters with this configuration fail to deploy.
                      - cell "May 31, 2025" [ref=e1288]
                      - cell "Clusters" [ref=e1289]
                      - cell "4.6.32" [ref=e1290]
                    - row "Deletion of GCP clusters fails for clusters associated with cloud with cloud accounts that have been created on older Palette versions. May 5, 2025 Clusters 4.6.25" [ref=e1291]:
                      - cell "Deletion of GCP clusters fails for clusters associated with cloud with cloud accounts that have been created on older Palette versions." [ref=e1292]:
                        - text: Deletion of
                        - link "GCP clusters" [ref=e1293] [cursor=pointer]:
                          - /url: /clusters/public-cloud/gcp/
                        - text: fails for clusters associated with cloud with cloud accounts that have been created on older Palette versions.
                      - cell "May 5, 2025" [ref=e1294]
                      - cell "Clusters" [ref=e1295]
                      - cell "4.6.25" [ref=e1296]
                    - row "Software Bill of Materials (SBOM) scans are failing to execute successfully for all clusters. May 5, 2025 Clusters 4.6.24" [ref=e1297]:
                      - cell "Software Bill of Materials (SBOM) scans are failing to execute successfully for all clusters." [ref=e1298]:
                        - link "Software Bill of Materials (SBOM)" [ref=e1299] [cursor=pointer]:
                          - /url: /clusters/cluster-management/compliance-scan/#sbom-dependencies--vulnerabilities
                        - text: scans are failing to execute successfully for all clusters.
                      - cell "May 5, 2025" [ref=e1300]
                      - cell "Clusters" [ref=e1301]
                      - cell "4.6.24" [ref=e1302]
                    - row "Registry Mapping Rules provided with stylus.externalRegistries.registryMappingRules are not respected for connected edge clusters. April 19, 2025 Edge 4.6.23" [ref=e1303]:
                      - cell "Registry Mapping Rules provided with stylus.externalRegistries.registryMappingRules are not respected for connected edge clusters." [ref=e1304]:
                        - link "Registry Mapping Rules" [ref=e1305] [cursor=pointer]:
                          - /url: /clusters/edge/edge-configuration/installer-reference/#registry-mapping-rules
                        - text: provided with
                        - code [ref=e1306]: stylus.externalRegistries.registryMappingRules
                        - text: are not respected for connected
                        - link "edge clusters" [ref=e1307] [cursor=pointer]:
                          - /url: /clusters/edge/
                        - text: .
                      - cell "April 19, 2025" [ref=e1308]
                      - cell "Edge" [ref=e1309]
                      - cell "4.6.23" [ref=e1310]
                    - row "For clusters deployed with agent mode on Palette agent version 4.5.14, having a custom stylus.path defined in the user-data file causes Kubernetes upgrades to fail. This affects all Kubernetes packs. March 15, 2025 Edge 4.6.12" [ref=e1311]:
                      - cell "For clusters deployed with agent mode on Palette agent version 4.5.14, having a custom stylus.path defined in the user-data file causes Kubernetes upgrades to fail. This affects all Kubernetes packs." [ref=e1312]:
                        - text: For clusters deployed with
                        - link "agent mode" [ref=e1313] [cursor=pointer]:
                          - /url: /deployment-modes/agent-mode/
                        - text: on Palette agent version 4.5.14, having a custom
                        - code [ref=e1314]: stylus.path
                        - text: defined in the
                        - strong [ref=e1315]: user-data
                        - text: file causes Kubernetes upgrades to fail. This affects all Kubernetes packs.
                      - cell "March 15, 2025" [ref=e1316]
                      - cell "Edge" [ref=e1317]
                      - cell "4.6.12" [ref=e1318]
                    - row "If an Edge host operating a cluster in connected mode loses connection to Palette, the cluster will not auto-renew its Public Key Infrastructure (PKI) certificates. When it re-establishes the connection to Palette, the Edge host will renew the certificates if the existing certificates have less than 30 days before they expire. March 15, 2025 Edge 4.6.12" [ref=e1319]:
                      - cell "If an Edge host operating a cluster in connected mode loses connection to Palette, the cluster will not auto-renew its Public Key Infrastructure (PKI) certificates. When it re-establishes the connection to Palette, the Edge host will renew the certificates if the existing certificates have less than 30 days before they expire." [ref=e1320]
                      - cell "March 15, 2025" [ref=e1321]
                      - cell "Edge" [ref=e1322]
                      - cell "4.6.12" [ref=e1323]
                    - row "Cluster Removal of Palette and VerteX clusters will get stuck if you attempt to delete a cluster with expired cloud account credentials. This may lead to cloud resources not being correctly removed and incurring unexpected cloud costs. March 15, 2025 Clusters 4.6.12" [ref=e1324]:
                      - cell "Cluster Removal of Palette and VerteX clusters will get stuck if you attempt to delete a cluster with expired cloud account credentials. This may lead to cloud resources not being correctly removed and incurring unexpected cloud costs." [ref=e1325]:
                        - link "Cluster Removal" [ref=e1326] [cursor=pointer]:
                          - /url: /clusters/cluster-management/remove-clusters/
                        - text: of Palette and VerteX clusters will get stuck if you attempt to delete a cluster with expired cloud account credentials. This may lead to cloud resources not being correctly removed and incurring unexpected cloud costs.
                      - cell "March 15, 2025" [ref=e1327]
                      - cell "Clusters" [ref=e1328]
                      - cell "4.6.12" [ref=e1329]
                    - row "Palette Extended Kubernetes - Edge (PXK-E) version 1.31 is not yet supported for Edge clusters. February 24, 2025 Edge 4.6.12" [ref=e1330]:
                      - cell "Palette Extended Kubernetes - Edge (PXK-E) version 1.31 is not yet supported for Edge clusters." [ref=e1331]
                      - cell "February 24, 2025" [ref=e1332]
                      - cell "Edge" [ref=e1333]
                      - cell "4.6.12" [ref=e1334]
                    - row "K3s certificate renewals for Two-node Edge clusters are not managed by Palette. Two-node clusters will use a default certificate list. February 17, 2025 Edge 4.6.8" [ref=e1335]:
                      - cell "K3s certificate renewals for Two-node Edge clusters are not managed by Palette. Two-node clusters will use a default certificate list." [ref=e1336]:
                        - text: K3s certificate renewals for
                        - link "Two-node Edge clusters" [ref=e1337] [cursor=pointer]:
                          - /url: /clusters/edge/architecture/two-node/
                        - text: are not managed by Palette. Two-node clusters will use a default certificate list.
                      - cell "February 17, 2025" [ref=e1338]
                      - cell "Edge" [ref=e1339]
                      - cell "4.6.8" [ref=e1340]
                    - row "Clusters that are currently deployed and attempting to upgrade from Kubernetes version 1.31.1 to newer versions are getting stuck in the upgrade state. This issue also affects Day-2 operations related to changes in the Kubernetes layer YAML and the resize of control plane nodes for clusters using Kubernetes version 1.31.1. The root of the problem is an internal conversion error stemming from an internal type change introduced in Kubernetes version 1.31.x. January 19, 2025 Clusters, Packs 4.6.0" [ref=e1341]:
                      - cell "Clusters that are currently deployed and attempting to upgrade from Kubernetes version 1.31.1 to newer versions are getting stuck in the upgrade state. This issue also affects Day-2 operations related to changes in the Kubernetes layer YAML and the resize of control plane nodes for clusters using Kubernetes version 1.31.1. The root of the problem is an internal conversion error stemming from an internal type change introduced in Kubernetes version 1.31.x." [ref=e1342]
                      - cell "January 19, 2025" [ref=e1343]
                      - cell "Clusters, Packs" [ref=e1344]
                      - cell "4.6.0" [ref=e1345]
                    - row "Deployed clusters using a Palette version before 4.4.20 will not have accurate Kubernetes certificate expiration information displayed on the Palette UI certificate page. Starting with Palette version 4.4.20, the certificate information displayed on the certificate management page will be correct. October 31, 2024 Clusters 4.4.20" [ref=e1346]:
                      - cell "Deployed clusters using a Palette version before 4.4.20 will not have accurate Kubernetes certificate expiration information displayed on the Palette UI certificate page. Starting with Palette version 4.4.20, the certificate information displayed on the certificate management page will be correct." [ref=e1347]
                      - cell "October 31, 2024" [ref=e1348]
                      - cell "Clusters" [ref=e1349]
                      - cell "4.4.20" [ref=e1350]
                    - row "A change in the Edge Cluster API endpoint affects Terraform and API workflows for Edge cluster creation or modification. The field spec.cloudConfig.controlPlaneEndpoint.type no longer accepts IP addresses. The accepted values are now VIP, External, and DDNS. As a result, API and Terraform workflows for Edge cluster creation or modification that use the IP address are currently unavailable. September 18, 2024 Edge 4.4.19" [ref=e1351]:
                      - cell "A change in the Edge Cluster API endpoint affects Terraform and API workflows for Edge cluster creation or modification. The field spec.cloudConfig.controlPlaneEndpoint.type no longer accepts IP addresses. The accepted values are now VIP, External, and DDNS. As a result, API and Terraform workflows for Edge cluster creation or modification that use the IP address are currently unavailable." [ref=e1352]:
                        - text: A change in the
                        - link "Edge Cluster" [ref=e1353] [cursor=pointer]:
                          - /url: https://docs.spectrocloud.com/api/v1/v-1-spectro-clusters-edge-native-create/
                        - text: API endpoint affects Terraform and API workflows for Edge cluster creation or modification. The field
                        - code [ref=e1354]: spec.cloudConfig.controlPlaneEndpoint.type
                        - text: no longer accepts IP addresses. The accepted values are now
                        - code [ref=e1355]: VIP
                        - text: ","
                        - code [ref=e1356]: External
                        - text: ", and"
                        - code [ref=e1357]: DDNS
                        - text: . As a result, API and Terraform workflows for Edge cluster creation or modification that use the IP address are currently unavailable.
                      - cell "September 18, 2024" [ref=e1358]
                      - cell "Edge" [ref=e1359]
                      - cell "4.4.19" [ref=e1360]
                    - row "In a VMware environment, self-hosted Palette instances do not receive a unique cluster ID when deployed, which can cause issues during a node repave event, such as a Kubernetes version upgrade. Specifically, Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) will experience start problems due to the lack of a unique cluster ID. April 14, 2024 Self-Hosted 4.4.14" [ref=e1361]:
                      - cell "In a VMware environment, self-hosted Palette instances do not receive a unique cluster ID when deployed, which can cause issues during a node repave event, such as a Kubernetes version upgrade. Specifically, Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) will experience start problems due to the lack of a unique cluster ID." [ref=e1362]
                      - cell "April 14, 2024" [ref=e1363]
                      - cell "Self-Hosted" [ref=e1364]
                      - cell "4.4.14" [ref=e1365]
                    - row "The Palette agent fails to pull Helm charts from private registries when initializing Edge clusters. July 25, 2024 Edge 4.4.12" [ref=e1366]:
                      - cell "The Palette agent fails to pull Helm charts from private registries when initializing Edge clusters." [ref=e1367]
                      - cell "July 25, 2024" [ref=e1368]
                      - cell "Edge" [ref=e1369]
                      - cell "4.4.12" [ref=e1370]
                    - row "The VerteX enterprise cluster is unable to complete backup operations. June 6, 2024 VerteX 4.4.11" [ref=e1371]:
                      - cell "The VerteX enterprise cluster is unable to complete backup operations." [ref=e1372]
                      - cell "June 6, 2024" [ref=e1373]
                      - cell "VerteX" [ref=e1374]
                      - cell "4.4.11" [ref=e1375]
                    - row "An issue prevents RKE2 and Palette eXtended Kubernetes (PXK) on version 1.29.4 from operating correctly with Canonical MAAS. July 20, 2024 Packs, Clusters 4.4.11" [ref=e1376]:
                      - cell "An issue prevents RKE2 and Palette eXtended Kubernetes (PXK) on version 1.29.4 from operating correctly with Canonical MAAS." [ref=e1377]
                      - cell "July 20, 2024" [ref=e1378]
                      - cell "Packs, Clusters" [ref=e1379]
                      - cell "4.4.11" [ref=e1380]
                    - row "Deploying self-hosted Palette or VerteX to a vSphere environment fails if vCenter has standalone hosts directly under a data center. Persistent Volume (PV) provisioning fails due to an upstream issue with the vSphere Container Storage Interface (CSI) for all versions before v3.2.0. Palette and VerteX use the vSphere CSI version 3.1.2 internally. The issue may also occur in workload clusters deployed on vSphere using the same vSphere CSI for storage volume provisioning. July 20, 2024 Self-Hosted 4.4.11" [ref=e1381]:
                      - cell "Deploying self-hosted Palette or VerteX to a vSphere environment fails if vCenter has standalone hosts directly under a data center. Persistent Volume (PV) provisioning fails due to an upstream issue with the vSphere Container Storage Interface (CSI) for all versions before v3.2.0. Palette and VerteX use the vSphere CSI version 3.1.2 internally. The issue may also occur in workload clusters deployed on vSphere using the same vSphere CSI for storage volume provisioning." [ref=e1382]
                      - cell "July 20, 2024" [ref=e1383]
                      - cell "Self-Hosted" [ref=e1384]
                      - cell "4.4.11" [ref=e1385]
                    - row "When you upgrade VerteX from version 4.3.x to 4.4.x, a few system pods may remain unhealthy and experience CrashLoopBackOff errors. This issue only impacts VMware vSphere-based installations and occurs because the internal Mongo DNS is incorrectly configured in the configserver ConfigMap. June 29, 2024 Self-Hosted, VerteX 4.4.7" [ref=e1386]:
                      - cell "When you upgrade VerteX from version 4.3.x to 4.4.x, a few system pods may remain unhealthy and experience CrashLoopBackOff errors. This issue only impacts VMware vSphere-based installations and occurs because the internal Mongo DNS is incorrectly configured in the configserver ConfigMap." [ref=e1387]:
                        - text: When you upgrade VerteX from version 4.3.x to 4.4.x, a few system pods may remain unhealthy and experience
                        - emphasis [ref=e1388]: CrashLoopBackOff
                        - text: errors. This issue only impacts VMware vSphere-based installations and occurs because the internal Mongo DNS is incorrectly configured in the
                        - code [ref=e1389]: configserver
                        - text: ConfigMap.
                      - cell "June 29, 2024" [ref=e1390]
                      - cell "Self-Hosted, VerteX" [ref=e1391]
                      - cell "4.4.7" [ref=e1392]
                    - row "Clusters are failing to start on AWS EKS when using the AWS VPC CNI pack and Kubernetes versions 1.28. February 26, 2024 Packs, Clusters 4.2.13" [ref=e1393]:
                      - cell "Clusters are failing to start on AWS EKS when using the AWS VPC CNI pack and Kubernetes versions 1.28." [ref=e1394]
                      - cell "February 26, 2024" [ref=e1395]
                      - cell "Packs, Clusters" [ref=e1396]
                      - cell "4.2.13" [ref=e1397]
                    - row "The Kubernetes Dashboard fails to load when added to a cluster profile and deployed. February 26, 2024 Packs 4.2.13" [ref=e1398]:
                      - cell "The Kubernetes Dashboard fails to load when added to a cluster profile and deployed." [ref=e1399]
                      - cell "February 26, 2024" [ref=e1400]
                      - cell "Packs" [ref=e1401]
                      - cell "4.2.13" [ref=e1402]
                    - row "Clusters using MicroK8s are failing to launch pods due to a mismatch in node affinity labels. February 26, 2024 Packs, Clusters 4.2.13" [ref=e1403]:
                      - cell "Clusters using MicroK8s are failing to launch pods due to a mismatch in node affinity labels." [ref=e1404]
                      - cell "February 26, 2024" [ref=e1405]
                      - cell "Packs, Clusters" [ref=e1406]
                      - cell "4.2.13" [ref=e1407]
                    - row "MAAS clusters are failing to deploy when the default image endpoint is not set in an airgap environment. February 26, 2024 Clusters 4.2.13" [ref=e1408]:
                      - cell "MAAS clusters are failing to deploy when the default image endpoint is not set in an airgap environment." [ref=e1409]
                      - cell "February 26, 2024" [ref=e1410]
                      - cell "Clusters" [ref=e1411]
                      - cell "4.2.13" [ref=e1412]
              - generic [ref=e1413]:
                - generic [ref=e1415]:
                  - text: "Tags:"
                  - list [ref=e1416]:
                    - listitem [ref=e1417]:
                      - link "known-issues" [ref=e1418] [cursor=pointer]:
                        - /url: /tags/known-issues/
                - link "Edit this page" [ref=e1421] [cursor=pointer]:
                  - /url: https://github.com/spectrocloud/librarium/blob/master/docs/docs-content/release-notes/known-issues.md
                  - img [ref=e1422]
                  - text: Edit this page
            - navigation "Docs pages" [ref=e1426]:
              - link "Previous « Announcements" [ref=e1427] [cursor=pointer]:
                - /url: /release-notes/announcements/
                - generic [ref=e1428]: Previous
                - generic [ref=e1429]: « Announcements
              - link "Next Find Breaking Changes »" [ref=e1430] [cursor=pointer]:
                - /url: /release-notes/breaking-changes/
                - generic [ref=e1431]: Next
                - generic [ref=e1432]: Find Breaking Changes »
          - list [ref=e1435]:
            - listitem [ref=e1436]:
              - link "Active Known Issues" [ref=e1437] [cursor=pointer]:
                - /url: "#active-known-issues"
            - listitem [ref=e1438]:
              - link "Resolved Known Issues" [ref=e1439] [cursor=pointer]:
                - /url: "#resolved-known-issues"
  - button "Project Logo Ask AI" [ref=e1440] [cursor=pointer]:
    - generic [ref=e1443]:
      - img "Project Logo" [ref=e1444]
      - paragraph [ref=e1445]: Ask AI
```